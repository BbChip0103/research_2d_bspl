{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical, multi_gpu_model, Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.load(path.join(data_dir, 'imagenet_6_class_172_train_data.npz'))\n",
    "# val_data = np.load(path.join(data_dir, 'imagenet_6_class_172_val_data.npz'))\n",
    "\n",
    "x_train = np.load(path.join(data_dir, 'imagenet_6_class_172_x_train.npy'))\n",
    "y_train = np.load(path.join(data_dir, 'imagenet_6_class_172_y_train.npy'))\n",
    "x_val = np.load(path.join(data_dir, 'imagenet_6_class_172_x_val.npy'))\n",
    "y_val = np.load(path.join(data_dir, 'imagenet_6_class_172_y_val.npy'))\n",
    "y_list = np.load(path.join(data_dir, 'imagenet_6_class_172_y_list.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((235111, 172, 172, 3),\n",
       " (235111,),\n",
       " (3183, 172, 172, 3),\n",
       " (3183,),\n",
       " (3183, 172, 172, 3),\n",
       " (3183,),\n",
       " (6,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train = train_data['x_data']\n",
    "# y_train = train_data['y_data']\n",
    "# x_val = val_data['x_data']\n",
    "# y_val = val_data['y_data']\n",
    "x_test = x_val\n",
    "y_test = y_val\n",
    "# y_list = val_data['y_list']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((235111, 172, 172, 3),\n",
       " (235111, 6),\n",
       " (3183, 172, 172, 3),\n",
       " (3183, 6),\n",
       " (3183, 172, 172, 3),\n",
       " (3183, 6))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = y_val\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train[0].shape\n",
    "output_size = len(y_list)\n",
    "\n",
    "# input_shape = (172,172,3)\n",
    "# output_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2d_cnn_custom_ch_64_DO(conv_num=1):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    for i in range(conv_num):\n",
    "        x = Conv2D(kernel_size=5, filters=64*(2**(i//2)), strides=(1,1), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling2D(pool_size=2, strides=(2,2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "  \n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 473344)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 473344)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 2840070   \n",
      "=================================================================\n",
      "Total params: 2,844,934\n",
      "Trainable params: 2,844,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 118336)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 118336)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 710022    \n",
      "=================================================================\n",
      "Total params: 817,350\n",
      "Trainable params: 817,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 61952)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 61952)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 371718    \n",
      "=================================================================\n",
      "Total params: 683,974\n",
      "Trainable params: 683,974\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 15488)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15488)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 92934     \n",
      "=================================================================\n",
      "Total params: 814,918\n",
      "Trainable params: 814,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 55302     \n",
      "=================================================================\n",
      "Total params: 1,596,742\n",
      "Trainable params: 1,596,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 13830     \n",
      "=================================================================\n",
      "Total params: 3,193,926\n",
      "Trainable params: 3,193,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 6,469,702\n",
      "Trainable params: 6,469,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 8):\n",
    "    model = build_2d_cnn_custom_ch_64_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalanceDataGenerator(Sequence):\n",
    "    def __init__(self, x_data, y_data, batch_size, shuffle=True):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_size = int(np.sum(y_data, axis=0).min())\n",
    "        self.data_shape = x_data.shape[1:]\n",
    "        self.y_label = self.y_data.argmax(axis=1)\n",
    "        self.labels = np.unique(self.y_label)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.labels) * self.sample_size / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.zeros((len(self.labels), self.sample_size))\n",
    "        for i, label in enumerate(self.labels):\n",
    "            y_index = np.argwhere(self.y_label==label).squeeze()\n",
    "            if self.shuffle == True:\n",
    "                self.indexes[i] = np.random.choice(y_index, \n",
    "                                   self.sample_size, \n",
    "                                   replace=False)\n",
    "            else:\n",
    "                self.indexes[i] = y_index[:self.sample_size]\n",
    "                \n",
    "        self.indexes = self.indexes.flatten().astype(np.int32)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "                \n",
    "    def __getitem__(self, batch_idx):\n",
    "        indices = self.indexes[batch_idx*self.batch_size: (batch_idx+1)*self.batch_size]\n",
    "        return self.x_data[indices], self.y_data[indices]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "data_generator = BalanceDataGenerator(x_train, y_train,\n",
    "                                      batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.4741 - acc: 0.3856WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 1.3368 - acc: 0.4425\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.33683, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/001-1.3368.hdf5\n",
      "242/242 [==============================] - 74s 305ms/step - loss: 1.4728 - acc: 0.3866 - val_loss: 1.3368 - val_acc: 0.4425\n",
      "Epoch 2/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.2137 - acc: 0.5158WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 1.1710 - acc: 0.5825\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.33683 to 1.17098, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/002-1.1710.hdf5\n",
      "242/242 [==============================] - 57s 237ms/step - loss: 1.2141 - acc: 0.5154 - val_loss: 1.1710 - val_acc: 0.5825\n",
      "Epoch 3/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.0875 - acc: 0.5791WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 1.2453 - acc: 0.5066\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.17098\n",
      "242/242 [==============================] - 54s 223ms/step - loss: 1.0874 - acc: 0.5788 - val_loss: 1.2453 - val_acc: 0.5066\n",
      "Epoch 4/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.0207 - acc: 0.6047WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 1.3901 - acc: 0.4500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.17098\n",
      "242/242 [==============================] - 38s 155ms/step - loss: 1.0216 - acc: 0.6044 - val_loss: 1.3901 - val_acc: 0.4500\n",
      "Epoch 5/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.9465 - acc: 0.6405WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 1.0204 - acc: 0.5956\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.17098 to 1.02040, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/005-1.0204.hdf5\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.9460 - acc: 0.6411 - val_loss: 1.0204 - val_acc: 0.5956\n",
      "Epoch 6/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8758 - acc: 0.6685WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.7694 - acc: 0.7250\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.02040 to 0.76940, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/006-0.7694.hdf5\n",
      "242/242 [==============================] - 69s 284ms/step - loss: 0.8760 - acc: 0.6687 - val_loss: 0.7694 - val_acc: 0.7250\n",
      "Epoch 7/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7988 - acc: 0.6943WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.8592 - acc: 0.6950\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.76940\n",
      "242/242 [==============================] - 64s 266ms/step - loss: 0.7993 - acc: 0.6942 - val_loss: 0.8592 - val_acc: 0.6950\n",
      "Epoch 8/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7646 - acc: 0.7148WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.8085 - acc: 0.7025\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.76940\n",
      "242/242 [==============================] - 63s 261ms/step - loss: 0.7641 - acc: 0.7149 - val_loss: 0.8085 - val_acc: 0.7025\n",
      "Epoch 9/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7474 - acc: 0.7134WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 1.1046 - acc: 0.5603\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.76940\n",
      "242/242 [==============================] - 40s 166ms/step - loss: 0.7490 - acc: 0.7131 - val_loss: 1.1046 - val_acc: 0.5603\n",
      "Epoch 10/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7087 - acc: 0.7246WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 1.0616 - acc: 0.5797\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.76940\n",
      "242/242 [==============================] - 70s 289ms/step - loss: 0.7085 - acc: 0.7249 - val_loss: 1.0616 - val_acc: 0.5797\n",
      "Epoch 11/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6826 - acc: 0.7407WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.7630 - acc: 0.7144\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.76940 to 0.76297, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/011-0.7630.hdf5\n",
      "242/242 [==============================] - 48s 200ms/step - loss: 0.6818 - acc: 0.7408 - val_loss: 0.7630 - val_acc: 0.7144\n",
      "Epoch 12/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6545 - acc: 0.7487WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.5853 - acc: 0.7972\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.76297 to 0.58529, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/012-0.5853.hdf5\n",
      "242/242 [==============================] - 60s 250ms/step - loss: 0.6552 - acc: 0.7486 - val_loss: 0.5853 - val_acc: 0.7972\n",
      "Epoch 13/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6367 - acc: 0.7599WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.5849 - acc: 0.8037\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.58529 to 0.58493, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/013-0.5849.hdf5\n",
      "242/242 [==============================] - 52s 215ms/step - loss: 0.6367 - acc: 0.7601 - val_loss: 0.5849 - val_acc: 0.8037\n",
      "Epoch 14/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6147 - acc: 0.7647WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.6894 - acc: 0.7375\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.58493\n",
      "242/242 [==============================] - 60s 248ms/step - loss: 0.6138 - acc: 0.7651 - val_loss: 0.6894 - val_acc: 0.7375\n",
      "Epoch 15/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6006 - acc: 0.7765WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.5925 - acc: 0.7912\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.58493\n",
      "242/242 [==============================] - 55s 228ms/step - loss: 0.6004 - acc: 0.7770 - val_loss: 0.5925 - val_acc: 0.7912\n",
      "Epoch 16/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5861 - acc: 0.7769WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.4378 - acc: 0.8637\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.58493 to 0.43779, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/016-0.4378.hdf5\n",
      "242/242 [==============================] - 48s 200ms/step - loss: 0.5859 - acc: 0.7768 - val_loss: 0.4378 - val_acc: 0.8637\n",
      "Epoch 17/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.7849WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.4711 - acc: 0.8381\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.43779\n",
      "242/242 [==============================] - 56s 233ms/step - loss: 0.5582 - acc: 0.7849 - val_loss: 0.4711 - val_acc: 0.8381\n",
      "Epoch 18/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.8051WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.6905 - acc: 0.7534\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.43779\n",
      "242/242 [==============================] - 53s 218ms/step - loss: 0.5251 - acc: 0.8052 - val_loss: 0.6905 - val_acc: 0.7534\n",
      "Epoch 19/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.7911WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.4160 - acc: 0.8725\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.43779 to 0.41601, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/019-0.4160.hdf5\n",
      "242/242 [==============================] - 59s 243ms/step - loss: 0.5462 - acc: 0.7912 - val_loss: 0.4160 - val_acc: 0.8725\n",
      "Epoch 20/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5262 - acc: 0.7985WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.6461 - acc: 0.7650\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 54s 224ms/step - loss: 0.5256 - acc: 0.7987 - val_loss: 0.6461 - val_acc: 0.7650\n",
      "Epoch 21/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8082WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.4684 - acc: 0.8413\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 60s 249ms/step - loss: 0.5070 - acc: 0.8087 - val_loss: 0.4684 - val_acc: 0.8413\n",
      "Epoch 22/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4762 - acc: 0.8193WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.5505 - acc: 0.8044\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 50s 208ms/step - loss: 0.4758 - acc: 0.8194 - val_loss: 0.5505 - val_acc: 0.8044\n",
      "Epoch 23/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8206WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.6721 - acc: 0.7625\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 59s 243ms/step - loss: 0.4799 - acc: 0.8204 - val_loss: 0.6721 - val_acc: 0.7625\n",
      "Epoch 24/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4813 - acc: 0.8150WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.6403 - acc: 0.7850\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 52s 213ms/step - loss: 0.4819 - acc: 0.8145 - val_loss: 0.6403 - val_acc: 0.7850\n",
      "Epoch 25/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4564 - acc: 0.8255WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.4985 - acc: 0.8206\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 53s 220ms/step - loss: 0.4559 - acc: 0.8257 - val_loss: 0.4985 - val_acc: 0.8206\n",
      "Epoch 26/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4324 - acc: 0.8352WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.4589 - acc: 0.8403\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.4322 - acc: 0.8354 - val_loss: 0.4589 - val_acc: 0.8403\n",
      "Epoch 27/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4356 - acc: 0.8344WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.7634 - acc: 0.6975\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 52s 216ms/step - loss: 0.4350 - acc: 0.8346 - val_loss: 0.7634 - val_acc: 0.6975\n",
      "Epoch 28/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.8385WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.4743 - acc: 0.8331\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 51s 209ms/step - loss: 0.4348 - acc: 0.8386 - val_loss: 0.4743 - val_acc: 0.8331\n",
      "Epoch 29/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4202 - acc: 0.8415WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.5852 - acc: 0.7853\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 48s 199ms/step - loss: 0.4192 - acc: 0.8419 - val_loss: 0.5852 - val_acc: 0.7853\n",
      "Epoch 30/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8454WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.4417 - acc: 0.8475\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 53s 219ms/step - loss: 0.4125 - acc: 0.8451 - val_loss: 0.4417 - val_acc: 0.8475\n",
      "Epoch 31/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.8500WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.4874 - acc: 0.8400\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.4049 - acc: 0.8498 - val_loss: 0.4874 - val_acc: 0.8400\n",
      "Epoch 32/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8598WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.5916 - acc: 0.7713\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 55s 226ms/step - loss: 0.3743 - acc: 0.8594 - val_loss: 0.5916 - val_acc: 0.7713\n",
      "Epoch 33/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8586WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 31s 10ms/sample - loss: 0.4925 - acc: 0.8200\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 65s 269ms/step - loss: 0.3776 - acc: 0.8587 - val_loss: 0.4925 - val_acc: 0.8200\n",
      "Epoch 34/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8560WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.5277 - acc: 0.8150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.41601\n",
      "242/242 [==============================] - 63s 261ms/step - loss: 0.3904 - acc: 0.8561 - val_loss: 0.5277 - val_acc: 0.8150\n",
      "Epoch 35/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3713 - acc: 0.8578WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.4128 - acc: 0.8616\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.41601 to 0.41279, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/035-0.4128.hdf5\n",
      "242/242 [==============================] - 53s 221ms/step - loss: 0.3715 - acc: 0.8577 - val_loss: 0.4128 - val_acc: 0.8616\n",
      "Epoch 36/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8689WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.3222 - acc: 0.8994\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.41279 to 0.32220, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/036-0.3222.hdf5\n",
      "242/242 [==============================] - 50s 206ms/step - loss: 0.3575 - acc: 0.8690 - val_loss: 0.3222 - val_acc: 0.8994\n",
      "Epoch 37/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8763WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.4689 - acc: 0.8238\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.32220\n",
      "242/242 [==============================] - 63s 261ms/step - loss: 0.3350 - acc: 0.8765 - val_loss: 0.4689 - val_acc: 0.8238\n",
      "Epoch 38/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8720WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.3614 - acc: 0.8531\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.32220\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.3437 - acc: 0.8720 - val_loss: 0.3614 - val_acc: 0.8531\n",
      "Epoch 39/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.8734WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.4029 - acc: 0.8675\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.32220\n",
      "242/242 [==============================] - 53s 217ms/step - loss: 0.3458 - acc: 0.8732 - val_loss: 0.4029 - val_acc: 0.8675\n",
      "Epoch 40/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8745WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3524 - acc: 0.8850\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.32220\n",
      "242/242 [==============================] - 46s 189ms/step - loss: 0.3346 - acc: 0.8748 - val_loss: 0.3524 - val_acc: 0.8850\n",
      "Epoch 41/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8742WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 9ms/sample - loss: 0.4245 - acc: 0.8400\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.32220\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.3357 - acc: 0.8741 - val_loss: 0.4245 - val_acc: 0.8400\n",
      "Epoch 42/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.8726WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.3792 - acc: 0.8756\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.32220\n",
      "242/242 [==============================] - 55s 226ms/step - loss: 0.3330 - acc: 0.8726 - val_loss: 0.3792 - val_acc: 0.8756\n",
      "Epoch 43/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8839WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3531 - acc: 0.8656\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.32220\n",
      "242/242 [==============================] - 55s 229ms/step - loss: 0.3225 - acc: 0.8835 - val_loss: 0.3531 - val_acc: 0.8656\n",
      "Epoch 44/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.8831WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.3553 - acc: 0.8753\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.32220\n",
      "242/242 [==============================] - 55s 227ms/step - loss: 0.3125 - acc: 0.8828 - val_loss: 0.3553 - val_acc: 0.8753\n",
      "Epoch 45/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.8855WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.3065 - acc: 0.8913\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.32220 to 0.30650, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/045-0.3065.hdf5\n",
      "242/242 [==============================] - 54s 222ms/step - loss: 0.3100 - acc: 0.8856 - val_loss: 0.3065 - val_acc: 0.8913\n",
      "Epoch 46/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.8889WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.4516 - acc: 0.8491\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.30650\n",
      "242/242 [==============================] - 60s 247ms/step - loss: 0.3030 - acc: 0.8888 - val_loss: 0.4516 - val_acc: 0.8491\n",
      "Epoch 47/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.8833WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.5917 - acc: 0.7875\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.30650\n",
      "242/242 [==============================] - 54s 223ms/step - loss: 0.3122 - acc: 0.8835 - val_loss: 0.5917 - val_acc: 0.7875\n",
      "Epoch 48/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.8918WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3816 - acc: 0.8781\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.30650\n",
      "242/242 [==============================] - 55s 226ms/step - loss: 0.2839 - acc: 0.8920 - val_loss: 0.3816 - val_acc: 0.8781\n",
      "Epoch 49/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.8985WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.3122 - acc: 0.8875\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.30650\n",
      "242/242 [==============================] - 58s 242ms/step - loss: 0.2827 - acc: 0.8985 - val_loss: 0.3122 - val_acc: 0.8875\n",
      "Epoch 50/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.8949WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.4794 - acc: 0.8359\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.30650\n",
      "242/242 [==============================] - 59s 243ms/step - loss: 0.2818 - acc: 0.8950 - val_loss: 0.4794 - val_acc: 0.8359\n",
      "Epoch 51/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.8999WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3879 - acc: 0.8606\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.30650\n",
      "242/242 [==============================] - 43s 177ms/step - loss: 0.2728 - acc: 0.8998 - val_loss: 0.3879 - val_acc: 0.8606\n",
      "Epoch 52/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.8967WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.4734 - acc: 0.8359\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.30650\n",
      "242/242 [==============================] - 51s 209ms/step - loss: 0.2771 - acc: 0.8968 - val_loss: 0.4734 - val_acc: 0.8359\n",
      "Epoch 53/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.8933WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3076 - acc: 0.8925\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.30650\n",
      "242/242 [==============================] - 56s 232ms/step - loss: 0.2783 - acc: 0.8932 - val_loss: 0.3076 - val_acc: 0.8925\n",
      "Epoch 54/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.8983WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 30s 9ms/sample - loss: 0.3349 - acc: 0.8825\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.30650\n",
      "242/242 [==============================] - 62s 255ms/step - loss: 0.2694 - acc: 0.8984 - val_loss: 0.3349 - val_acc: 0.8825\n",
      "Epoch 55/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.9044WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2037 - acc: 0.9381\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.30650 to 0.20366, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/055-0.2037.hdf5\n",
      "242/242 [==============================] - 51s 210ms/step - loss: 0.2651 - acc: 0.9044 - val_loss: 0.2037 - val_acc: 0.9381\n",
      "Epoch 56/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9014WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2542 - acc: 0.9225\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 65s 271ms/step - loss: 0.2610 - acc: 0.9016 - val_loss: 0.2542 - val_acc: 0.9225\n",
      "Epoch 57/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2524 - acc: 0.9076WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.3380 - acc: 0.8900\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 55s 228ms/step - loss: 0.2523 - acc: 0.9077 - val_loss: 0.3380 - val_acc: 0.8900\n",
      "Epoch 58/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9077WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.4729 - acc: 0.8175\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 70s 289ms/step - loss: 0.2539 - acc: 0.9077 - val_loss: 0.4729 - val_acc: 0.8175\n",
      "Epoch 59/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.9109WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 30s 9ms/sample - loss: 0.3988 - acc: 0.8675\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 71s 293ms/step - loss: 0.2490 - acc: 0.9112 - val_loss: 0.3988 - val_acc: 0.8675\n",
      "Epoch 60/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9101WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.3201 - acc: 0.8900\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 52s 216ms/step - loss: 0.2518 - acc: 0.9102 - val_loss: 0.3201 - val_acc: 0.8900\n",
      "Epoch 61/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9119WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.4585 - acc: 0.8381\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 57s 237ms/step - loss: 0.2383 - acc: 0.9119 - val_loss: 0.4585 - val_acc: 0.8381\n",
      "Epoch 62/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.9096WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.3814 - acc: 0.8625\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 64s 266ms/step - loss: 0.2544 - acc: 0.9098 - val_loss: 0.3814 - val_acc: 0.8625\n",
      "Epoch 63/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9135WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3262 - acc: 0.8991\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 49s 204ms/step - loss: 0.2343 - acc: 0.9135 - val_loss: 0.3262 - val_acc: 0.8991\n",
      "Epoch 64/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9152WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2849 - acc: 0.8981\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 53s 219ms/step - loss: 0.2285 - acc: 0.9151 - val_loss: 0.2849 - val_acc: 0.8981\n",
      "Epoch 65/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9179WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.3599 - acc: 0.87311s - loss: 0.3609 - acc: 0.\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 51s 209ms/step - loss: 0.2252 - acc: 0.9179 - val_loss: 0.3599 - val_acc: 0.8731\n",
      "Epoch 66/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9177WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.2541 - acc: 0.9194\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 59s 245ms/step - loss: 0.2286 - acc: 0.9175 - val_loss: 0.2541 - val_acc: 0.9194\n",
      "Epoch 67/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9196WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.3289 - acc: 0.8694\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.2160 - acc: 0.9194 - val_loss: 0.3289 - val_acc: 0.8694\n",
      "Epoch 68/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9196WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3112 - acc: 0.8806\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 51s 212ms/step - loss: 0.2264 - acc: 0.9194 - val_loss: 0.3112 - val_acc: 0.8806\n",
      "Epoch 69/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9161WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.2706 - acc: 0.9028\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 56s 233ms/step - loss: 0.2268 - acc: 0.9163 - val_loss: 0.2706 - val_acc: 0.9028\n",
      "Epoch 70/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9218WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.3715 - acc: 0.8775\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 61s 252ms/step - loss: 0.2142 - acc: 0.9218 - val_loss: 0.3715 - val_acc: 0.8775\n",
      "Epoch 71/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9284WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.2822 - acc: 0.8972\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 52s 216ms/step - loss: 0.2060 - acc: 0.9287 - val_loss: 0.2822 - val_acc: 0.8972\n",
      "Epoch 72/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9271- ETA: 3s -WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.3329 - acc: 0.8925\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.20366\n",
      "242/242 [==============================] - 56s 233ms/step - loss: 0.2035 - acc: 0.9272 - val_loss: 0.3329 - val_acc: 0.8925\n",
      "Epoch 73/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9264WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2026 - acc: 0.9375\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.20366 to 0.20264, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/073-0.2026.hdf5\n",
      "242/242 [==============================] - 60s 249ms/step - loss: 0.2053 - acc: 0.9259 - val_loss: 0.2026 - val_acc: 0.9375\n",
      "Epoch 74/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9292WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.3066 - acc: 0.9150\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 59s 245ms/step - loss: 0.2024 - acc: 0.9290 - val_loss: 0.3066 - val_acc: 0.9150\n",
      "Epoch 75/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9271WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2908 - acc: 0.9081\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 59s 245ms/step - loss: 0.2028 - acc: 0.9272 - val_loss: 0.2908 - val_acc: 0.9081\n",
      "Epoch 76/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9253WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2233 - acc: 0.9275\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 56s 233ms/step - loss: 0.2141 - acc: 0.9251 - val_loss: 0.2233 - val_acc: 0.9275\n",
      "Epoch 77/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9240WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2685 - acc: 0.9150\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 53s 218ms/step - loss: 0.2125 - acc: 0.9241 - val_loss: 0.2685 - val_acc: 0.9150\n",
      "Epoch 78/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9262WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2531 - acc: 0.9300\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 57s 237ms/step - loss: 0.1966 - acc: 0.9263 - val_loss: 0.2531 - val_acc: 0.9300\n",
      "Epoch 79/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9294WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.5727 - acc: 0.8100\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 49s 202ms/step - loss: 0.2008 - acc: 0.9292 - val_loss: 0.5727 - val_acc: 0.8100\n",
      "Epoch 80/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9322WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.4246 - acc: 0.8575\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 52s 216ms/step - loss: 0.1859 - acc: 0.9319 - val_loss: 0.4246 - val_acc: 0.8575\n",
      "Epoch 81/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9325WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.2261 - acc: 0.9275\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 69s 286ms/step - loss: 0.1914 - acc: 0.9323 - val_loss: 0.2261 - val_acc: 0.9275\n",
      "Epoch 82/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9298WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2662 - acc: 0.9159\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 52s 215ms/step - loss: 0.1905 - acc: 0.9300 - val_loss: 0.2662 - val_acc: 0.9159\n",
      "Epoch 83/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9334WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 30s 9ms/sample - loss: 0.2196 - acc: 0.9250\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 72s 299ms/step - loss: 0.1882 - acc: 0.9335 - val_loss: 0.2196 - val_acc: 0.9250\n",
      "Epoch 84/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9339WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2628 - acc: 0.9125\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 53s 218ms/step - loss: 0.1819 - acc: 0.9338 - val_loss: 0.2628 - val_acc: 0.9125\n",
      "Epoch 85/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9332WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.2681 - acc: 0.9225\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 67s 279ms/step - loss: 0.1884 - acc: 0.9333 - val_loss: 0.2681 - val_acc: 0.9225\n",
      "Epoch 86/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9293WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 31s 10ms/sample - loss: 0.2532 - acc: 0.9050\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 63s 261ms/step - loss: 0.1990 - acc: 0.9293 - val_loss: 0.2532 - val_acc: 0.9050\n",
      "Epoch 87/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9350WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 5ms/sample - loss: 0.3684 - acc: 0.8628\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 53s 220ms/step - loss: 0.1853 - acc: 0.9349 - val_loss: 0.3684 - val_acc: 0.8628\n",
      "Epoch 88/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9391WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.2807 - acc: 0.9009\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 55s 228ms/step - loss: 0.1750 - acc: 0.9392 - val_loss: 0.2807 - val_acc: 0.9009\n",
      "Epoch 89/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9392WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.4225 - acc: 0.8516\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 60s 249ms/step - loss: 0.1778 - acc: 0.9392 - val_loss: 0.4225 - val_acc: 0.8516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9387WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.2348 - acc: 0.9250\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 55s 228ms/step - loss: 0.1705 - acc: 0.9388 - val_loss: 0.2348 - val_acc: 0.9250\n",
      "Epoch 91/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9332WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.3486 - acc: 0.8678\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 59s 243ms/step - loss: 0.1872 - acc: 0.9331 - val_loss: 0.3486 - val_acc: 0.8678\n",
      "Epoch 92/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9400WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2555 - acc: 0.9159\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 61s 252ms/step - loss: 0.1650 - acc: 0.9402 - val_loss: 0.2555 - val_acc: 0.9159\n",
      "Epoch 93/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9428WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2180 - acc: 0.9200\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.1619 - acc: 0.9426 - val_loss: 0.2180 - val_acc: 0.9200\n",
      "Epoch 94/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9424WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3184 - acc: 0.8775\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.20264\n",
      "242/242 [==============================] - 57s 237ms/step - loss: 0.1654 - acc: 0.9426 - val_loss: 0.3184 - val_acc: 0.8775\n",
      "Epoch 95/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9406- ETA: 0s - loss: 0.1649 - acc: WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.1725 - acc: 0.9450\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.20264 to 0.17245, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/095-0.1725.hdf5\n",
      "242/242 [==============================] - 58s 241ms/step - loss: 0.1667 - acc: 0.9404 - val_loss: 0.1725 - val_acc: 0.9450\n",
      "Epoch 96/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9411WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3198/3183 [==============================] - 17s 5ms/sample - loss: 0.2844 - acc: 0.9149\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.17245\n",
      "242/242 [==============================] - 52s 214ms/step - loss: 0.1650 - acc: 0.9411 - val_loss: 0.2844 - val_acc: 0.9149\n",
      "Epoch 97/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9373WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.2547 - acc: 0.8988\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.17245\n",
      "242/242 [==============================] - 71s 292ms/step - loss: 0.1734 - acc: 0.9374 - val_loss: 0.2547 - val_acc: 0.8988\n",
      "Epoch 98/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9465WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1646 - acc: 0.9550\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.17245 to 0.16460, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/098-0.1646.hdf5\n",
      "242/242 [==============================] - 56s 233ms/step - loss: 0.1551 - acc: 0.9464 - val_loss: 0.1646 - val_acc: 0.9550\n",
      "Epoch 99/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1532 - acc: 0.9439WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2707 - acc: 0.9162\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 55s 227ms/step - loss: 0.1531 - acc: 0.9439 - val_loss: 0.2707 - val_acc: 0.9162\n",
      "Epoch 100/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9495WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.2766 - acc: 0.9225\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 57s 236ms/step - loss: 0.1443 - acc: 0.9496 - val_loss: 0.2766 - val_acc: 0.9225\n",
      "Epoch 101/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9421WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.2854 - acc: 0.9131\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 67s 276ms/step - loss: 0.1628 - acc: 0.9422 - val_loss: 0.2854 - val_acc: 0.9131\n",
      "Epoch 102/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9410WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 9ms/sample - loss: 0.2713 - acc: 0.9100\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 61s 251ms/step - loss: 0.1671 - acc: 0.9411 - val_loss: 0.2713 - val_acc: 0.9100\n",
      "Epoch 103/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9453WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.2603 - acc: 0.9250\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 54s 222ms/step - loss: 0.1579 - acc: 0.9452 - val_loss: 0.2603 - val_acc: 0.9250\n",
      "Epoch 104/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9450WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2363 - acc: 0.9375\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 0.1544 - acc: 0.9453 - val_loss: 0.2363 - val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9414WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.3569 - acc: 0.8825\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 62s 254ms/step - loss: 0.1588 - acc: 0.9412 - val_loss: 0.3569 - val_acc: 0.8825\n",
      "Epoch 106/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9440WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3239 - acc: 0.8925\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 47s 196ms/step - loss: 0.1581 - acc: 0.9441 - val_loss: 0.3239 - val_acc: 0.8925\n",
      "Epoch 107/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9498WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2804 - acc: 0.9250\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 52s 217ms/step - loss: 0.1450 - acc: 0.9499 - val_loss: 0.2804 - val_acc: 0.9250\n",
      "Epoch 108/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.9460WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2321 - acc: 0.9287\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 0.1489 - acc: 0.9461 - val_loss: 0.2321 - val_acc: 0.9287\n",
      "Epoch 109/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9475WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2564 - acc: 0.9100\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 59s 243ms/step - loss: 0.1499 - acc: 0.9477 - val_loss: 0.2564 - val_acc: 0.9100\n",
      "Epoch 110/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9498WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2550 - acc: 0.9219\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 65s 268ms/step - loss: 0.1381 - acc: 0.9493 - val_loss: 0.2550 - val_acc: 0.9219\n",
      "Epoch 111/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9446WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2783 - acc: 0.9150\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 55s 226ms/step - loss: 0.1488 - acc: 0.9445 - val_loss: 0.2783 - val_acc: 0.9150\n",
      "Epoch 112/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9451WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1797 - acc: 0.9522\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 62s 256ms/step - loss: 0.1522 - acc: 0.9451 - val_loss: 0.1797 - val_acc: 0.9522\n",
      "Epoch 113/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9525WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.2404 - acc: 0.9259\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.1376 - acc: 0.9525 - val_loss: 0.2404 - val_acc: 0.9259\n",
      "Epoch 114/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9520WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.2872 - acc: 0.9128\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 76s 316ms/step - loss: 0.1391 - acc: 0.9521 - val_loss: 0.2872 - val_acc: 0.9128\n",
      "Epoch 115/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9542WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.2168 - acc: 0.9375\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 61s 250ms/step - loss: 0.1304 - acc: 0.9540 - val_loss: 0.2168 - val_acc: 0.9375\n",
      "Epoch 116/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9496WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.3461 - acc: 0.8947\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 67s 276ms/step - loss: 0.1424 - acc: 0.9494 - val_loss: 0.3461 - val_acc: 0.8947\n",
      "Epoch 117/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9422WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 33s 10ms/sample - loss: 0.2947 - acc: 0.8941\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 64s 265ms/step - loss: 0.1482 - acc: 0.9423 - val_loss: 0.2947 - val_acc: 0.8941\n",
      "Epoch 118/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9482WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2065 - acc: 0.9472\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.16460\n",
      "242/242 [==============================] - 61s 253ms/step - loss: 0.1437 - acc: 0.9483 - val_loss: 0.2065 - val_acc: 0.9472\n",
      "Epoch 119/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9522WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.1583 - acc: 0.9591\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.16460 to 0.15832, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/119-0.1583.hdf5\n",
      "242/242 [==============================] - 75s 308ms/step - loss: 0.1386 - acc: 0.9522 - val_loss: 0.1583 - val_acc: 0.9591\n",
      "Epoch 120/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9536WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.3442 - acc: 0.8800\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.15832\n",
      "242/242 [==============================] - 66s 274ms/step - loss: 0.1353 - acc: 0.9534 - val_loss: 0.3442 - val_acc: 0.8800\n",
      "Epoch 121/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9519WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.1990 - acc: 0.9216\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.15832\n",
      "242/242 [==============================] - 64s 263ms/step - loss: 0.1423 - acc: 0.9518 - val_loss: 0.1990 - val_acc: 0.9216\n",
      "Epoch 122/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9494WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2933 - acc: 0.9081\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.15832\n",
      "242/242 [==============================] - 54s 221ms/step - loss: 0.1377 - acc: 0.9494 - val_loss: 0.2933 - val_acc: 0.9081\n",
      "Epoch 123/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9530WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.2280 - acc: 0.9366\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.15832\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.1320 - acc: 0.9529 - val_loss: 0.2280 - val_acc: 0.9366\n",
      "Epoch 124/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9530WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.2138 - acc: 0.9356\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.15832\n",
      "242/242 [==============================] - 70s 290ms/step - loss: 0.1272 - acc: 0.9531 - val_loss: 0.2138 - val_acc: 0.9356\n",
      "Epoch 125/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9539WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.1363 - acc: 0.9675\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.15832 to 0.13628, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/125-0.1363.hdf5\n",
      "242/242 [==============================] - 70s 289ms/step - loss: 0.1293 - acc: 0.9538 - val_loss: 0.1363 - val_acc: 0.9675\n",
      "Epoch 126/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9550WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 31s 10ms/sample - loss: 0.2795 - acc: 0.9175\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 71s 294ms/step - loss: 0.1250 - acc: 0.9552 - val_loss: 0.2795 - val_acc: 0.9175\n",
      "Epoch 127/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9550- ETA: 0s - loss: 0.1322 - acc: 0.954WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.1683 - acc: 0.9591\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 57s 235ms/step - loss: 0.1320 - acc: 0.9550 - val_loss: 0.1683 - val_acc: 0.9591\n",
      "Epoch 128/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9539WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2734 - acc: 0.9075\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 56s 231ms/step - loss: 0.1340 - acc: 0.9540 - val_loss: 0.2734 - val_acc: 0.9075\n",
      "Epoch 129/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9537WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 9ms/sample - loss: 0.2873 - acc: 0.9000\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 68s 279ms/step - loss: 0.1273 - acc: 0.9538 - val_loss: 0.2873 - val_acc: 0.9000\n",
      "Epoch 130/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9566WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1777 - acc: 0.9419\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 57s 234ms/step - loss: 0.1290 - acc: 0.9565 - val_loss: 0.1777 - val_acc: 0.9419\n",
      "Epoch 131/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9555WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3162 - acc: 0.8850\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 57s 236ms/step - loss: 0.1261 - acc: 0.9555 - val_loss: 0.3162 - val_acc: 0.8850\n",
      "Epoch 132/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9543WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 32s 10ms/sample - loss: 0.1372 - acc: 0.9625\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 62s 256ms/step - loss: 0.1291 - acc: 0.9543 - val_loss: 0.1372 - val_acc: 0.9625\n",
      "Epoch 133/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9576WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.1436 - acc: 0.9566\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 59s 245ms/step - loss: 0.1190 - acc: 0.9575 - val_loss: 0.1436 - val_acc: 0.9566\n",
      "Epoch 134/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9567WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 9ms/sample - loss: 0.1411 - acc: 0.9572\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 71s 292ms/step - loss: 0.1232 - acc: 0.9563 - val_loss: 0.1411 - val_acc: 0.9572\n",
      "Epoch 135/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9567WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.1864 - acc: 0.9475\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 54s 222ms/step - loss: 0.1269 - acc: 0.9569 - val_loss: 0.1864 - val_acc: 0.9475\n",
      "Epoch 136/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9550WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.2845 - acc: 0.9025\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 59s 245ms/step - loss: 0.1311 - acc: 0.9548 - val_loss: 0.2845 - val_acc: 0.9025\n",
      "Epoch 137/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9563WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.2725 - acc: 0.90500s - loss: 0.2744 - acc: 0.904\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 63s 259ms/step - loss: 0.1216 - acc: 0.9563 - val_loss: 0.2725 - val_acc: 0.9050\n",
      "Epoch 138/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9521WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1672 - acc: 0.9500\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 73s 300ms/step - loss: 0.1278 - acc: 0.9523 - val_loss: 0.1672 - val_acc: 0.9500\n",
      "Epoch 139/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9585WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.4470 - acc: 0.8341\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 60s 248ms/step - loss: 0.1151 - acc: 0.9587 - val_loss: 0.4470 - val_acc: 0.8341\n",
      "Epoch 140/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9571WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.2549 - acc: 0.9175\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 59s 243ms/step - loss: 0.1222 - acc: 0.9569 - val_loss: 0.2549 - val_acc: 0.9175\n",
      "Epoch 141/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9580WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.2545 - acc: 0.9125\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 58s 240ms/step - loss: 0.1184 - acc: 0.9580 - val_loss: 0.2545 - val_acc: 0.9125\n",
      "Epoch 142/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9592WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.1377 - acc: 0.9669\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 60s 248ms/step - loss: 0.1156 - acc: 0.9591 - val_loss: 0.1377 - val_acc: 0.9669\n",
      "Epoch 143/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9585WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1893 - acc: 0.9300\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 65s 271ms/step - loss: 0.1166 - acc: 0.9585 - val_loss: 0.1893 - val_acc: 0.9300\n",
      "Epoch 144/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9618WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.2061 - acc: 0.9306\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.1120 - acc: 0.9618 - val_loss: 0.2061 - val_acc: 0.9306\n",
      "Epoch 145/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9570WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2634 - acc: 0.9191\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 47s 194ms/step - loss: 0.1201 - acc: 0.9571 - val_loss: 0.2634 - val_acc: 0.9191\n",
      "Epoch 146/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9537WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 5ms/sample - loss: 0.2744 - acc: 0.8900\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 48s 198ms/step - loss: 0.1248 - acc: 0.9539 - val_loss: 0.2744 - val_acc: 0.8900\n",
      "Epoch 147/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9610WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.1720 - acc: 0.9525\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 66s 271ms/step - loss: 0.1092 - acc: 0.9609 - val_loss: 0.1720 - val_acc: 0.9525\n",
      "Epoch 148/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9608WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2019 - acc: 0.9256\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 52s 217ms/step - loss: 0.1049 - acc: 0.9610 - val_loss: 0.2019 - val_acc: 0.9256\n",
      "Epoch 149/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9595WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2076 - acc: 0.9319\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 56s 232ms/step - loss: 0.1151 - acc: 0.9597 - val_loss: 0.2076 - val_acc: 0.9319\n",
      "Epoch 150/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9544WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 31s 10ms/sample - loss: 0.2508 - acc: 0.9250\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 75s 311ms/step - loss: 0.1249 - acc: 0.9546 - val_loss: 0.2508 - val_acc: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9628WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1658 - acc: 0.9350\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 59s 245ms/step - loss: 0.1111 - acc: 0.9630 - val_loss: 0.1658 - val_acc: 0.9350\n",
      "Epoch 152/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9640WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.1719 - acc: 0.9394\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 56s 233ms/step - loss: 0.1078 - acc: 0.9640 - val_loss: 0.1719 - val_acc: 0.9394\n",
      "Epoch 153/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9642WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.1380 - acc: 0.9575\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 60s 248ms/step - loss: 0.1034 - acc: 0.9643 - val_loss: 0.1380 - val_acc: 0.9575\n",
      "Epoch 154/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9607WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2125 - acc: 0.9425\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 52s 213ms/step - loss: 0.1156 - acc: 0.9606 - val_loss: 0.2125 - val_acc: 0.9425\n",
      "Epoch 155/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9635WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2577 - acc: 0.9225\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 57s 235ms/step - loss: 0.1026 - acc: 0.9634 - val_loss: 0.2577 - val_acc: 0.9225\n",
      "Epoch 156/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9584WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2738 - acc: 0.9025\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 54s 222ms/step - loss: 0.1192 - acc: 0.9583 - val_loss: 0.2738 - val_acc: 0.9025\n",
      "Epoch 157/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9665WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1871 - acc: 0.9425\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 59s 242ms/step - loss: 0.0934 - acc: 0.9665 - val_loss: 0.1871 - val_acc: 0.9425\n",
      "Epoch 158/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9641WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2391 - acc: 0.9344\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 41s 169ms/step - loss: 0.1060 - acc: 0.9640 - val_loss: 0.2391 - val_acc: 0.9344\n",
      "Epoch 159/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9625WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2037 - acc: 0.9325\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 51s 211ms/step - loss: 0.1058 - acc: 0.9625 - val_loss: 0.2037 - val_acc: 0.9325\n",
      "Epoch 160/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9643- ETA: WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.1956 - acc: 0.9400\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.13628\n",
      "242/242 [==============================] - 69s 285ms/step - loss: 0.1031 - acc: 0.9642 - val_loss: 0.1956 - val_acc: 0.9400\n",
      "Epoch 161/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9633WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1184 - acc: 0.9597\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.13628 to 0.11836, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/161-0.1184.hdf5\n",
      "242/242 [==============================] - 42s 175ms/step - loss: 0.1067 - acc: 0.9635 - val_loss: 0.1184 - val_acc: 0.9597\n",
      "Epoch 162/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9625WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2253 - acc: 0.9256\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 58s 239ms/step - loss: 0.1050 - acc: 0.9625 - val_loss: 0.2253 - val_acc: 0.9256\n",
      "Epoch 163/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9641WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2727 - acc: 0.9075\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 53s 220ms/step - loss: 0.1048 - acc: 0.9639 - val_loss: 0.2727 - val_acc: 0.9075\n",
      "Epoch 164/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9640WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.2352 - acc: 0.9175\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 65s 268ms/step - loss: 0.1025 - acc: 0.9640 - val_loss: 0.2352 - val_acc: 0.9175\n",
      "Epoch 165/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9620WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.1256 - acc: 0.9650\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 54s 225ms/step - loss: 0.1055 - acc: 0.9620 - val_loss: 0.1256 - val_acc: 0.9650\n",
      "Epoch 166/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9640WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.1843 - acc: 0.9425\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 65s 267ms/step - loss: 0.1039 - acc: 0.9639 - val_loss: 0.1843 - val_acc: 0.9425\n",
      "Epoch 167/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9636WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2321 - acc: 0.9244\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 0.1053 - acc: 0.9638 - val_loss: 0.2321 - val_acc: 0.9244\n",
      "Epoch 168/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9643WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 9ms/sample - loss: 0.2274 - acc: 0.9175\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 61s 254ms/step - loss: 0.0983 - acc: 0.9643 - val_loss: 0.2274 - val_acc: 0.9175\n",
      "Epoch 169/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9647WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.2349 - acc: 0.9250\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 62s 256ms/step - loss: 0.1027 - acc: 0.9645 - val_loss: 0.2349 - val_acc: 0.9250\n",
      "Epoch 170/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9630WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 31s 10ms/sample - loss: 0.3130 - acc: 0.8975\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 65s 269ms/step - loss: 0.1040 - acc: 0.9629 - val_loss: 0.3130 - val_acc: 0.8975\n",
      "Epoch 171/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9621WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2745 - acc: 0.9200\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 57s 235ms/step - loss: 0.1102 - acc: 0.9619 - val_loss: 0.2745 - val_acc: 0.9200\n",
      "Epoch 172/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9663WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1832 - acc: 0.9419\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.1003 - acc: 0.9662 - val_loss: 0.1832 - val_acc: 0.9419\n",
      "Epoch 173/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9644WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2598 - acc: 0.9100\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 54s 222ms/step - loss: 0.0980 - acc: 0.9644 - val_loss: 0.2598 - val_acc: 0.9100\n",
      "Epoch 174/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9646WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1798 - acc: 0.9375\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 61s 253ms/step - loss: 0.1039 - acc: 0.9647 - val_loss: 0.1798 - val_acc: 0.9375\n",
      "Epoch 175/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9654WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3052 - acc: 0.9025\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.1041 - acc: 0.9655 - val_loss: 0.3052 - val_acc: 0.9025\n",
      "Epoch 176/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9655WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2729 - acc: 0.9056\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 50s 207ms/step - loss: 0.0953 - acc: 0.9655 - val_loss: 0.2729 - val_acc: 0.9056\n",
      "Epoch 177/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9637WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.1331 - acc: 0.9566\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 54s 221ms/step - loss: 0.1005 - acc: 0.9638 - val_loss: 0.1331 - val_acc: 0.9566\n",
      "Epoch 178/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9666WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.1931 - acc: 0.9375\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 58s 240ms/step - loss: 0.0959 - acc: 0.9665 - val_loss: 0.1931 - val_acc: 0.9375\n",
      "Epoch 179/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9659WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1550 - acc: 0.9372\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 58s 240ms/step - loss: 0.0941 - acc: 0.9661 - val_loss: 0.1550 - val_acc: 0.9372\n",
      "Epoch 180/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9681WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 31s 10ms/sample - loss: 0.1993 - acc: 0.9350\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 69s 283ms/step - loss: 0.0909 - acc: 0.9681 - val_loss: 0.1993 - val_acc: 0.9350\n",
      "Epoch 181/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9697WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.1741 - acc: 0.9388\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 64s 265ms/step - loss: 0.0882 - acc: 0.9695 - val_loss: 0.1741 - val_acc: 0.9388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9643WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 30s 9ms/sample - loss: 0.1938 - acc: 0.9325\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 64s 266ms/step - loss: 0.1019 - acc: 0.9643 - val_loss: 0.1938 - val_acc: 0.9325\n",
      "Epoch 183/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9656WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1635 - acc: 0.9447\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 58s 240ms/step - loss: 0.0994 - acc: 0.9657 - val_loss: 0.1635 - val_acc: 0.9447\n",
      "Epoch 184/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9692WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 30s 9ms/sample - loss: 0.2136 - acc: 0.9100\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 69s 284ms/step - loss: 0.0906 - acc: 0.9693 - val_loss: 0.2136 - val_acc: 0.9100\n",
      "Epoch 185/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9665WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.1762 - acc: 0.9375\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.11836\n",
      "242/242 [==============================] - 72s 299ms/step - loss: 0.0969 - acc: 0.9666 - val_loss: 0.1762 - val_acc: 0.9375\n",
      "Epoch 186/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9665WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.1154 - acc: 0.9600\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.11836 to 0.11539, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/186-0.1154.hdf5\n",
      "242/242 [==============================] - 51s 212ms/step - loss: 0.0916 - acc: 0.9665 - val_loss: 0.1154 - val_acc: 0.9600\n",
      "Epoch 187/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9676WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.1840 - acc: 0.9322\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 57s 235ms/step - loss: 0.0920 - acc: 0.9675 - val_loss: 0.1840 - val_acc: 0.9322\n",
      "Epoch 188/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9656WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2037 - acc: 0.9425\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 53s 220ms/step - loss: 0.0976 - acc: 0.9655 - val_loss: 0.2037 - val_acc: 0.9425\n",
      "Epoch 189/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9658WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2857 - acc: 0.8959\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 41s 168ms/step - loss: 0.1004 - acc: 0.9659 - val_loss: 0.2857 - val_acc: 0.8959\n",
      "Epoch 190/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9695WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2518 - acc: 0.9175\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 51s 210ms/step - loss: 0.0880 - acc: 0.9696 - val_loss: 0.2518 - val_acc: 0.9175\n",
      "Epoch 191/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9706WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2865 - acc: 0.8975\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 35s 144ms/step - loss: 0.0832 - acc: 0.9707 - val_loss: 0.2865 - val_acc: 0.8975\n",
      "Epoch 192/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9678WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.1470 - acc: 0.9525\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 49s 202ms/step - loss: 0.0868 - acc: 0.9679 - val_loss: 0.1470 - val_acc: 0.9525\n",
      "Epoch 193/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9656WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.1449 - acc: 0.9625\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 65s 268ms/step - loss: 0.0951 - acc: 0.9657 - val_loss: 0.1449 - val_acc: 0.9625\n",
      "Epoch 194/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9706WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3123 - acc: 0.8925\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 75s 311ms/step - loss: 0.0887 - acc: 0.9706 - val_loss: 0.3123 - val_acc: 0.8925\n",
      "Epoch 195/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9689WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1610 - acc: 0.9544\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 64s 266ms/step - loss: 0.0860 - acc: 0.9689 - val_loss: 0.1610 - val_acc: 0.9544\n",
      "Epoch 196/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9660WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.2489 - acc: 0.9225\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 73s 301ms/step - loss: 0.0985 - acc: 0.9662 - val_loss: 0.2489 - val_acc: 0.9225\n",
      "Epoch 197/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9669WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.2020 - acc: 0.9413\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 65s 270ms/step - loss: 0.0922 - acc: 0.9668 - val_loss: 0.2020 - val_acc: 0.9413\n",
      "Epoch 198/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9695WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.2226 - acc: 0.9162\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 75s 310ms/step - loss: 0.0878 - acc: 0.9695 - val_loss: 0.2226 - val_acc: 0.9162\n",
      "Epoch 199/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9722WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.2042 - acc: 0.9350\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 57s 237ms/step - loss: 0.0805 - acc: 0.9722 - val_loss: 0.2042 - val_acc: 0.9350\n",
      "Epoch 200/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9674WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.2457 - acc: 0.9325\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 75s 311ms/step - loss: 0.0930 - acc: 0.9674 - val_loss: 0.2457 - val_acc: 0.9325\n",
      "Epoch 201/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9700WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 5ms/sample - loss: 0.2454 - acc: 0.9175\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.11539\n",
      "242/242 [==============================] - 49s 203ms/step - loss: 0.0831 - acc: 0.9701 - val_loss: 0.2454 - val_acc: 0.9175\n",
      "Epoch 202/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9682WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 5ms/sample - loss: 0.0961 - acc: 0.9750\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.11539 to 0.09615, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv_checkpoint/202-0.0961.hdf5\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.0886 - acc: 0.9682 - val_loss: 0.0961 - val_acc: 0.9750\n",
      "Epoch 203/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9724WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2332 - acc: 0.9237\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.0822 - acc: 0.9724 - val_loss: 0.2332 - val_acc: 0.9237\n",
      "Epoch 204/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9692WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.1552 - acc: 0.9575\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 64s 263ms/step - loss: 0.0858 - acc: 0.9691 - val_loss: 0.1552 - val_acc: 0.9575\n",
      "Epoch 205/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9730WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1192 - acc: 0.9622\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 63s 259ms/step - loss: 0.0774 - acc: 0.9730 - val_loss: 0.1192 - val_acc: 0.9622\n",
      "Epoch 206/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9702WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.1752 - acc: 0.9350\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 60s 246ms/step - loss: 0.0857 - acc: 0.9702 - val_loss: 0.1752 - val_acc: 0.9350\n",
      "Epoch 207/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9703WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1498 - acc: 0.9475\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 53s 221ms/step - loss: 0.0856 - acc: 0.9702 - val_loss: 0.1498 - val_acc: 0.9475\n",
      "Epoch 208/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9677WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1356 - acc: 0.9625\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 54s 222ms/step - loss: 0.0885 - acc: 0.9677 - val_loss: 0.1356 - val_acc: 0.9625\n",
      "Epoch 209/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9732WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1057 - acc: 0.9625\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 51s 211ms/step - loss: 0.0780 - acc: 0.9732 - val_loss: 0.1057 - val_acc: 0.9625\n",
      "Epoch 210/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9726WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.1502 - acc: 0.9506\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 49s 201ms/step - loss: 0.0813 - acc: 0.9727 - val_loss: 0.1502 - val_acc: 0.9506\n",
      "Epoch 211/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9712WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2120 - acc: 0.9450\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 54s 223ms/step - loss: 0.0783 - acc: 0.9711 - val_loss: 0.2120 - val_acc: 0.9450\n",
      "Epoch 212/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9703WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1458 - acc: 0.9575\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 52s 213ms/step - loss: 0.0872 - acc: 0.9702 - val_loss: 0.1458 - val_acc: 0.9575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9701WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.1883 - acc: 0.9500\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 60s 247ms/step - loss: 0.0871 - acc: 0.9701 - val_loss: 0.1883 - val_acc: 0.9500\n",
      "Epoch 214/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9699WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1692 - acc: 0.9450\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 59s 243ms/step - loss: 0.0829 - acc: 0.9700 - val_loss: 0.1692 - val_acc: 0.9450\n",
      "Epoch 215/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9687WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.1602 - acc: 0.9500\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 54s 221ms/step - loss: 0.0830 - acc: 0.9689 - val_loss: 0.1602 - val_acc: 0.9500\n",
      "Epoch 216/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9715WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.1425 - acc: 0.9525\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 61s 253ms/step - loss: 0.0809 - acc: 0.9714 - val_loss: 0.1425 - val_acc: 0.9525\n",
      "Epoch 217/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9685WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1451 - acc: 0.9444\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 57s 237ms/step - loss: 0.0872 - acc: 0.9682 - val_loss: 0.1451 - val_acc: 0.9444\n",
      "Epoch 218/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9731WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2073 - acc: 0.9394\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 54s 224ms/step - loss: 0.0777 - acc: 0.9728 - val_loss: 0.2073 - val_acc: 0.9394\n",
      "Epoch 219/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9730WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.1332 - acc: 0.9619\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.0775 - acc: 0.9730 - val_loss: 0.1332 - val_acc: 0.9619\n",
      "Epoch 220/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9752WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2178 - acc: 0.9225\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 53s 220ms/step - loss: 0.0729 - acc: 0.9752 - val_loss: 0.2178 - val_acc: 0.9225\n",
      "Epoch 221/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9718WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1461 - acc: 0.9475\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 54s 221ms/step - loss: 0.0861 - acc: 0.9717 - val_loss: 0.1461 - val_acc: 0.9475\n",
      "Epoch 222/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9714WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.1800 - acc: 0.9491\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 48s 200ms/step - loss: 0.0820 - acc: 0.9714 - val_loss: 0.1800 - val_acc: 0.9491\n",
      "Epoch 223/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9727WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1950 - acc: 0.9425\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 59s 244ms/step - loss: 0.0801 - acc: 0.9724 - val_loss: 0.1950 - val_acc: 0.9425\n",
      "Epoch 224/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9709WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2023 - acc: 0.9306\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 58s 239ms/step - loss: 0.0824 - acc: 0.9709 - val_loss: 0.2023 - val_acc: 0.9306\n",
      "Epoch 225/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9711WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2796 - acc: 0.9184\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 48s 197ms/step - loss: 0.0842 - acc: 0.9710 - val_loss: 0.2796 - val_acc: 0.9184\n",
      "Epoch 226/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9718WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.1658 - acc: 0.9650\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 57s 235ms/step - loss: 0.0811 - acc: 0.9720 - val_loss: 0.1658 - val_acc: 0.9650\n",
      "Epoch 227/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9754WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.2069 - acc: 0.9425\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 67s 276ms/step - loss: 0.0724 - acc: 0.9754 - val_loss: 0.2069 - val_acc: 0.9425\n",
      "Epoch 228/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9723WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1549 - acc: 0.9513\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 72s 299ms/step - loss: 0.0783 - acc: 0.9724 - val_loss: 0.1549 - val_acc: 0.9513\n",
      "Epoch 229/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9714WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.1851 - acc: 0.9475\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 56s 232ms/step - loss: 0.0804 - acc: 0.9714 - val_loss: 0.1851 - val_acc: 0.9475\n",
      "Epoch 230/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9711WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.1779 - acc: 0.9469\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 74s 308ms/step - loss: 0.0811 - acc: 0.9711 - val_loss: 0.1779 - val_acc: 0.9469\n",
      "Epoch 231/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9736WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.1803 - acc: 0.9438\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 60s 248ms/step - loss: 0.0780 - acc: 0.9736 - val_loss: 0.1803 - val_acc: 0.9438\n",
      "Epoch 232/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9720WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1084 - acc: 0.9600\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 61s 254ms/step - loss: 0.0791 - acc: 0.9719 - val_loss: 0.1084 - val_acc: 0.9600\n",
      "Epoch 233/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9723WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1425 - acc: 0.9556\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 56s 233ms/step - loss: 0.0812 - acc: 0.9724 - val_loss: 0.1425 - val_acc: 0.9556\n",
      "Epoch 234/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9720WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2384 - acc: 0.9275\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 49s 201ms/step - loss: 0.0777 - acc: 0.9720 - val_loss: 0.2384 - val_acc: 0.9275\n",
      "Epoch 235/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9744WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 33s 10ms/sample - loss: 0.1895 - acc: 0.9325\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 63s 260ms/step - loss: 0.0751 - acc: 0.9745 - val_loss: 0.1895 - val_acc: 0.9325\n",
      "Epoch 236/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9704WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.1638 - acc: 0.9400\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 0.0817 - acc: 0.9704 - val_loss: 0.1638 - val_acc: 0.9400\n",
      "Epoch 237/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9722WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.1370 - acc: 0.9650\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 0.0813 - acc: 0.9721 - val_loss: 0.1370 - val_acc: 0.9650\n",
      "Epoch 238/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9750WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2019 - acc: 0.9328\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 0.0745 - acc: 0.9750 - val_loss: 0.2019 - val_acc: 0.9328\n",
      "Epoch 239/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9736WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.1171 - acc: 0.9625\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 58s 240ms/step - loss: 0.0763 - acc: 0.9736 - val_loss: 0.1171 - val_acc: 0.9625\n",
      "Epoch 240/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9716WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 31s 10ms/sample - loss: 0.1994 - acc: 0.9325\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 66s 274ms/step - loss: 0.0782 - acc: 0.9718 - val_loss: 0.1994 - val_acc: 0.9325\n",
      "Epoch 241/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9736WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.2451 - acc: 0.9100\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 63s 261ms/step - loss: 0.0751 - acc: 0.9737 - val_loss: 0.2451 - val_acc: 0.9100\n",
      "Epoch 242/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9755WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3198/3183 [==============================] - 19s 6ms/sample - loss: 0.2143 - acc: 0.9400\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 50s 206ms/step - loss: 0.0694 - acc: 0.9756 - val_loss: 0.2143 - val_acc: 0.9400\n",
      "Epoch 243/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9744WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.1789 - acc: 0.9425\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 56s 232ms/step - loss: 0.0705 - acc: 0.9744 - val_loss: 0.1789 - val_acc: 0.9425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9732WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.1264 - acc: 0.9575\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 59s 242ms/step - loss: 0.0787 - acc: 0.9732 - val_loss: 0.1264 - val_acc: 0.9575\n",
      "Epoch 245/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9739WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.2258 - acc: 0.9375\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 59s 242ms/step - loss: 0.0711 - acc: 0.9737 - val_loss: 0.2258 - val_acc: 0.9375\n",
      "Epoch 246/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9757WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2690 - acc: 0.9028\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 58s 241ms/step - loss: 0.0707 - acc: 0.9758 - val_loss: 0.2690 - val_acc: 0.9028\n",
      "Epoch 247/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9741WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2217 - acc: 0.9325\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 60s 247ms/step - loss: 0.0757 - acc: 0.9741 - val_loss: 0.2217 - val_acc: 0.9325\n",
      "Epoch 248/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9759WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1862 - acc: 0.9425\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 54s 224ms/step - loss: 0.0725 - acc: 0.9760 - val_loss: 0.1862 - val_acc: 0.9425\n",
      "Epoch 249/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9778WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.1311 - acc: 0.9675\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 54s 222ms/step - loss: 0.0682 - acc: 0.9779 - val_loss: 0.1311 - val_acc: 0.9675\n",
      "Epoch 250/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9736WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.1308 - acc: 0.9575\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.0742 - acc: 0.9737 - val_loss: 0.1308 - val_acc: 0.9575\n",
      "Epoch 251/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9735WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.1757 - acc: 0.9513\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 56s 233ms/step - loss: 0.0743 - acc: 0.9734 - val_loss: 0.1757 - val_acc: 0.9513\n",
      "Epoch 252/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9720WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1948 - acc: 0.9425\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.09615\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 0.0790 - acc: 0.9721 - val_loss: 0.1948 - val_acc: 0.9425\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3183 [==============================] - 4s 1ms/sample - loss: 0.1987 - acc: 0.9259\n",
      "Loss: 0.19873026720746448 Accuracy: 0.9258561\n",
      "\n",
      "Epoch 1/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.4714 - acc: 0.3753WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 1.6209 - acc: 0.2906\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.62086, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/001-1.6209.hdf5\n",
      "242/242 [==============================] - 60s 250ms/step - loss: 1.4695 - acc: 0.3766 - val_loss: 1.6209 - val_acc: 0.2906\n",
      "Epoch 2/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.2125 - acc: 0.5141WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 1.0697 - acc: 0.6100\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.62086 to 1.06971, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/002-1.0697.hdf5\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 1.2124 - acc: 0.5145 - val_loss: 1.0697 - val_acc: 0.6100\n",
      "Epoch 3/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.0655 - acc: 0.5847WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 1.3923 - acc: 0.4175\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.06971\n",
      "242/242 [==============================] - 55s 226ms/step - loss: 1.0648 - acc: 0.5848 - val_loss: 1.3923 - val_acc: 0.4175\n",
      "Epoch 4/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.9601 - acc: 0.6375WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.9223 - acc: 0.6300\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.06971 to 0.92234, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/004-0.9223.hdf5\n",
      "242/242 [==============================] - 58s 240ms/step - loss: 0.9595 - acc: 0.6377 - val_loss: 0.9223 - val_acc: 0.6300\n",
      "Epoch 5/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8503 - acc: 0.6734WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.9360 - acc: 0.6300\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.92234\n",
      "242/242 [==============================] - 51s 212ms/step - loss: 0.8487 - acc: 0.6742 - val_loss: 0.9360 - val_acc: 0.6300\n",
      "Epoch 6/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7991 - acc: 0.6937WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.8758 - acc: 0.6850\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.92234 to 0.87585, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/006-0.8758.hdf5\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.7981 - acc: 0.6940 - val_loss: 0.8758 - val_acc: 0.6850\n",
      "Epoch 7/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7224 - acc: 0.7213WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.5852 - acc: 0.8056\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.87585 to 0.58519, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/007-0.5852.hdf5\n",
      "242/242 [==============================] - 51s 210ms/step - loss: 0.7239 - acc: 0.7211 - val_loss: 0.5852 - val_acc: 0.8056\n",
      "Epoch 8/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6956 - acc: 0.7282WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.9590 - acc: 0.6500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58519\n",
      "242/242 [==============================] - 54s 223ms/step - loss: 0.6951 - acc: 0.7285 - val_loss: 0.9590 - val_acc: 0.6500\n",
      "Epoch 9/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6593 - acc: 0.7493WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.6917 - acc: 0.7575\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.58519\n",
      "242/242 [==============================] - 61s 254ms/step - loss: 0.6600 - acc: 0.7492 - val_loss: 0.6917 - val_acc: 0.7575\n",
      "Epoch 10/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6135 - acc: 0.7678WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.8177 - acc: 0.6959\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.58519\n",
      "242/242 [==============================] - 57s 236ms/step - loss: 0.6138 - acc: 0.7675 - val_loss: 0.8177 - val_acc: 0.6959\n",
      "Epoch 11/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5985 - acc: 0.7751WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.6115 - acc: 0.7775\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.58519\n",
      "242/242 [==============================] - 69s 285ms/step - loss: 0.5990 - acc: 0.7750 - val_loss: 0.6115 - val_acc: 0.7775\n",
      "Epoch 12/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5805 - acc: 0.7818WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.6135 - acc: 0.7828\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.58519\n",
      "242/242 [==============================] - 60s 248ms/step - loss: 0.5800 - acc: 0.7821 - val_loss: 0.6135 - val_acc: 0.7828\n",
      "Epoch 13/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.7897WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.3672 - acc: 0.8897\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.58519 to 0.36717, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/013-0.3672.hdf5\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.5578 - acc: 0.7898 - val_loss: 0.3672 - val_acc: 0.8897\n",
      "Epoch 14/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.7926WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.7476 - acc: 0.7300\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.36717\n",
      "242/242 [==============================] - 56s 232ms/step - loss: 0.5349 - acc: 0.7928 - val_loss: 0.7476 - val_acc: 0.7300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.8021WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.7146 - acc: 0.7362\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.36717\n",
      "242/242 [==============================] - 55s 228ms/step - loss: 0.5186 - acc: 0.8019 - val_loss: 0.7146 - val_acc: 0.7362\n",
      "Epoch 16/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4984 - acc: 0.8076WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.5498 - acc: 0.8075\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.36717\n",
      "242/242 [==============================] - 54s 224ms/step - loss: 0.4995 - acc: 0.8071 - val_loss: 0.5498 - val_acc: 0.8075\n",
      "Epoch 17/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4979 - acc: 0.8110WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.6947 - acc: 0.7572\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.36717\n",
      "242/242 [==============================] - 55s 229ms/step - loss: 0.4974 - acc: 0.8111 - val_loss: 0.6947 - val_acc: 0.7572\n",
      "Epoch 18/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4751 - acc: 0.8199WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.8926 - acc: 0.6756\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.36717\n",
      "242/242 [==============================] - 53s 220ms/step - loss: 0.4742 - acc: 0.8203 - val_loss: 0.8926 - val_acc: 0.6756\n",
      "Epoch 19/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8259WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.6563 - acc: 0.7425\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.36717\n",
      "242/242 [==============================] - 61s 252ms/step - loss: 0.4563 - acc: 0.8258 - val_loss: 0.6563 - val_acc: 0.7425\n",
      "Epoch 20/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4487 - acc: 0.8324WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.3060 - acc: 0.8959\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.36717 to 0.30599, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/020-0.3060.hdf5\n",
      "242/242 [==============================] - 53s 218ms/step - loss: 0.4490 - acc: 0.8322 - val_loss: 0.3060 - val_acc: 0.8959\n",
      "Epoch 21/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4351 - acc: 0.8344WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.4933 - acc: 0.8284\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.30599\n",
      "242/242 [==============================] - 51s 209ms/step - loss: 0.4355 - acc: 0.8340 - val_loss: 0.4933 - val_acc: 0.8284\n",
      "Epoch 22/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8462WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.4917 - acc: 0.8175\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.30599\n",
      "242/242 [==============================] - 64s 265ms/step - loss: 0.4142 - acc: 0.8463 - val_loss: 0.4917 - val_acc: 0.8175\n",
      "Epoch 23/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4119 - acc: 0.8424WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.5411 - acc: 0.7794\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.30599\n",
      "242/242 [==============================] - 49s 202ms/step - loss: 0.4126 - acc: 0.8421 - val_loss: 0.5411 - val_acc: 0.7794\n",
      "Epoch 24/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.8453WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.7420 - acc: 0.7275\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.30599\n",
      "242/242 [==============================] - 54s 224ms/step - loss: 0.3952 - acc: 0.8452 - val_loss: 0.7420 - val_acc: 0.7275\n",
      "Epoch 25/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8546WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.6692 - acc: 0.7513\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.30599\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.3859 - acc: 0.8549 - val_loss: 0.6692 - val_acc: 0.7513\n",
      "Epoch 26/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3811 - acc: 0.8569WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.6987 - acc: 0.7425\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.30599\n",
      "242/242 [==============================] - 58s 239ms/step - loss: 0.3812 - acc: 0.8567 - val_loss: 0.6987 - val_acc: 0.7425\n",
      "Epoch 27/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3760 - acc: 0.8578WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.3559 - acc: 0.8825\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.30599\n",
      "242/242 [==============================] - 52s 217ms/step - loss: 0.3759 - acc: 0.8577 - val_loss: 0.3559 - val_acc: 0.8825\n",
      "Epoch 28/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3595 - acc: 0.8645WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3271 - acc: 0.8875\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.30599\n",
      "242/242 [==============================] - 57s 234ms/step - loss: 0.3603 - acc: 0.8643 - val_loss: 0.3271 - val_acc: 0.8875\n",
      "Epoch 29/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.8700WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.4502 - acc: 0.8500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.30599\n",
      "242/242 [==============================] - 51s 212ms/step - loss: 0.3425 - acc: 0.8700 - val_loss: 0.4502 - val_acc: 0.8500\n",
      "Epoch 30/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.8770WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.2282 - acc: 0.9250\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.30599 to 0.22822, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/030-0.2282.hdf5\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 0.3310 - acc: 0.8770 - val_loss: 0.2282 - val_acc: 0.9250\n",
      "Epoch 31/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8793WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2448 - acc: 0.9075\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.22822\n",
      "242/242 [==============================] - 51s 212ms/step - loss: 0.3216 - acc: 0.8794 - val_loss: 0.2448 - val_acc: 0.9075\n",
      "Epoch 32/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.8846WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.4234 - acc: 0.8350\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.22822\n",
      "242/242 [==============================] - 54s 222ms/step - loss: 0.3126 - acc: 0.8846 - val_loss: 0.4234 - val_acc: 0.8350\n",
      "Epoch 33/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.8775WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.3175 - acc: 0.8913\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.22822\n",
      "242/242 [==============================] - 55s 228ms/step - loss: 0.3190 - acc: 0.8778 - val_loss: 0.3175 - val_acc: 0.8913\n",
      "Epoch 34/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.8865WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3198/3183 [==============================] - 20s 6ms/sample - loss: 0.3331 - acc: 0.8865\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.22822\n",
      "242/242 [==============================] - 62s 256ms/step - loss: 0.3059 - acc: 0.8865 - val_loss: 0.3331 - val_acc: 0.8865\n",
      "Epoch 35/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.8940WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.2383 - acc: 0.9175\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.22822\n",
      "242/242 [==============================] - 56s 231ms/step - loss: 0.2886 - acc: 0.8943 - val_loss: 0.2383 - val_acc: 0.9175\n",
      "Epoch 36/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.8925WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.3079 - acc: 0.9025\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.22822\n",
      "242/242 [==============================] - 61s 251ms/step - loss: 0.2897 - acc: 0.8923 - val_loss: 0.3079 - val_acc: 0.9025\n",
      "Epoch 37/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.8967WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.4744 - acc: 0.8225\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.22822\n",
      "242/242 [==============================] - 51s 213ms/step - loss: 0.2789 - acc: 0.8969 - val_loss: 0.4744 - val_acc: 0.8225\n",
      "Epoch 38/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.8948WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3455 - acc: 0.8650\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.22822\n",
      "242/242 [==============================] - 59s 242ms/step - loss: 0.2844 - acc: 0.8949 - val_loss: 0.3455 - val_acc: 0.8650\n",
      "Epoch 39/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.8987WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2251 - acc: 0.9362\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.22822 to 0.22511, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/039-0.2251.hdf5\n",
      "242/242 [==============================] - 58s 241ms/step - loss: 0.2721 - acc: 0.8988 - val_loss: 0.2251 - val_acc: 0.9362\n",
      "Epoch 40/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9077WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.4800 - acc: 0.8256\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 54s 223ms/step - loss: 0.2532 - acc: 0.9076 - val_loss: 0.4800 - val_acc: 0.8256\n",
      "Epoch 41/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9042WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.3473 - acc: 0.8750\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 61s 253ms/step - loss: 0.2583 - acc: 0.9043 - val_loss: 0.3473 - val_acc: 0.8750\n",
      "Epoch 42/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9065WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.5032 - acc: 0.8050\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 74s 308ms/step - loss: 0.2589 - acc: 0.9065 - val_loss: 0.5032 - val_acc: 0.8050\n",
      "Epoch 43/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9115WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.3752 - acc: 0.8612\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 68s 283ms/step - loss: 0.2378 - acc: 0.9114 - val_loss: 0.3752 - val_acc: 0.8612\n",
      "Epoch 44/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9102WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.3839 - acc: 0.8450\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 54s 224ms/step - loss: 0.2389 - acc: 0.9103 - val_loss: 0.3839 - val_acc: 0.8450\n",
      "Epoch 45/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9171WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.4044 - acc: 0.8625\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 61s 251ms/step - loss: 0.2216 - acc: 0.9173 - val_loss: 0.4044 - val_acc: 0.8625\n",
      "Epoch 46/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9187WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.5001 - acc: 0.8197\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 56s 231ms/step - loss: 0.2326 - acc: 0.9188 - val_loss: 0.5001 - val_acc: 0.8197\n",
      "Epoch 47/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9133WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.4106 - acc: 0.8550\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 47s 196ms/step - loss: 0.2355 - acc: 0.9132 - val_loss: 0.4106 - val_acc: 0.8550\n",
      "Epoch 48/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9162WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2760 - acc: 0.9181\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 53s 220ms/step - loss: 0.2308 - acc: 0.9162 - val_loss: 0.2760 - val_acc: 0.9181\n",
      "Epoch 49/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9147WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.2914 - acc: 0.8900\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 71s 295ms/step - loss: 0.2295 - acc: 0.9150 - val_loss: 0.2914 - val_acc: 0.8900\n",
      "Epoch 50/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9212WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.3089 - acc: 0.8931\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 54s 223ms/step - loss: 0.2226 - acc: 0.9215 - val_loss: 0.3089 - val_acc: 0.8931\n",
      "Epoch 51/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9231WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2833 - acc: 0.9025\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 57s 236ms/step - loss: 0.2137 - acc: 0.9231 - val_loss: 0.2833 - val_acc: 0.9025\n",
      "Epoch 52/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9242WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2394 - acc: 0.9291\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 53s 217ms/step - loss: 0.2079 - acc: 0.9243 - val_loss: 0.2394 - val_acc: 0.9291\n",
      "Epoch 53/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9267WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.4049 - acc: 0.8600\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.1973 - acc: 0.9264 - val_loss: 0.4049 - val_acc: 0.8600\n",
      "Epoch 54/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9255WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2695 - acc: 0.8975\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 62s 256ms/step - loss: 0.2061 - acc: 0.9254 - val_loss: 0.2695 - val_acc: 0.8975\n",
      "Epoch 55/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9311WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 33s 10ms/sample - loss: 0.3689 - acc: 0.8725\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 75s 309ms/step - loss: 0.1948 - acc: 0.9309 - val_loss: 0.3689 - val_acc: 0.8725\n",
      "Epoch 56/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9350WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.4029 - acc: 0.8575\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 64s 263ms/step - loss: 0.1854 - acc: 0.9350 - val_loss: 0.4029 - val_acc: 0.8575\n",
      "Epoch 57/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9296WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2303 - acc: 0.9425\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.22511\n",
      "242/242 [==============================] - 64s 265ms/step - loss: 0.1912 - acc: 0.9298 - val_loss: 0.2303 - val_acc: 0.9425\n",
      "Epoch 58/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9322WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.2230 - acc: 0.9272\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.22511 to 0.22297, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/058-0.2230.hdf5\n",
      "242/242 [==============================] - 52s 214ms/step - loss: 0.1986 - acc: 0.9318 - val_loss: 0.2230 - val_acc: 0.9272\n",
      "Epoch 59/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9287WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.3024 - acc: 0.8800\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.22297\n",
      "242/242 [==============================] - 59s 245ms/step - loss: 0.1936 - acc: 0.9289 - val_loss: 0.3024 - val_acc: 0.8800\n",
      "Epoch 60/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9367WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.2678 - acc: 0.9075\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.22297\n",
      "242/242 [==============================] - 62s 256ms/step - loss: 0.1719 - acc: 0.9369 - val_loss: 0.2678 - val_acc: 0.9075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9364WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.3004 - acc: 0.8825\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.22297\n",
      "242/242 [==============================] - 68s 282ms/step - loss: 0.1801 - acc: 0.9364 - val_loss: 0.3004 - val_acc: 0.8825\n",
      "Epoch 62/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9456WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 31s 10ms/sample - loss: 0.3282 - acc: 0.8825\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.22297\n",
      "242/242 [==============================] - 63s 258ms/step - loss: 0.1653 - acc: 0.9455 - val_loss: 0.3282 - val_acc: 0.8825\n",
      "Epoch 63/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9396WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.2758 - acc: 0.9050\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.22297\n",
      "242/242 [==============================] - 71s 294ms/step - loss: 0.1689 - acc: 0.9398 - val_loss: 0.2758 - val_acc: 0.9050\n",
      "Epoch 64/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9369WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2857 - acc: 0.8875\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.22297\n",
      "242/242 [==============================] - 65s 270ms/step - loss: 0.1701 - acc: 0.9371 - val_loss: 0.2857 - val_acc: 0.8875\n",
      "Epoch 65/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9431WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.2185 - acc: 0.9250\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.22297 to 0.21853, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/065-0.2185.hdf5\n",
      "242/242 [==============================] - 60s 247ms/step - loss: 0.1619 - acc: 0.9431 - val_loss: 0.2185 - val_acc: 0.9250\n",
      "Epoch 66/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9424WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2679 - acc: 0.9019\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.21853\n",
      "242/242 [==============================] - 56s 233ms/step - loss: 0.1632 - acc: 0.9427 - val_loss: 0.2679 - val_acc: 0.9019\n",
      "Epoch 67/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9417WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2273 - acc: 0.9100\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.21853\n",
      "242/242 [==============================] - 64s 263ms/step - loss: 0.1578 - acc: 0.9419 - val_loss: 0.2273 - val_acc: 0.9100\n",
      "Epoch 68/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9408WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 35s 11ms/sample - loss: 0.5512 - acc: 0.7728\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.21853\n",
      "242/242 [==============================] - 70s 289ms/step - loss: 0.1568 - acc: 0.9409 - val_loss: 0.5512 - val_acc: 0.7728\n",
      "Epoch 69/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9450WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.1953 - acc: 0.9375\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.21853 to 0.19534, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/069-0.1953.hdf5\n",
      "242/242 [==============================] - 61s 250ms/step - loss: 0.1573 - acc: 0.9451 - val_loss: 0.1953 - val_acc: 0.9375\n",
      "Epoch 70/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9450WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.3449 - acc: 0.8925\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.19534\n",
      "242/242 [==============================] - 52s 217ms/step - loss: 0.1501 - acc: 0.9452 - val_loss: 0.3449 - val_acc: 0.8925\n",
      "Epoch 71/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9493WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3402 - acc: 0.8700\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.19534\n",
      "242/242 [==============================] - 67s 277ms/step - loss: 0.1480 - acc: 0.9494 - val_loss: 0.3402 - val_acc: 0.8700\n",
      "Epoch 72/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9469WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.3679 - acc: 0.8628\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19534\n",
      "242/242 [==============================] - 51s 210ms/step - loss: 0.1512 - acc: 0.9466 - val_loss: 0.3679 - val_acc: 0.8628\n",
      "Epoch 73/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9460WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1480 - acc: 0.9569\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.19534 to 0.14804, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/073-0.1480.hdf5\n",
      "242/242 [==============================] - 58s 241ms/step - loss: 0.1502 - acc: 0.9461 - val_loss: 0.1480 - val_acc: 0.9569\n",
      "Epoch 74/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9465WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1983 - acc: 0.9200\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.14804\n",
      "242/242 [==============================] - 57s 235ms/step - loss: 0.1495 - acc: 0.9461 - val_loss: 0.1983 - val_acc: 0.9200\n",
      "Epoch 75/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9477WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.3455 - acc: 0.8700\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.14804\n",
      "242/242 [==============================] - 63s 261ms/step - loss: 0.1502 - acc: 0.9477 - val_loss: 0.3455 - val_acc: 0.8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9470WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1955 - acc: 0.9400\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.14804\n",
      "242/242 [==============================] - 50s 205ms/step - loss: 0.1509 - acc: 0.9469 - val_loss: 0.1955 - val_acc: 0.9400\n",
      "Epoch 77/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9516WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.4582 - acc: 0.8475\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.14804\n",
      "242/242 [==============================] - 52s 215ms/step - loss: 0.1340 - acc: 0.9515 - val_loss: 0.4582 - val_acc: 0.8475\n",
      "Epoch 78/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9530WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.3110 - acc: 0.8844\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.14804\n",
      "242/242 [==============================] - 63s 259ms/step - loss: 0.1394 - acc: 0.9531 - val_loss: 0.3110 - val_acc: 0.8844\n",
      "Epoch 79/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9523WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3198/3183 [==============================] - 19s 6ms/sample - loss: 0.2830 - acc: 0.8990\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.14804\n",
      "242/242 [==============================] - 59s 242ms/step - loss: 0.1383 - acc: 0.9522 - val_loss: 0.2830 - val_acc: 0.8990\n",
      "Epoch 80/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9528WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.3927 - acc: 0.8775\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.14804\n",
      "242/242 [==============================] - 54s 223ms/step - loss: 0.1303 - acc: 0.9528 - val_loss: 0.3927 - val_acc: 0.8775\n",
      "Epoch 81/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9525WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.1887 - acc: 0.9400\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.14804\n",
      "242/242 [==============================] - 66s 271ms/step - loss: 0.1371 - acc: 0.9523 - val_loss: 0.1887 - val_acc: 0.9400\n",
      "Epoch 82/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9562WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3198/3183 [==============================] - 20s 6ms/sample - loss: 0.4357 - acc: 0.8468\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.14804\n",
      "242/242 [==============================] - 51s 212ms/step - loss: 0.1249 - acc: 0.9561 - val_loss: 0.4357 - val_acc: 0.8468\n",
      "Epoch 83/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9520WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1431 - acc: 0.9506\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.14804 to 0.14308, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/083-0.1431.hdf5\n",
      "242/242 [==============================] - 67s 275ms/step - loss: 0.1320 - acc: 0.9519 - val_loss: 0.1431 - val_acc: 0.9506\n",
      "Epoch 84/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9567WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2600 - acc: 0.8950\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 59s 243ms/step - loss: 0.1209 - acc: 0.9565 - val_loss: 0.2600 - val_acc: 0.8950\n",
      "Epoch 85/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9541WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.3536 - acc: 0.8775\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 74s 305ms/step - loss: 0.1281 - acc: 0.9542 - val_loss: 0.3536 - val_acc: 0.8775\n",
      "Epoch 86/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9544WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.3391 - acc: 0.8975\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 54s 224ms/step - loss: 0.1307 - acc: 0.9545 - val_loss: 0.3391 - val_acc: 0.8975\n",
      "Epoch 87/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9602WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.2661 - acc: 0.8884\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.1157 - acc: 0.9602 - val_loss: 0.2661 - val_acc: 0.8884\n",
      "Epoch 88/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9558WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.1543 - acc: 0.9400\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 61s 253ms/step - loss: 0.1231 - acc: 0.9559 - val_loss: 0.1543 - val_acc: 0.9400\n",
      "Epoch 89/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9595WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2169 - acc: 0.9228\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 86s 356ms/step - loss: 0.1257 - acc: 0.9593 - val_loss: 0.2169 - val_acc: 0.9228\n",
      "Epoch 90/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9567WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.3254 - acc: 0.8725\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 52s 216ms/step - loss: 0.1261 - acc: 0.9568 - val_loss: 0.3254 - val_acc: 0.8725\n",
      "Epoch 91/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9561WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.1794 - acc: 0.9369\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 57s 237ms/step - loss: 0.1188 - acc: 0.9561 - val_loss: 0.1794 - val_acc: 0.9369\n",
      "Epoch 92/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9569WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.1512 - acc: 0.9500\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 66s 274ms/step - loss: 0.1192 - acc: 0.9568 - val_loss: 0.1512 - val_acc: 0.9500\n",
      "Epoch 93/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9615WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3582 - acc: 0.8606\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 94s 387ms/step - loss: 0.1074 - acc: 0.9615 - val_loss: 0.3582 - val_acc: 0.8606\n",
      "Epoch 94/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9615WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2229 - acc: 0.9303\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 69s 286ms/step - loss: 0.1084 - acc: 0.9615 - val_loss: 0.2229 - val_acc: 0.9303\n",
      "Epoch 95/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9620WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 30s 10ms/sample - loss: 0.2658 - acc: 0.9075\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 63s 259ms/step - loss: 0.1084 - acc: 0.9621 - val_loss: 0.2658 - val_acc: 0.9075\n",
      "Epoch 96/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9561WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1666 - acc: 0.9375\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 54s 224ms/step - loss: 0.1216 - acc: 0.9559 - val_loss: 0.1666 - val_acc: 0.9375\n",
      "Epoch 97/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9599WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2598 - acc: 0.905015s -\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 55s 226ms/step - loss: 0.1116 - acc: 0.9600 - val_loss: 0.2598 - val_acc: 0.9050\n",
      "Epoch 98/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9585WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3198/3183 [==============================] - 20s 6ms/sample - loss: 0.3339 - acc: 0.8915\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 51s 210ms/step - loss: 0.1142 - acc: 0.9582 - val_loss: 0.3339 - val_acc: 0.8915\n",
      "Epoch 99/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9599WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1790 - acc: 0.9366\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 54s 223ms/step - loss: 0.1180 - acc: 0.9599 - val_loss: 0.1790 - val_acc: 0.9366\n",
      "Epoch 100/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9620WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.3016 - acc: 0.8850\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 49s 203ms/step - loss: 0.1125 - acc: 0.9621 - val_loss: 0.3016 - val_acc: 0.8850\n",
      "Epoch 101/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9602WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.1930 - acc: 0.9350\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 68s 280ms/step - loss: 0.1130 - acc: 0.9601 - val_loss: 0.1930 - val_acc: 0.9350\n",
      "Epoch 102/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9578WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2907 - acc: 0.9000\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 51s 210ms/step - loss: 0.1135 - acc: 0.9578 - val_loss: 0.2907 - val_acc: 0.9000\n",
      "Epoch 103/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9615WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.2108 - acc: 0.9200\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 57s 238ms/step - loss: 0.1003 - acc: 0.9615 - val_loss: 0.2108 - val_acc: 0.9200\n",
      "Epoch 104/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9600WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.3616 - acc: 0.8687\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 62s 258ms/step - loss: 0.1082 - acc: 0.9601 - val_loss: 0.3616 - val_acc: 0.8687\n",
      "Epoch 105/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9587WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.1614 - acc: 0.9450\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 64s 265ms/step - loss: 0.1100 - acc: 0.9586 - val_loss: 0.1614 - val_acc: 0.9450\n",
      "Epoch 106/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9669WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2242 - acc: 0.9169\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 57s 234ms/step - loss: 0.0937 - acc: 0.9669 - val_loss: 0.2242 - val_acc: 0.9169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9625WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2235 - acc: 0.9175\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 58s 239ms/step - loss: 0.1054 - acc: 0.9624 - val_loss: 0.2235 - val_acc: 0.9175\n",
      "Epoch 108/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9629WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.4264 - acc: 0.8425\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 60s 247ms/step - loss: 0.1037 - acc: 0.9627 - val_loss: 0.4264 - val_acc: 0.8425\n",
      "Epoch 109/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9638WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2410 - acc: 0.9041\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 56s 232ms/step - loss: 0.1000 - acc: 0.9639 - val_loss: 0.2410 - val_acc: 0.9041\n",
      "Epoch 110/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9664- ETA: 2s - loss: 0WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2143 - acc: 0.9128\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.14308\n",
      "242/242 [==============================] - 53s 218ms/step - loss: 0.0964 - acc: 0.9666 - val_loss: 0.2143 - val_acc: 0.9128\n",
      "Epoch 111/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9668WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.1155 - acc: 0.9525\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.14308 to 0.11550, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/111-0.1155.hdf5\n",
      "242/242 [==============================] - 77s 317ms/step - loss: 0.0948 - acc: 0.9666 - val_loss: 0.1155 - val_acc: 0.9525\n",
      "Epoch 112/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9672WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1774 - acc: 0.9350\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 62s 254ms/step - loss: 0.1007 - acc: 0.9673 - val_loss: 0.1774 - val_acc: 0.9350\n",
      "Epoch 113/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9669WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2953 - acc: 0.8800\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 51s 213ms/step - loss: 0.0981 - acc: 0.9668 - val_loss: 0.2953 - val_acc: 0.8800\n",
      "Epoch 114/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9683WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2107 - acc: 0.9175\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 40s 165ms/step - loss: 0.0922 - acc: 0.9682 - val_loss: 0.2107 - val_acc: 0.9175\n",
      "Epoch 115/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9653WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1763 - acc: 0.9481\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 54s 223ms/step - loss: 0.0971 - acc: 0.9654 - val_loss: 0.1763 - val_acc: 0.9481\n",
      "Epoch 116/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9682WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2091 - acc: 0.9300\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 51s 213ms/step - loss: 0.0930 - acc: 0.9682 - val_loss: 0.2091 - val_acc: 0.9300\n",
      "Epoch 117/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9687WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1461 - acc: 0.9500\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 38s 156ms/step - loss: 0.0898 - acc: 0.9688 - val_loss: 0.1461 - val_acc: 0.9500\n",
      "Epoch 118/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9700WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 33s 10ms/sample - loss: 0.2528 - acc: 0.9081\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 65s 269ms/step - loss: 0.0875 - acc: 0.9699 - val_loss: 0.2528 - val_acc: 0.9081\n",
      "Epoch 119/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9671WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2083 - acc: 0.9237\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 56s 232ms/step - loss: 0.0887 - acc: 0.9671 - val_loss: 0.2083 - val_acc: 0.9237\n",
      "Epoch 120/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9700WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.4322 - acc: 0.8419\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 65s 270ms/step - loss: 0.0828 - acc: 0.9698 - val_loss: 0.4322 - val_acc: 0.8419\n",
      "Epoch 121/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9640WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.1502 - acc: 0.9475\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 65s 268ms/step - loss: 0.0953 - acc: 0.9641 - val_loss: 0.1502 - val_acc: 0.9475\n",
      "Epoch 122/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/242 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9692WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.2665 - acc: 0.9200\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 79s 326ms/step - loss: 0.0922 - acc: 0.9692 - val_loss: 0.2665 - val_acc: 0.9200\n",
      "Epoch 123/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9714WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1166 - acc: 0.9634\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 56s 233ms/step - loss: 0.0929 - acc: 0.9714 - val_loss: 0.1166 - val_acc: 0.9634\n",
      "Epoch 124/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9668WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1349 - acc: 0.9650\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 51s 210ms/step - loss: 0.0899 - acc: 0.9667 - val_loss: 0.1349 - val_acc: 0.9650\n",
      "Epoch 125/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9691WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 31s 10ms/sample - loss: 0.2636 - acc: 0.9150\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.0889 - acc: 0.9692 - val_loss: 0.2636 - val_acc: 0.9150\n",
      "Epoch 126/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9648WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2948 - acc: 0.8988\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 52s 216ms/step - loss: 0.0963 - acc: 0.9649 - val_loss: 0.2948 - val_acc: 0.8988\n",
      "Epoch 127/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9695WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.3273 - acc: 0.8691\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 0.0891 - acc: 0.9696 - val_loss: 0.3273 - val_acc: 0.8691\n",
      "Epoch 128/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9690WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3002 - acc: 0.8950\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 54s 225ms/step - loss: 0.0847 - acc: 0.9692 - val_loss: 0.3002 - val_acc: 0.8950\n",
      "Epoch 129/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9696WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.3034 - acc: 0.8928\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 61s 250ms/step - loss: 0.0901 - acc: 0.9693 - val_loss: 0.3034 - val_acc: 0.8928\n",
      "Epoch 130/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9705WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.1659 - acc: 0.9400\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 62s 255ms/step - loss: 0.0842 - acc: 0.9705 - val_loss: 0.1659 - val_acc: 0.9400\n",
      "Epoch 131/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9689WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1323 - acc: 0.9553\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 53s 218ms/step - loss: 0.0873 - acc: 0.9689 - val_loss: 0.1323 - val_acc: 0.9553\n",
      "Epoch 132/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9686WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1492 - acc: 0.9550\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 57s 237ms/step - loss: 0.0848 - acc: 0.9687 - val_loss: 0.1492 - val_acc: 0.9550\n",
      "Epoch 133/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9736WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.3848 - acc: 0.8619\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 57s 235ms/step - loss: 0.0807 - acc: 0.9736 - val_loss: 0.3848 - val_acc: 0.8619\n",
      "Epoch 134/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9692WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.2502 - acc: 0.9075\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 58s 238ms/step - loss: 0.0892 - acc: 0.9694 - val_loss: 0.2502 - val_acc: 0.9075\n",
      "Epoch 135/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9694WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1666 - acc: 0.9447\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 53s 217ms/step - loss: 0.0866 - acc: 0.9695 - val_loss: 0.1666 - val_acc: 0.9447\n",
      "Epoch 136/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9748WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.1819 - acc: 0.9300\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 50s 206ms/step - loss: 0.0763 - acc: 0.9747 - val_loss: 0.1819 - val_acc: 0.9300\n",
      "Epoch 137/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9709WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.1423 - acc: 0.9563\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 48s 199ms/step - loss: 0.0851 - acc: 0.9709 - val_loss: 0.1423 - val_acc: 0.9563\n",
      "Epoch 138/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9700WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2491 - acc: 0.9031\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 47s 196ms/step - loss: 0.0848 - acc: 0.9699 - val_loss: 0.2491 - val_acc: 0.9031\n",
      "Epoch 139/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9758WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.3659 - acc: 0.8800\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 47s 196ms/step - loss: 0.0720 - acc: 0.9758 - val_loss: 0.3659 - val_acc: 0.8800\n",
      "Epoch 140/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9689WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2401 - acc: 0.9166\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 45s 186ms/step - loss: 0.0859 - acc: 0.9690 - val_loss: 0.2401 - val_acc: 0.9166\n",
      "Epoch 141/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9703WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2763 - acc: 0.8975\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 44s 180ms/step - loss: 0.0870 - acc: 0.9702 - val_loss: 0.2763 - val_acc: 0.8975\n",
      "Epoch 142/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9720WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1728 - acc: 0.9444\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 49s 205ms/step - loss: 0.0774 - acc: 0.9721 - val_loss: 0.1728 - val_acc: 0.9444\n",
      "Epoch 143/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9748WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2087 - acc: 0.9250\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 43s 177ms/step - loss: 0.0723 - acc: 0.9747 - val_loss: 0.2087 - val_acc: 0.9250\n",
      "Epoch 144/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9737WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2319 - acc: 0.9131\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 40s 165ms/step - loss: 0.0789 - acc: 0.9738 - val_loss: 0.2319 - val_acc: 0.9131\n",
      "Epoch 145/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9761WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2396 - acc: 0.9194\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 46s 190ms/step - loss: 0.0708 - acc: 0.9761 - val_loss: 0.2396 - val_acc: 0.9194\n",
      "Epoch 146/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9722WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1521 - acc: 0.9600\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 39s 163ms/step - loss: 0.0785 - acc: 0.9722 - val_loss: 0.1521 - val_acc: 0.9600\n",
      "Epoch 147/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9730WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.1981 - acc: 0.9219\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 47s 192ms/step - loss: 0.0819 - acc: 0.9731 - val_loss: 0.1981 - val_acc: 0.9219\n",
      "Epoch 148/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9715WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.2166 - acc: 0.9200\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 46s 189ms/step - loss: 0.0758 - acc: 0.9714 - val_loss: 0.2166 - val_acc: 0.9200\n",
      "Epoch 149/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9737WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.1735 - acc: 0.9425\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 50s 209ms/step - loss: 0.0763 - acc: 0.9737 - val_loss: 0.1735 - val_acc: 0.9425\n",
      "Epoch 150/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9735WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 34s 11ms/sample - loss: 0.1570 - acc: 0.9550\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 62s 256ms/step - loss: 0.0761 - acc: 0.9734 - val_loss: 0.1570 - val_acc: 0.9550\n",
      "Epoch 151/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9752WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1553 - acc: 0.9438\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 67s 276ms/step - loss: 0.0709 - acc: 0.9753 - val_loss: 0.1553 - val_acc: 0.9438\n",
      "Epoch 152/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9748WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3198/3183 [==============================] - 41s 13ms/sample - loss: 0.2789 - acc: 0.9040\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.11550\n",
      "242/242 [==============================] - 59s 243ms/step - loss: 0.0757 - acc: 0.9748 - val_loss: 0.2789 - val_acc: 0.9040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9716WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 52s 16ms/sample - loss: 0.1155 - acc: 0.9575\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.11550 to 0.11547, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/153-0.1155.hdf5\n",
      "242/242 [==============================] - 78s 324ms/step - loss: 0.0782 - acc: 0.9717 - val_loss: 0.1155 - val_acc: 0.9575\n",
      "Epoch 154/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9758WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.1699 - acc: 0.9459\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.11547\n",
      "242/242 [==============================] - 68s 282ms/step - loss: 0.0741 - acc: 0.9759 - val_loss: 0.1699 - val_acc: 0.9459\n",
      "Epoch 155/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9754WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.0999 - acc: 0.9734\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.11547 to 0.09993, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv_checkpoint/155-0.0999.hdf5\n",
      "242/242 [==============================] - 65s 268ms/step - loss: 0.0635 - acc: 0.9754 - val_loss: 0.0999 - val_acc: 0.9734\n",
      "Epoch 156/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9714WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1891 - acc: 0.9297\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 68s 283ms/step - loss: 0.0813 - acc: 0.9714 - val_loss: 0.1891 - val_acc: 0.9297\n",
      "Epoch 157/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9776WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1726 - acc: 0.9400\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 64s 263ms/step - loss: 0.0624 - acc: 0.9777 - val_loss: 0.1726 - val_acc: 0.9400\n",
      "Epoch 158/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9753WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1627 - acc: 0.9350\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 65s 271ms/step - loss: 0.0679 - acc: 0.9753 - val_loss: 0.1627 - val_acc: 0.9350\n",
      "Epoch 159/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9745WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2418 - acc: 0.9228\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 69s 285ms/step - loss: 0.0748 - acc: 0.9745 - val_loss: 0.2418 - val_acc: 0.9228\n",
      "Epoch 160/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9759- ETA: 0s - loss: 0.0643 - acc:WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 15ms/sample - loss: 0.1303 - acc: 0.9516\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 64s 266ms/step - loss: 0.0641 - acc: 0.9760 - val_loss: 0.1303 - val_acc: 0.9516\n",
      "Epoch 161/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9744WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1571 - acc: 0.9334\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 62s 257ms/step - loss: 0.0717 - acc: 0.9745 - val_loss: 0.1571 - val_acc: 0.9334\n",
      "Epoch 162/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9743WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2199 - acc: 0.9209\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.0690 - acc: 0.9744 - val_loss: 0.2199 - val_acc: 0.9209\n",
      "Epoch 163/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9743WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1554 - acc: 0.9375\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 63s 261ms/step - loss: 0.0772 - acc: 0.9742 - val_loss: 0.1554 - val_acc: 0.9375\n",
      "Epoch 164/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9762WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.2236 - acc: 0.9038\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 65s 267ms/step - loss: 0.0624 - acc: 0.9761 - val_loss: 0.2236 - val_acc: 0.9038\n",
      "Epoch 165/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9752WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 52s 16ms/sample - loss: 0.2448 - acc: 0.9200\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 69s 285ms/step - loss: 0.0690 - acc: 0.9752 - val_loss: 0.2448 - val_acc: 0.9200\n",
      "Epoch 166/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9782WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 5ms/sample - loss: 0.1707 - acc: 0.9459\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 66s 272ms/step - loss: 0.0631 - acc: 0.9782 - val_loss: 0.1707 - val_acc: 0.9459\n",
      "Epoch 167/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9744WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.1264 - acc: 0.9556\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 67s 275ms/step - loss: 0.0739 - acc: 0.9745 - val_loss: 0.1264 - val_acc: 0.9556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9783WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1540 - acc: 0.9444\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 65s 268ms/step - loss: 0.0635 - acc: 0.9782 - val_loss: 0.1540 - val_acc: 0.9444\n",
      "Epoch 169/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9742WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2407 - acc: 0.9125\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 65s 269ms/step - loss: 0.0663 - acc: 0.9743 - val_loss: 0.2407 - val_acc: 0.9125\n",
      "Epoch 170/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9717WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.1263 - acc: 0.9600\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 65s 269ms/step - loss: 0.0770 - acc: 0.9718 - val_loss: 0.1263 - val_acc: 0.9600\n",
      "Epoch 171/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9782WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1369 - acc: 0.9522\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 75s 309ms/step - loss: 0.0633 - acc: 0.9782 - val_loss: 0.1369 - val_acc: 0.9522\n",
      "Epoch 172/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9764WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3263 - acc: 0.8875\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 71s 292ms/step - loss: 0.0661 - acc: 0.9765 - val_loss: 0.3263 - val_acc: 0.8875\n",
      "Epoch 173/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9778WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3141 - acc: 0.8872\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.0631 - acc: 0.9777 - val_loss: 0.3141 - val_acc: 0.8872\n",
      "Epoch 174/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9764WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 15ms/sample - loss: 0.2339 - acc: 0.9300\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 68s 283ms/step - loss: 0.0694 - acc: 0.9764 - val_loss: 0.2339 - val_acc: 0.9300\n",
      "Epoch 175/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9797WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1693 - acc: 0.9550\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.0599 - acc: 0.9797 - val_loss: 0.1693 - val_acc: 0.9550\n",
      "Epoch 176/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9781WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.1773 - acc: 0.9450\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.0616 - acc: 0.9782 - val_loss: 0.1773 - val_acc: 0.9450\n",
      "Epoch 177/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9763WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1707 - acc: 0.9456\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 71s 294ms/step - loss: 0.0644 - acc: 0.9763 - val_loss: 0.1707 - val_acc: 0.9456\n",
      "Epoch 178/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9785WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2007 - acc: 0.9337\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 71s 292ms/step - loss: 0.0605 - acc: 0.9786 - val_loss: 0.2007 - val_acc: 0.9337\n",
      "Epoch 179/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9767WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.3002 - acc: 0.8938\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 69s 284ms/step - loss: 0.0652 - acc: 0.9766 - val_loss: 0.3002 - val_acc: 0.8938\n",
      "Epoch 180/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9782WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1095 - acc: 0.9675\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 73s 301ms/step - loss: 0.0595 - acc: 0.9783 - val_loss: 0.1095 - val_acc: 0.9675\n",
      "Epoch 181/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9749WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.1339 - acc: 0.9475\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.0716 - acc: 0.9750 - val_loss: 0.1339 - val_acc: 0.9475\n",
      "Epoch 182/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9772WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.1522 - acc: 0.9525\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 67s 277ms/step - loss: 0.0628 - acc: 0.9770 - val_loss: 0.1522 - val_acc: 0.9525\n",
      "Epoch 183/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9796WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.1275 - acc: 0.9575\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 66s 272ms/step - loss: 0.0585 - acc: 0.9797 - val_loss: 0.1275 - val_acc: 0.9575\n",
      "Epoch 184/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9777WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2511 - acc: 0.8900\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 62s 258ms/step - loss: 0.0651 - acc: 0.9776 - val_loss: 0.2511 - val_acc: 0.8900\n",
      "Epoch 185/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9782- ETA: 2s WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.2003 - acc: 0.9275\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 66s 272ms/step - loss: 0.0597 - acc: 0.9782 - val_loss: 0.2003 - val_acc: 0.9275\n",
      "Epoch 186/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9776WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.1675 - acc: 0.9341\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 68s 280ms/step - loss: 0.0639 - acc: 0.9777 - val_loss: 0.1675 - val_acc: 0.9341\n",
      "Epoch 187/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9784WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.2018 - acc: 0.9244\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 68s 280ms/step - loss: 0.0574 - acc: 0.9784 - val_loss: 0.2018 - val_acc: 0.9244\n",
      "Epoch 188/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9771WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.1706 - acc: 0.9625\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 74s 306ms/step - loss: 0.0667 - acc: 0.9771 - val_loss: 0.1706 - val_acc: 0.9625\n",
      "Epoch 189/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9738WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 49s 15ms/sample - loss: 0.1572 - acc: 0.9475\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 71s 294ms/step - loss: 0.0747 - acc: 0.9737 - val_loss: 0.1572 - val_acc: 0.9475\n",
      "Epoch 190/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9832WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.1690 - acc: 0.9475\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 72s 299ms/step - loss: 0.0538 - acc: 0.9831 - val_loss: 0.1690 - val_acc: 0.9475\n",
      "Epoch 191/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9771WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2544 - acc: 0.9056\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 65s 270ms/step - loss: 0.0667 - acc: 0.9770 - val_loss: 0.2544 - val_acc: 0.9056\n",
      "Epoch 192/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9818WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.2640 - acc: 0.9025\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 76s 315ms/step - loss: 0.0541 - acc: 0.9819 - val_loss: 0.2640 - val_acc: 0.9025\n",
      "Epoch 193/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9778WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1237 - acc: 0.9600\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 71s 295ms/step - loss: 0.0620 - acc: 0.9778 - val_loss: 0.1237 - val_acc: 0.9600\n",
      "Epoch 194/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9828WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2249 - acc: 0.9300\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 67s 279ms/step - loss: 0.0512 - acc: 0.9828 - val_loss: 0.2249 - val_acc: 0.9300\n",
      "Epoch 195/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9775WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 16ms/sample - loss: 0.2529 - acc: 0.9103\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 68s 282ms/step - loss: 0.0600 - acc: 0.9776 - val_loss: 0.2529 - val_acc: 0.9103\n",
      "Epoch 196/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9758WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.1532 - acc: 0.9450\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 65s 269ms/step - loss: 0.0650 - acc: 0.9758 - val_loss: 0.1532 - val_acc: 0.9450\n",
      "Epoch 197/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9797WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 43s 13ms/sample - loss: 0.1487 - acc: 0.9556\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 64s 265ms/step - loss: 0.0597 - acc: 0.9798 - val_loss: 0.1487 - val_acc: 0.9556\n",
      "Epoch 198/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9798WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.2392 - acc: 0.9100\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 68s 282ms/step - loss: 0.0569 - acc: 0.9799 - val_loss: 0.2392 - val_acc: 0.9100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9786WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.1549 - acc: 0.9475\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 64s 263ms/step - loss: 0.0580 - acc: 0.9787 - val_loss: 0.1549 - val_acc: 0.9475\n",
      "Epoch 200/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9783WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 43s 14ms/sample - loss: 0.1715 - acc: 0.9375\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 63s 258ms/step - loss: 0.0653 - acc: 0.9783 - val_loss: 0.1715 - val_acc: 0.9375\n",
      "Epoch 201/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9816WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.2185 - acc: 0.9300\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 64s 266ms/step - loss: 0.0532 - acc: 0.9816 - val_loss: 0.2185 - val_acc: 0.9300\n",
      "Epoch 202/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9786WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.3246 - acc: 0.9000\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 64s 266ms/step - loss: 0.0645 - acc: 0.9786 - val_loss: 0.3246 - val_acc: 0.9000\n",
      "Epoch 203/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9777WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.2871 - acc: 0.8897\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 63s 259ms/step - loss: 0.0609 - acc: 0.9778 - val_loss: 0.2871 - val_acc: 0.8897\n",
      "Epoch 204/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9789WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.1505 - acc: 0.9488\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 66s 273ms/step - loss: 0.0593 - acc: 0.9789 - val_loss: 0.1505 - val_acc: 0.9488\n",
      "Epoch 205/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9792WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 54s 17ms/sample - loss: 0.1772 - acc: 0.9391\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.09993\n",
      "242/242 [==============================] - 71s 295ms/step - loss: 0.0573 - acc: 0.9792 - val_loss: 0.1772 - val_acc: 0.9391\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3183 [==============================] - 4s 1ms/sample - loss: 0.2345 - acc: 0.9243\n",
      "Loss: 0.23450051145549095 Accuracy: 0.9242853\n",
      "\n",
      "Epoch 1/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.4623 - acc: 0.3773WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 1.4063 - acc: 0.3850\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.40632, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/001-1.4063.hdf5\n",
      "242/242 [==============================] - 73s 302ms/step - loss: 1.4615 - acc: 0.3778 - val_loss: 1.4063 - val_acc: 0.3850\n",
      "Epoch 2/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.1885 - acc: 0.5261WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 1.2435 - acc: 0.4875\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.40632 to 1.24345, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/002-1.2435.hdf5\n",
      "242/242 [==============================] - 72s 298ms/step - loss: 1.1871 - acc: 0.5267 - val_loss: 1.2435 - val_acc: 0.4875\n",
      "Epoch 3/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.9959 - acc: 0.6072WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 51s 16ms/sample - loss: 1.3263 - acc: 0.4863\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.24345\n",
      "242/242 [==============================] - 76s 313ms/step - loss: 0.9954 - acc: 0.6074 - val_loss: 1.3263 - val_acc: 0.4863\n",
      "Epoch 4/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8727 - acc: 0.6611WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.9042 - acc: 0.6550\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.24345 to 0.90423, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/004-0.9042.hdf5\n",
      "242/242 [==============================] - 75s 311ms/step - loss: 0.8722 - acc: 0.6613 - val_loss: 0.9042 - val_acc: 0.6550\n",
      "Epoch 5/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8065 - acc: 0.6911WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.5011 - acc: 0.8306\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.90423 to 0.50105, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/005-0.5011.hdf5\n",
      "242/242 [==============================] - 78s 324ms/step - loss: 0.8053 - acc: 0.6917 - val_loss: 0.5011 - val_acc: 0.8306\n",
      "Epoch 6/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7459 - acc: 0.7103WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.8087 - acc: 0.6938\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.50105\n",
      "242/242 [==============================] - 81s 336ms/step - loss: 0.7466 - acc: 0.7101 - val_loss: 0.8087 - val_acc: 0.6938\n",
      "Epoch 7/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6936 - acc: 0.7335WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 1.3425 - acc: 0.4828\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.50105\n",
      "242/242 [==============================] - 69s 285ms/step - loss: 0.6931 - acc: 0.7335 - val_loss: 1.3425 - val_acc: 0.4828\n",
      "Epoch 8/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6542 - acc: 0.7509WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 16ms/sample - loss: 0.8082 - acc: 0.7088\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.50105\n",
      "242/242 [==============================] - 84s 347ms/step - loss: 0.6536 - acc: 0.7511 - val_loss: 0.8082 - val_acc: 0.7088\n",
      "Epoch 9/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6401 - acc: 0.7513WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 16ms/sample - loss: 0.4258 - acc: 0.8675\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50105 to 0.42582, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/009-0.4258.hdf5\n",
      "242/242 [==============================] - 77s 318ms/step - loss: 0.6403 - acc: 0.7511 - val_loss: 0.4258 - val_acc: 0.8675\n",
      "Epoch 10/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5951 - acc: 0.7642WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 49s 15ms/sample - loss: 0.8899 - acc: 0.6672\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.42582\n",
      "242/242 [==============================] - 71s 292ms/step - loss: 0.5952 - acc: 0.7640 - val_loss: 0.8899 - val_acc: 0.6672\n",
      "Epoch 11/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5652 - acc: 0.7870WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.6412 - acc: 0.7744\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.42582\n",
      "242/242 [==============================] - 68s 282ms/step - loss: 0.5656 - acc: 0.7871 - val_loss: 0.6412 - val_acc: 0.7744\n",
      "Epoch 12/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.7851WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.6915 - acc: 0.7475\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.42582\n",
      "242/242 [==============================] - 66s 274ms/step - loss: 0.5557 - acc: 0.7847 - val_loss: 0.6915 - val_acc: 0.7475\n",
      "Epoch 13/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.7932WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.9055 - acc: 0.6600\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.42582\n",
      "242/242 [==============================] - 68s 281ms/step - loss: 0.5323 - acc: 0.7930 - val_loss: 0.9055 - val_acc: 0.6600\n",
      "Epoch 14/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5055 - acc: 0.8089WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.8611 - acc: 0.6872\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.42582\n",
      "242/242 [==============================] - 72s 297ms/step - loss: 0.5058 - acc: 0.8087 - val_loss: 0.8611 - val_acc: 0.6872\n",
      "Epoch 15/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/242 [============================>.] - ETA: 0s - loss: 0.4884 - acc: 0.8174WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3429 - acc: 0.8838\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.42582 to 0.34285, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/015-0.3429.hdf5\n",
      "242/242 [==============================] - 70s 287ms/step - loss: 0.4890 - acc: 0.8171 - val_loss: 0.3429 - val_acc: 0.8838\n",
      "Epoch 16/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4710 - acc: 0.8206WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.4814 - acc: 0.8334\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.34285\n",
      "242/242 [==============================] - 70s 290ms/step - loss: 0.4703 - acc: 0.8206 - val_loss: 0.4814 - val_acc: 0.8334\n",
      "Epoch 17/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4400 - acc: 0.8339WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 43s 14ms/sample - loss: 0.7039 - acc: 0.7378\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.34285\n",
      "242/242 [==============================] - 64s 266ms/step - loss: 0.4395 - acc: 0.8341 - val_loss: 0.7039 - val_acc: 0.7378\n",
      "Epoch 18/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.8348WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.8598 - acc: 0.6725\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.34285\n",
      "242/242 [==============================] - 70s 288ms/step - loss: 0.4351 - acc: 0.8350 - val_loss: 0.8598 - val_acc: 0.6725\n",
      "Epoch 19/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8487WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.4153 - acc: 0.8416\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.34285\n",
      "242/242 [==============================] - 66s 275ms/step - loss: 0.4002 - acc: 0.8491 - val_loss: 0.4153 - val_acc: 0.8416\n",
      "Epoch 20/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8486WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.7758 - acc: 0.7550\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.34285\n",
      "242/242 [==============================] - 66s 274ms/step - loss: 0.3940 - acc: 0.8485 - val_loss: 0.7758 - val_acc: 0.7550\n",
      "Epoch 21/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8544WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 17ms/sample - loss: 0.6654 - acc: 0.7681\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.34285\n",
      "242/242 [==============================] - 75s 309ms/step - loss: 0.3840 - acc: 0.8544 - val_loss: 0.6654 - val_acc: 0.7681\n",
      "Epoch 22/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3598 - acc: 0.8634WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.3498 - acc: 0.8575\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.34285\n",
      "242/242 [==============================] - 72s 296ms/step - loss: 0.3602 - acc: 0.8634 - val_loss: 0.3498 - val_acc: 0.8575\n",
      "Epoch 23/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3543 - acc: 0.8669WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.3346 - acc: 0.8900 may duplicate your\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.34285 to 0.33457, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/023-0.3346.hdf5\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.3547 - acc: 0.8665 - val_loss: 0.3346 - val_acc: 0.8900\n",
      "Epoch 24/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8690WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 43s 13ms/sample - loss: 0.3091 - acc: 0.8775\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.33457 to 0.30913, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/024-0.3091.hdf5\n",
      "242/242 [==============================] - 66s 273ms/step - loss: 0.3416 - acc: 0.8689 - val_loss: 0.3091 - val_acc: 0.8775\n",
      "Epoch 25/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3318 - acc: 0.8758WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.3514 - acc: 0.8681\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.30913\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.3317 - acc: 0.8760 - val_loss: 0.3514 - val_acc: 0.8681\n",
      "Epoch 26/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8782WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 43s 14ms/sample - loss: 0.3700 - acc: 0.8700\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.30913\n",
      "242/242 [==============================] - 65s 268ms/step - loss: 0.3181 - acc: 0.8783 - val_loss: 0.3700 - val_acc: 0.8700\n",
      "Epoch 27/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8769WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.3550 - acc: 0.8691\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.30913\n",
      "242/242 [==============================] - 68s 282ms/step - loss: 0.3250 - acc: 0.8770 - val_loss: 0.3550 - val_acc: 0.8691\n",
      "Epoch 28/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2945 - acc: 0.8881WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.5465 - acc: 0.78566s - loss: 0.4579\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.30913\n",
      "242/242 [==============================] - 67s 276ms/step - loss: 0.2953 - acc: 0.8880 - val_loss: 0.5465 - val_acc: 0.7856\n",
      "Epoch 29/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.8928WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.4820 - acc: 0.8150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.30913\n",
      "242/242 [==============================] - 70s 288ms/step - loss: 0.2855 - acc: 0.8928 - val_loss: 0.4820 - val_acc: 0.8150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.8914WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3198/3183 [==============================] - 48s 15ms/sample - loss: 0.6600 - acc: 0.7677\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.30913\n",
      "242/242 [==============================] - 69s 285ms/step - loss: 0.2875 - acc: 0.8914 - val_loss: 0.6600 - val_acc: 0.7677\n",
      "Epoch 31/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.8974WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 41s 13ms/sample - loss: 0.6238 - acc: 0.7675\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.30913\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.2777 - acc: 0.8971 - val_loss: 0.6238 - val_acc: 0.7675\n",
      "Epoch 32/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9013WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.3581 - acc: 0.8756\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.30913\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.2674 - acc: 0.9015 - val_loss: 0.3581 - val_acc: 0.8756\n",
      "Epoch 33/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.9067WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.3429 - acc: 0.8550\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.30913\n",
      "242/242 [==============================] - 74s 307ms/step - loss: 0.2502 - acc: 0.9067 - val_loss: 0.3429 - val_acc: 0.8550\n",
      "Epoch 34/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9072WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.3093 - acc: 0.8894\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.30913\n",
      "242/242 [==============================] - 67s 275ms/step - loss: 0.2487 - acc: 0.9071 - val_loss: 0.3093 - val_acc: 0.8894\n",
      "Epoch 35/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9091WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.6054 - acc: 0.7725\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.30913\n",
      "242/242 [==============================] - 70s 288ms/step - loss: 0.2464 - acc: 0.9089 - val_loss: 0.6054 - val_acc: 0.7725\n",
      "Epoch 36/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9145WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.2716 - acc: 0.9128\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.30913 to 0.27158, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/036-0.2716.hdf5\n",
      "242/242 [==============================] - 68s 280ms/step - loss: 0.2348 - acc: 0.9144 - val_loss: 0.2716 - val_acc: 0.9128\n",
      "Epoch 37/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9120WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.4021 - acc: 0.8575\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.27158\n",
      "242/242 [==============================] - 68s 280ms/step - loss: 0.2357 - acc: 0.9123 - val_loss: 0.4021 - val_acc: 0.8575\n",
      "Epoch 38/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9195WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.4106 - acc: 0.8444\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.27158\n",
      "242/242 [==============================] - 72s 297ms/step - loss: 0.2173 - acc: 0.9196 - val_loss: 0.4106 - val_acc: 0.8444\n",
      "Epoch 39/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9187WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.3021 - acc: 0.8819\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.27158\n",
      "242/242 [==============================] - 71s 291ms/step - loss: 0.2147 - acc: 0.9184 - val_loss: 0.3021 - val_acc: 0.8819\n",
      "Epoch 40/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9262WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2660 - acc: 0.9044\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.27158 to 0.26595, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/040-0.2660.hdf5\n",
      "242/242 [==============================] - 70s 287ms/step - loss: 0.2069 - acc: 0.9263 - val_loss: 0.2660 - val_acc: 0.9044\n",
      "Epoch 41/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9197WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2450 - acc: 0.9150\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.26595 to 0.24497, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/041-0.2450.hdf5\n",
      "242/242 [==============================] - 71s 295ms/step - loss: 0.2215 - acc: 0.9194 - val_loss: 0.2450 - val_acc: 0.9150\n",
      "Epoch 42/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9281WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.4401 - acc: 0.8425\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.24497\n",
      "242/242 [==============================] - 72s 298ms/step - loss: 0.1951 - acc: 0.9281 - val_loss: 0.4401 - val_acc: 0.8425\n",
      "Epoch 43/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9257WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.2856 - acc: 0.9000\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.24497\n",
      "242/242 [==============================] - 71s 294ms/step - loss: 0.1976 - acc: 0.9258 - val_loss: 0.2856 - val_acc: 0.9000\n",
      "Epoch 44/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9303WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2266 - acc: 0.9200\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.24497 to 0.22660, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/044-0.2266.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 68s 279ms/step - loss: 0.1892 - acc: 0.9305 - val_loss: 0.2266 - val_acc: 0.9200\n",
      "Epoch 45/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9313WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.3378 - acc: 0.8675\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.22660\n",
      "242/242 [==============================] - 65s 268ms/step - loss: 0.1894 - acc: 0.9312 - val_loss: 0.3378 - val_acc: 0.8675\n",
      "Epoch 46/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9313WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3283 - acc: 0.8925\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.22660\n",
      "242/242 [==============================] - 68s 281ms/step - loss: 0.1827 - acc: 0.9316 - val_loss: 0.3283 - val_acc: 0.8925\n",
      "Epoch 47/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9341WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 51s 16ms/sample - loss: 0.4947 - acc: 0.8022\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.22660\n",
      "242/242 [==============================] - 73s 300ms/step - loss: 0.1824 - acc: 0.9344 - val_loss: 0.4947 - val_acc: 0.8022\n",
      "Epoch 48/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9369WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3092 - acc: 0.8991\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.22660\n",
      "242/242 [==============================] - 73s 301ms/step - loss: 0.1653 - acc: 0.9370 - val_loss: 0.3092 - val_acc: 0.8991\n",
      "Epoch 49/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1681 - acc: 0.9387WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3057 - acc: 0.8925\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.22660\n",
      "242/242 [==============================] - 65s 270ms/step - loss: 0.1681 - acc: 0.9388 - val_loss: 0.3057 - val_acc: 0.8925\n",
      "Epoch 50/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9409WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.4342 - acc: 0.8469\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.22660\n",
      "242/242 [==============================] - 67s 276ms/step - loss: 0.1623 - acc: 0.9408 - val_loss: 0.4342 - val_acc: 0.8469\n",
      "Epoch 51/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9413WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3559 - acc: 0.8647\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.22660\n",
      "242/242 [==============================] - 70s 291ms/step - loss: 0.1614 - acc: 0.9409 - val_loss: 0.3559 - val_acc: 0.8647\n",
      "Epoch 52/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9444WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.2603 - acc: 0.8994\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.22660\n",
      "242/242 [==============================] - 68s 282ms/step - loss: 0.1562 - acc: 0.9445 - val_loss: 0.2603 - val_acc: 0.8994\n",
      "Epoch 53/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9447WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.2523 - acc: 0.9100\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.22660\n",
      "242/242 [==============================] - 70s 288ms/step - loss: 0.1535 - acc: 0.9449 - val_loss: 0.2523 - val_acc: 0.9100\n",
      "Epoch 54/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9454WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.3703 - acc: 0.8744\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.22660\n",
      "242/242 [==============================] - 70s 290ms/step - loss: 0.1502 - acc: 0.9456 - val_loss: 0.3703 - val_acc: 0.8744\n",
      "Epoch 55/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9429WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1998 - acc: 0.9353\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.22660 to 0.19976, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/055-0.1998.hdf5\n",
      "242/242 [==============================] - 70s 290ms/step - loss: 0.1512 - acc: 0.9430 - val_loss: 0.1998 - val_acc: 0.9353\n",
      "Epoch 56/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9445WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2749 - acc: 0.9178\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.19976\n",
      "242/242 [==============================] - 66s 274ms/step - loss: 0.1545 - acc: 0.9444 - val_loss: 0.2749 - val_acc: 0.9178\n",
      "Epoch 57/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9510WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1789 - acc: 0.9400\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.19976 to 0.17889, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/057-0.1789.hdf5\n",
      "242/242 [==============================] - 68s 281ms/step - loss: 0.1419 - acc: 0.9511 - val_loss: 0.1789 - val_acc: 0.9400\n",
      "Epoch 58/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9481WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2829 - acc: 0.8909\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.17889\n",
      "242/242 [==============================] - 68s 280ms/step - loss: 0.1446 - acc: 0.9480 - val_loss: 0.2829 - val_acc: 0.8909\n",
      "Epoch 59/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9507WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 15ms/sample - loss: 0.4440 - acc: 0.8350\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.17889\n",
      "242/242 [==============================] - 71s 293ms/step - loss: 0.1406 - acc: 0.9503 - val_loss: 0.4440 - val_acc: 0.8350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9475WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.4121 - acc: 0.8631\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.17889\n",
      "242/242 [==============================] - 69s 285ms/step - loss: 0.1472 - acc: 0.9475 - val_loss: 0.4121 - val_acc: 0.8631\n",
      "Epoch 61/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9553WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2508 - acc: 0.9087\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.17889\n",
      "242/242 [==============================] - 64s 265ms/step - loss: 0.1234 - acc: 0.9553 - val_loss: 0.2508 - val_acc: 0.9087\n",
      "Epoch 62/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9538WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.1641 - acc: 0.9475\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.17889 to 0.16414, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/062-0.1641.hdf5\n",
      "242/242 [==============================] - 71s 293ms/step - loss: 0.1309 - acc: 0.9538 - val_loss: 0.1641 - val_acc: 0.9475\n",
      "Epoch 63/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9523WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.2778 - acc: 0.8950\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.16414\n",
      "242/242 [==============================] - 67s 275ms/step - loss: 0.1373 - acc: 0.9524 - val_loss: 0.2778 - val_acc: 0.8950\n",
      "Epoch 64/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9565WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 42s 13ms/sample - loss: 0.1867 - acc: 0.9422\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.16414\n",
      "242/242 [==============================] - 65s 267ms/step - loss: 0.1235 - acc: 0.9567 - val_loss: 0.1867 - val_acc: 0.9422\n",
      "Epoch 65/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9584WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1652 - acc: 0.9350\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.16414\n",
      "242/242 [==============================] - 72s 297ms/step - loss: 0.1225 - acc: 0.9585 - val_loss: 0.1652 - val_acc: 0.9350\n",
      "Epoch 66/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9540WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2572 - acc: 0.8975\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.16414\n",
      "242/242 [==============================] - 70s 291ms/step - loss: 0.1279 - acc: 0.9540 - val_loss: 0.2572 - val_acc: 0.8975\n",
      "Epoch 67/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9530WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.3222 - acc: 0.8800\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.16414\n",
      "242/242 [==============================] - 69s 283ms/step - loss: 0.1316 - acc: 0.9529 - val_loss: 0.3222 - val_acc: 0.8800\n",
      "Epoch 68/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9601WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.3348 - acc: 0.8788\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.16414\n",
      "242/242 [==============================] - 72s 296ms/step - loss: 0.1221 - acc: 0.9603 - val_loss: 0.3348 - val_acc: 0.8788\n",
      "Epoch 69/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9589WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2140 - acc: 0.9162\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.16414\n",
      "242/242 [==============================] - 69s 284ms/step - loss: 0.1166 - acc: 0.9589 - val_loss: 0.2140 - val_acc: 0.9162\n",
      "Epoch 70/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9610WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2052 - acc: 0.9325\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.16414\n",
      "242/242 [==============================] - 63s 262ms/step - loss: 0.1086 - acc: 0.9611 - val_loss: 0.2052 - val_acc: 0.9325\n",
      "Epoch 71/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9593WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.1411 - acc: 0.9419\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.16414 to 0.14113, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/071-0.1411.hdf5\n",
      "242/242 [==============================] - 70s 289ms/step - loss: 0.1131 - acc: 0.9593 - val_loss: 0.1411 - val_acc: 0.9419\n",
      "Epoch 72/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9589WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 38s 12ms/sample - loss: 0.1764 - acc: 0.9337\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.14113\n",
      "242/242 [==============================] - 62s 255ms/step - loss: 0.1181 - acc: 0.9588 - val_loss: 0.1764 - val_acc: 0.9337\n",
      "Epoch 73/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9572WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 42s 13ms/sample - loss: 0.2288 - acc: 0.9122\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.14113\n",
      "242/242 [==============================] - 64s 265ms/step - loss: 0.1214 - acc: 0.9572 - val_loss: 0.2288 - val_acc: 0.9122\n",
      "Epoch 74/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9623WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 40s 13ms/sample - loss: 0.2536 - acc: 0.9000\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.14113\n",
      "242/242 [==============================] - 64s 262ms/step - loss: 0.1097 - acc: 0.9622 - val_loss: 0.2536 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9622WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.1312 - acc: 0.9569\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.14113 to 0.13118, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/075-0.1312.hdf5\n",
      "242/242 [==============================] - 70s 288ms/step - loss: 0.1054 - acc: 0.9623 - val_loss: 0.1312 - val_acc: 0.9569\n",
      "Epoch 76/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9623WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.1911 - acc: 0.9347\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 66s 272ms/step - loss: 0.1046 - acc: 0.9623 - val_loss: 0.1911 - val_acc: 0.9347\n",
      "Epoch 77/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9628WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 40s 12ms/sample - loss: 0.4122 - acc: 0.8575\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 63s 258ms/step - loss: 0.1109 - acc: 0.9629 - val_loss: 0.4122 - val_acc: 0.8575\n",
      "Epoch 78/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9607WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2321 - acc: 0.9209\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.1116 - acc: 0.9608 - val_loss: 0.2321 - val_acc: 0.9209\n",
      "Epoch 79/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9670WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 42s 13ms/sample - loss: 0.2843 - acc: 0.8950\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 65s 269ms/step - loss: 0.0978 - acc: 0.9670 - val_loss: 0.2843 - val_acc: 0.8950\n",
      "Epoch 80/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9632WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 41s 13ms/sample - loss: 0.2079 - acc: 0.9181\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 64s 263ms/step - loss: 0.1047 - acc: 0.9632 - val_loss: 0.2079 - val_acc: 0.9181\n",
      "Epoch 81/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9621WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 41s 13ms/sample - loss: 0.2414 - acc: 0.9300\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 62s 256ms/step - loss: 0.1080 - acc: 0.9620 - val_loss: 0.2414 - val_acc: 0.9300\n",
      "Epoch 82/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9628WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.4069 - acc: 0.8500\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 73s 301ms/step - loss: 0.1050 - acc: 0.9627 - val_loss: 0.4069 - val_acc: 0.8500\n",
      "Epoch 83/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9657WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.1572 - acc: 0.9459\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 67s 279ms/step - loss: 0.0969 - acc: 0.9653 - val_loss: 0.1572 - val_acc: 0.9459\n",
      "Epoch 84/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9647WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2271 - acc: 0.9212\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 72s 297ms/step - loss: 0.0963 - acc: 0.9646 - val_loss: 0.2271 - val_acc: 0.9212\n",
      "Epoch 85/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9690WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1522 - acc: 0.9484\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 69s 286ms/step - loss: 0.0861 - acc: 0.9690 - val_loss: 0.1522 - val_acc: 0.9484\n",
      "Epoch 86/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9647WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2853 - acc: 0.9044\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 70s 291ms/step - loss: 0.0978 - acc: 0.9647 - val_loss: 0.2853 - val_acc: 0.9044\n",
      "Epoch 87/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9650WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.2521 - acc: 0.9178\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 67s 276ms/step - loss: 0.0999 - acc: 0.9649 - val_loss: 0.2521 - val_acc: 0.9178\n",
      "Epoch 88/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9658WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 8ms/sample - loss: 0.2335 - acc: 0.9325\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 75s 312ms/step - loss: 0.0915 - acc: 0.9659 - val_loss: 0.2335 - val_acc: 0.9325\n",
      "Epoch 89/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9658WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.2216 - acc: 0.9116\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 76s 313ms/step - loss: 0.0950 - acc: 0.9660 - val_loss: 0.2216 - val_acc: 0.9116\n",
      "Epoch 90/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9684WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.2352 - acc: 0.9306\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 69s 285ms/step - loss: 0.0871 - acc: 0.9683 - val_loss: 0.2352 - val_acc: 0.9306\n",
      "Epoch 91/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9681WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 51s 16ms/sample - loss: 0.4512 - acc: 0.8575\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 72s 296ms/step - loss: 0.0919 - acc: 0.9681 - val_loss: 0.4512 - val_acc: 0.8575\n",
      "Epoch 92/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9698WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2064 - acc: 0.9475\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 70s 288ms/step - loss: 0.0884 - acc: 0.9699 - val_loss: 0.2064 - val_acc: 0.9475\n",
      "Epoch 93/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9710WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2742 - acc: 0.9300\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 67s 276ms/step - loss: 0.0803 - acc: 0.9710 - val_loss: 0.2742 - val_acc: 0.9300\n",
      "Epoch 94/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9656WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3440 - acc: 0.8900\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 69s 287ms/step - loss: 0.0961 - acc: 0.9657 - val_loss: 0.3440 - val_acc: 0.8900\n",
      "Epoch 95/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9690WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.3667 - acc: 0.8931\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 69s 284ms/step - loss: 0.0866 - acc: 0.9690 - val_loss: 0.3667 - val_acc: 0.8931\n",
      "Epoch 96/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9663WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2169 - acc: 0.9306\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 65s 270ms/step - loss: 0.0951 - acc: 0.9661 - val_loss: 0.2169 - val_acc: 0.9306\n",
      "Epoch 97/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9707WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 16ms/sample - loss: 0.2028 - acc: 0.9500\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 72s 297ms/step - loss: 0.0794 - acc: 0.9707 - val_loss: 0.2028 - val_acc: 0.9500\n",
      "Epoch 98/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9726WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3808 - acc: 0.8659\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 68s 283ms/step - loss: 0.0795 - acc: 0.9726 - val_loss: 0.3808 - val_acc: 0.8659\n",
      "Epoch 99/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9698WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.3133 - acc: 0.9075\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 68s 280ms/step - loss: 0.0838 - acc: 0.9698 - val_loss: 0.3133 - val_acc: 0.9075\n",
      "Epoch 100/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9701WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2372 - acc: 0.9159\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 70s 290ms/step - loss: 0.0842 - acc: 0.9700 - val_loss: 0.2372 - val_acc: 0.9159\n",
      "Epoch 101/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9706WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.1525 - acc: 0.9469\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 65s 267ms/step - loss: 0.0789 - acc: 0.9705 - val_loss: 0.1525 - val_acc: 0.9469\n",
      "Epoch 102/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9670WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 49s 15ms/sample - loss: 0.2838 - acc: 0.9087\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 77s 317ms/step - loss: 0.0883 - acc: 0.9668 - val_loss: 0.2838 - val_acc: 0.9087\n",
      "Epoch 103/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9704WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.4350 - acc: 0.8775\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 69s 284ms/step - loss: 0.0805 - acc: 0.9704 - val_loss: 0.4350 - val_acc: 0.8775\n",
      "Epoch 104/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9695WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.4048 - acc: 0.8675\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 66s 273ms/step - loss: 0.0847 - acc: 0.9696 - val_loss: 0.4048 - val_acc: 0.8675\n",
      "Epoch 105/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9726WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.1586 - acc: 0.9600\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 68s 281ms/step - loss: 0.0798 - acc: 0.9724 - val_loss: 0.1586 - val_acc: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9730WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2031 - acc: 0.9413\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 67s 277ms/step - loss: 0.0805 - acc: 0.9730 - val_loss: 0.2031 - val_acc: 0.9413\n",
      "Epoch 107/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9748WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.2712 - acc: 0.9150\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 66s 272ms/step - loss: 0.0754 - acc: 0.9749 - val_loss: 0.2712 - val_acc: 0.9150\n",
      "Epoch 108/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9759WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2553 - acc: 0.9450\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 65s 270ms/step - loss: 0.0704 - acc: 0.9759 - val_loss: 0.2553 - val_acc: 0.9450\n",
      "Epoch 109/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9689WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.3138 - acc: 0.9022\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 68s 281ms/step - loss: 0.0880 - acc: 0.9690 - val_loss: 0.3138 - val_acc: 0.9022\n",
      "Epoch 110/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9708WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.1617 - acc: 0.9413\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 68s 282ms/step - loss: 0.0817 - acc: 0.9708 - val_loss: 0.1617 - val_acc: 0.9413\n",
      "Epoch 111/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9764WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.2997 - acc: 0.8869\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 69s 283ms/step - loss: 0.0671 - acc: 0.9765 - val_loss: 0.2997 - val_acc: 0.8869\n",
      "Epoch 112/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9766WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.1778 - acc: 0.9325\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 68s 281ms/step - loss: 0.0654 - acc: 0.9766 - val_loss: 0.1778 - val_acc: 0.9325\n",
      "Epoch 113/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9759WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 36s 11ms/sample - loss: 0.2292 - acc: 0.9184\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 68s 280ms/step - loss: 0.0730 - acc: 0.9759 - val_loss: 0.2292 - val_acc: 0.9184\n",
      "Epoch 114/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9710WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.2431 - acc: 0.9147\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.13118\n",
      "242/242 [==============================] - 68s 281ms/step - loss: 0.0846 - acc: 0.9711 - val_loss: 0.2431 - val_acc: 0.9147\n",
      "Epoch 115/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9727WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1236 - acc: 0.9500\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.13118 to 0.12357, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/115-0.1236.hdf5\n",
      "242/242 [==============================] - 68s 280ms/step - loss: 0.0765 - acc: 0.9728 - val_loss: 0.1236 - val_acc: 0.9500\n",
      "Epoch 116/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9736WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 43s 13ms/sample - loss: 0.1817 - acc: 0.9500\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 65s 267ms/step - loss: 0.0755 - acc: 0.9734 - val_loss: 0.1817 - val_acc: 0.9500\n",
      "Epoch 117/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9762WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 16ms/sample - loss: 0.1356 - acc: 0.9597\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 71s 295ms/step - loss: 0.0686 - acc: 0.9762 - val_loss: 0.1356 - val_acc: 0.9597\n",
      "Epoch 118/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9743WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.1305 - acc: 0.9550\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 67s 277ms/step - loss: 0.0733 - acc: 0.9743 - val_loss: 0.1305 - val_acc: 0.9550\n",
      "Epoch 119/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9758WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.2425 - acc: 0.8994\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 66s 271ms/step - loss: 0.0686 - acc: 0.9757 - val_loss: 0.2425 - val_acc: 0.8994\n",
      "Epoch 120/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9758WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 42s 13ms/sample - loss: 0.3011 - acc: 0.9050\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 64s 263ms/step - loss: 0.0688 - acc: 0.9758 - val_loss: 0.3011 - val_acc: 0.9050\n",
      "Epoch 121/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/242 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9775WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1433 - acc: 0.9575\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 72s 297ms/step - loss: 0.0647 - acc: 0.9774 - val_loss: 0.1433 - val_acc: 0.9575\n",
      "Epoch 122/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9753WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2140 - acc: 0.9200\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 69s 286ms/step - loss: 0.0752 - acc: 0.9752 - val_loss: 0.2140 - val_acc: 0.9200\n",
      "Epoch 123/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9759WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.1833 - acc: 0.9391\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 71s 295ms/step - loss: 0.0716 - acc: 0.9760 - val_loss: 0.1833 - val_acc: 0.9391\n",
      "Epoch 124/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9712WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1741 - acc: 0.9506\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 68s 283ms/step - loss: 0.0829 - acc: 0.9712 - val_loss: 0.1741 - val_acc: 0.9506\n",
      "Epoch 125/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9764WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2032 - acc: 0.9337\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 68s 281ms/step - loss: 0.0731 - acc: 0.9765 - val_loss: 0.2032 - val_acc: 0.9337\n",
      "Epoch 126/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9707WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.2325 - acc: 0.9375\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.0829 - acc: 0.9708 - val_loss: 0.2325 - val_acc: 0.9375\n",
      "Epoch 127/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9784WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.2455 - acc: 0.9275\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.12357\n",
      "242/242 [==============================] - 67s 275ms/step - loss: 0.0598 - acc: 0.9784 - val_loss: 0.2455 - val_acc: 0.9275\n",
      "Epoch 128/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9749WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 41s 13ms/sample - loss: 0.1227 - acc: 0.9481\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.12357 to 0.12270, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/128-0.1227.hdf5\n",
      "242/242 [==============================] - 64s 266ms/step - loss: 0.0738 - acc: 0.9750 - val_loss: 0.1227 - val_acc: 0.9481\n",
      "Epoch 129/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9769WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1410 - acc: 0.9525\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.12270\n",
      "242/242 [==============================] - 67s 279ms/step - loss: 0.0659 - acc: 0.9770 - val_loss: 0.1410 - val_acc: 0.9525\n",
      "Epoch 130/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9764WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 40s 13ms/sample - loss: 0.1048 - acc: 0.9600\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.12270 to 0.10482, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/130-0.1048.hdf5\n",
      "242/242 [==============================] - 65s 268ms/step - loss: 0.0727 - acc: 0.9765 - val_loss: 0.1048 - val_acc: 0.9600\n",
      "Epoch 131/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9771WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 43s 14ms/sample - loss: 0.1562 - acc: 0.9438\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 66s 274ms/step - loss: 0.0622 - acc: 0.9772 - val_loss: 0.1562 - val_acc: 0.9438\n",
      "Epoch 132/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9735WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2504 - acc: 0.9144\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 66s 273ms/step - loss: 0.0796 - acc: 0.9735 - val_loss: 0.2504 - val_acc: 0.9144\n",
      "Epoch 133/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9763WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1887 - acc: 0.9375\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.0657 - acc: 0.9764 - val_loss: 0.1887 - val_acc: 0.9375\n",
      "Epoch 134/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9755WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1593 - acc: 0.9422\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 70s 289ms/step - loss: 0.0667 - acc: 0.9755 - val_loss: 0.1593 - val_acc: 0.9422\n",
      "Epoch 135/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9761WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2451 - acc: 0.9225\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 69s 287ms/step - loss: 0.0645 - acc: 0.9761 - val_loss: 0.2451 - val_acc: 0.9225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9742WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 16ms/sample - loss: 0.1540 - acc: 0.9522\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 73s 303ms/step - loss: 0.0708 - acc: 0.9741 - val_loss: 0.1540 - val_acc: 0.9522\n",
      "Epoch 137/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9780WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 43s 13ms/sample - loss: 0.1793 - acc: 0.9359\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 66s 275ms/step - loss: 0.0630 - acc: 0.9781 - val_loss: 0.1793 - val_acc: 0.9359\n",
      "Epoch 138/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9803WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.1684 - acc: 0.9394\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 66s 274ms/step - loss: 0.0596 - acc: 0.9802 - val_loss: 0.1684 - val_acc: 0.9394\n",
      "Epoch 139/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9764WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.1531 - acc: 0.9500\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 68s 281ms/step - loss: 0.0641 - acc: 0.9764 - val_loss: 0.1531 - val_acc: 0.9500\n",
      "Epoch 140/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9749WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.1524 - acc: 0.9525\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 68s 282ms/step - loss: 0.0719 - acc: 0.9747 - val_loss: 0.1524 - val_acc: 0.9525\n",
      "Epoch 141/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9769WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.1392 - acc: 0.9525\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 65s 268ms/step - loss: 0.0622 - acc: 0.9770 - val_loss: 0.1392 - val_acc: 0.9525\n",
      "Epoch 142/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9780WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 52s 16ms/sample - loss: 0.1650 - acc: 0.9350\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.10482\n",
      "242/242 [==============================] - 74s 304ms/step - loss: 0.0622 - acc: 0.9781 - val_loss: 0.1650 - val_acc: 0.9350\n",
      "Epoch 143/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9759WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.0895 - acc: 0.9697\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.10482 to 0.08952, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/143-0.0895.hdf5\n",
      "242/242 [==============================] - 69s 283ms/step - loss: 0.0643 - acc: 0.9759 - val_loss: 0.0895 - val_acc: 0.9697\n",
      "Epoch 144/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9792WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.2434 - acc: 0.9166\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 71s 292ms/step - loss: 0.0591 - acc: 0.9792 - val_loss: 0.2434 - val_acc: 0.9166\n",
      "Epoch 145/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9779WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1687 - acc: 0.9491\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 71s 295ms/step - loss: 0.0621 - acc: 0.9778 - val_loss: 0.1687 - val_acc: 0.9491\n",
      "Epoch 146/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9771WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 16ms/sample - loss: 0.1291 - acc: 0.9466\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 72s 298ms/step - loss: 0.0652 - acc: 0.9770 - val_loss: 0.1291 - val_acc: 0.9466\n",
      "Epoch 147/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9787WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.2317 - acc: 0.9281\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 69s 284ms/step - loss: 0.0575 - acc: 0.9787 - val_loss: 0.2317 - val_acc: 0.9281\n",
      "Epoch 148/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9778WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1795 - acc: 0.9453\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 67s 277ms/step - loss: 0.0632 - acc: 0.9778 - val_loss: 0.1795 - val_acc: 0.9453\n",
      "Epoch 149/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9786WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.2630 - acc: 0.9100\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 69s 287ms/step - loss: 0.0626 - acc: 0.9784 - val_loss: 0.2630 - val_acc: 0.9100\n",
      "Epoch 150/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9806WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3235 - acc: 0.8875\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 69s 283ms/step - loss: 0.0575 - acc: 0.9806 - val_loss: 0.3235 - val_acc: 0.8875\n",
      "Epoch 151/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/242 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9770WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1719 - acc: 0.9525\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 67s 276ms/step - loss: 0.0633 - acc: 0.9769 - val_loss: 0.1719 - val_acc: 0.9525\n",
      "Epoch 152/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9801WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 49s 15ms/sample - loss: 0.2848 - acc: 0.9206\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 70s 291ms/step - loss: 0.0567 - acc: 0.9800 - val_loss: 0.2848 - val_acc: 0.9206\n",
      "Epoch 153/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9807WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.1093 - acc: 0.9594\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 66s 272ms/step - loss: 0.0560 - acc: 0.9807 - val_loss: 0.1093 - val_acc: 0.9594\n",
      "Epoch 154/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9769WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2131 - acc: 0.9306\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 62s 257ms/step - loss: 0.0587 - acc: 0.9770 - val_loss: 0.2131 - val_acc: 0.9306\n",
      "Epoch 155/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9788WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.1029 - acc: 0.9650\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 66s 273ms/step - loss: 0.0590 - acc: 0.9789 - val_loss: 0.1029 - val_acc: 0.9650\n",
      "Epoch 156/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9784WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2322 - acc: 0.9356\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 66s 271ms/step - loss: 0.0608 - acc: 0.9784 - val_loss: 0.2322 - val_acc: 0.9356\n",
      "Epoch 157/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9800WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 16ms/sample - loss: 0.1586 - acc: 0.9500\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 72s 296ms/step - loss: 0.0565 - acc: 0.9800 - val_loss: 0.1586 - val_acc: 0.9500\n",
      "Epoch 158/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9797WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.1722 - acc: 0.9425\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 68s 281ms/step - loss: 0.0561 - acc: 0.9798 - val_loss: 0.1722 - val_acc: 0.9425\n",
      "Epoch 159/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9771WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2119 - acc: 0.9494\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 65s 270ms/step - loss: 0.0642 - acc: 0.9772 - val_loss: 0.2119 - val_acc: 0.9494\n",
      "Epoch 160/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9802WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.2023 - acc: 0.9284\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.08952\n",
      "242/242 [==============================] - 70s 289ms/step - loss: 0.0582 - acc: 0.9802 - val_loss: 0.2023 - val_acc: 0.9284\n",
      "Epoch 161/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9780WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.0739 - acc: 0.9744\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.08952 to 0.07387, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/161-0.0739.hdf5\n",
      "242/242 [==============================] - 68s 283ms/step - loss: 0.0619 - acc: 0.9779 - val_loss: 0.0739 - val_acc: 0.9744\n",
      "Epoch 162/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9780WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.0961 - acc: 0.9538\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 68s 281ms/step - loss: 0.0637 - acc: 0.9778 - val_loss: 0.0961 - val_acc: 0.9538\n",
      "Epoch 163/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9805WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1881 - acc: 0.9400\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 69s 286ms/step - loss: 0.0517 - acc: 0.9806 - val_loss: 0.1881 - val_acc: 0.9400\n",
      "Epoch 164/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9791WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.2301 - acc: 0.9250\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 69s 285ms/step - loss: 0.0583 - acc: 0.9791 - val_loss: 0.2301 - val_acc: 0.9250\n",
      "Epoch 165/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9822WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1566 - acc: 0.9525\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.0525 - acc: 0.9822 - val_loss: 0.1566 - val_acc: 0.9525\n",
      "Epoch 166/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9804WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.1436 - acc: 0.9575\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 68s 282ms/step - loss: 0.0571 - acc: 0.9802 - val_loss: 0.1436 - val_acc: 0.9575\n",
      "Epoch 167/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9797WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.1524 - acc: 0.9563\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 68s 280ms/step - loss: 0.0546 - acc: 0.9796 - val_loss: 0.1524 - val_acc: 0.9563\n",
      "Epoch 168/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9819WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2655 - acc: 0.9200\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 75s 311ms/step - loss: 0.0513 - acc: 0.9820 - val_loss: 0.2655 - val_acc: 0.9200\n",
      "Epoch 169/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9788WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.0935 - acc: 0.9700\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 76s 312ms/step - loss: 0.0593 - acc: 0.9787 - val_loss: 0.0935 - val_acc: 0.9700\n",
      "Epoch 170/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9847WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 55s 17ms/sample - loss: 0.2410 - acc: 0.9316\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 76s 315ms/step - loss: 0.0435 - acc: 0.9847 - val_loss: 0.2410 - val_acc: 0.9316\n",
      "Epoch 171/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9798WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.0792 - acc: 0.9725\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 75s 310ms/step - loss: 0.0574 - acc: 0.9799 - val_loss: 0.0792 - val_acc: 0.9725\n",
      "Epoch 172/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9800WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 55s 17ms/sample - loss: 0.2777 - acc: 0.8953\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 77s 317ms/step - loss: 0.0557 - acc: 0.9798 - val_loss: 0.2777 - val_acc: 0.8953\n",
      "Epoch 173/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9834WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2024 - acc: 0.9250\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 74s 307ms/step - loss: 0.0502 - acc: 0.9832 - val_loss: 0.2024 - val_acc: 0.9250\n",
      "Epoch 174/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9815WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 54s 17ms/sample - loss: 0.1671 - acc: 0.9316\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 76s 314ms/step - loss: 0.0545 - acc: 0.9815 - val_loss: 0.1671 - val_acc: 0.9316\n",
      "Epoch 175/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9798WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1559 - acc: 0.9575\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 78s 321ms/step - loss: 0.0583 - acc: 0.9799 - val_loss: 0.1559 - val_acc: 0.9575\n",
      "Epoch 176/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9819WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.1749 - acc: 0.9350\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 83s 344ms/step - loss: 0.0447 - acc: 0.9818 - val_loss: 0.1749 - val_acc: 0.9350\n",
      "Epoch 177/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9822WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2150 - acc: 0.9075\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 76s 316ms/step - loss: 0.0506 - acc: 0.9820 - val_loss: 0.2150 - val_acc: 0.9075\n",
      "Epoch 178/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9805WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.3679 - acc: 0.8784\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 75s 309ms/step - loss: 0.0525 - acc: 0.9804 - val_loss: 0.3679 - val_acc: 0.8784\n",
      "Epoch 179/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9834WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2124 - acc: 0.9231\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 76s 316ms/step - loss: 0.0456 - acc: 0.9834 - val_loss: 0.2124 - val_acc: 0.9231\n",
      "Epoch 180/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9824WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 17ms/sample - loss: 0.1436 - acc: 0.9625\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 74s 308ms/step - loss: 0.0521 - acc: 0.9824 - val_loss: 0.1436 - val_acc: 0.9625\n",
      "Epoch 181/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9845WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 54s 17ms/sample - loss: 0.1989 - acc: 0.9275\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 76s 314ms/step - loss: 0.0431 - acc: 0.9846 - val_loss: 0.1989 - val_acc: 0.9275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9810WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.2592 - acc: 0.9234\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 76s 313ms/step - loss: 0.0534 - acc: 0.9810 - val_loss: 0.2592 - val_acc: 0.9234\n",
      "Epoch 183/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9796WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.1860 - acc: 0.9131\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 79s 327ms/step - loss: 0.0555 - acc: 0.9797 - val_loss: 0.1860 - val_acc: 0.9131\n",
      "Epoch 184/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9808WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 52s 16ms/sample - loss: 0.2356 - acc: 0.9200\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 73s 302ms/step - loss: 0.0521 - acc: 0.9808 - val_loss: 0.2356 - val_acc: 0.9200\n",
      "Epoch 185/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9781WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 16ms/sample - loss: 0.1395 - acc: 0.9475\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 74s 307ms/step - loss: 0.0578 - acc: 0.9780 - val_loss: 0.1395 - val_acc: 0.9475\n",
      "Epoch 186/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9810WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.1160 - acc: 0.9600\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 84s 346ms/step - loss: 0.0528 - acc: 0.9810 - val_loss: 0.1160 - val_acc: 0.9600\n",
      "Epoch 187/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9788WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1382 - acc: 0.9434\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 75s 309ms/step - loss: 0.0610 - acc: 0.9788 - val_loss: 0.1382 - val_acc: 0.9434\n",
      "Epoch 188/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9855WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1413 - acc: 0.9475\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 77s 320ms/step - loss: 0.0427 - acc: 0.9853 - val_loss: 0.1413 - val_acc: 0.9475\n",
      "Epoch 189/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9816WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.3521 - acc: 0.8806\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 77s 318ms/step - loss: 0.0473 - acc: 0.9817 - val_loss: 0.3521 - val_acc: 0.8806\n",
      "Epoch 190/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9846WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.0944 - acc: 0.9584\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.07387\n",
      "242/242 [==============================] - 76s 312ms/step - loss: 0.0499 - acc: 0.9846 - val_loss: 0.0944 - val_acc: 0.9584\n",
      "Epoch 191/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9836WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.0713 - acc: 0.9650\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.07387 to 0.07135, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv_checkpoint/191-0.0713.hdf5\n",
      "242/242 [==============================] - 76s 313ms/step - loss: 0.0468 - acc: 0.9837 - val_loss: 0.0713 - val_acc: 0.9650\n",
      "Epoch 192/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9843WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.1443 - acc: 0.9475\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 315ms/step - loss: 0.0444 - acc: 0.9844 - val_loss: 0.1443 - val_acc: 0.9475\n",
      "Epoch 193/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9813WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 49s 15ms/sample - loss: 0.1464 - acc: 0.9425\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 313ms/step - loss: 0.0529 - acc: 0.9813 - val_loss: 0.1464 - val_acc: 0.9425\n",
      "Epoch 194/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9870WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 56s 17ms/sample - loss: 0.1551 - acc: 0.9600\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 77s 317ms/step - loss: 0.0380 - acc: 0.9869 - val_loss: 0.1551 - val_acc: 0.9600\n",
      "Epoch 195/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9842WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 49s 15ms/sample - loss: 0.1439 - acc: 0.9475\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 316ms/step - loss: 0.0470 - acc: 0.9843 - val_loss: 0.1439 - val_acc: 0.9475\n",
      "Epoch 196/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9816WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1225 - acc: 0.9669\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 313ms/step - loss: 0.0519 - acc: 0.9817 - val_loss: 0.1225 - val_acc: 0.9669\n",
      "Epoch 197/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9856WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1904 - acc: 0.9456\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 314ms/step - loss: 0.0418 - acc: 0.9856 - val_loss: 0.1904 - val_acc: 0.9456\n",
      "Epoch 198/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9835WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 52s 16ms/sample - loss: 0.2488 - acc: 0.9200\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 74s 307ms/step - loss: 0.0492 - acc: 0.9836 - val_loss: 0.2488 - val_acc: 0.9200\n",
      "Epoch 199/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9844WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2228 - acc: 0.9419\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 78s 321ms/step - loss: 0.0453 - acc: 0.9844 - val_loss: 0.2228 - val_acc: 0.9419\n",
      "Epoch 200/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9814WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2490 - acc: 0.9375\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 78s 321ms/step - loss: 0.0532 - acc: 0.9814 - val_loss: 0.2490 - val_acc: 0.9375\n",
      "Epoch 201/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9817WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 17ms/sample - loss: 0.3270 - acc: 0.9000\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 75s 310ms/step - loss: 0.0522 - acc: 0.9817 - val_loss: 0.3270 - val_acc: 0.9000\n",
      "Epoch 202/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9846WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 17ms/sample - loss: 0.2426 - acc: 0.9388\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 313ms/step - loss: 0.0450 - acc: 0.9845 - val_loss: 0.2426 - val_acc: 0.9388\n",
      "Epoch 203/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9871WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 54s 17ms/sample - loss: 0.1919 - acc: 0.9581\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 316ms/step - loss: 0.0382 - acc: 0.9872 - val_loss: 0.1919 - val_acc: 0.9581\n",
      "Epoch 204/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9832WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.1939 - acc: 0.9453\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 77s 320ms/step - loss: 0.0492 - acc: 0.9832 - val_loss: 0.1939 - val_acc: 0.9453\n",
      "Epoch 205/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9856WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 16ms/sample - loss: 0.1950 - acc: 0.9316\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 313ms/step - loss: 0.0452 - acc: 0.9854 - val_loss: 0.1950 - val_acc: 0.9316\n",
      "Epoch 206/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9825WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 51s 16ms/sample - loss: 0.3078 - acc: 0.9013\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 83s 341ms/step - loss: 0.0527 - acc: 0.9826 - val_loss: 0.3078 - val_acc: 0.9013\n",
      "Epoch 207/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9824WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.3472 - acc: 0.8984\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 79s 328ms/step - loss: 0.0486 - acc: 0.9822 - val_loss: 0.3472 - val_acc: 0.8984\n",
      "Epoch 208/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9841WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1773 - acc: 0.9509\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 78s 322ms/step - loss: 0.0461 - acc: 0.9842 - val_loss: 0.1773 - val_acc: 0.9509\n",
      "Epoch 209/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9833WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 56s 17ms/sample - loss: 0.2698 - acc: 0.9153\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 79s 325ms/step - loss: 0.0468 - acc: 0.9833 - val_loss: 0.2698 - val_acc: 0.9153\n",
      "Epoch 210/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9857WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 56s 17ms/sample - loss: 0.2457 - acc: 0.9325\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 87s 360ms/step - loss: 0.0411 - acc: 0.9856 - val_loss: 0.2457 - val_acc: 0.9325\n",
      "Epoch 211/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9843WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.1827 - acc: 0.9369\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 86s 354ms/step - loss: 0.0401 - acc: 0.9844 - val_loss: 0.1827 - val_acc: 0.9369\n",
      "Epoch 212/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9876WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 52s 16ms/sample - loss: 0.4523 - acc: 0.8700\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 78s 324ms/step - loss: 0.0369 - acc: 0.9877 - val_loss: 0.4523 - val_acc: 0.8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9824WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 54s 17ms/sample - loss: 0.1454 - acc: 0.9491\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 87s 358ms/step - loss: 0.0516 - acc: 0.9824 - val_loss: 0.1454 - val_acc: 0.9491\n",
      "Epoch 214/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9839WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 51s 16ms/sample - loss: 0.2673 - acc: 0.9150\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 80s 330ms/step - loss: 0.0466 - acc: 0.9840 - val_loss: 0.2673 - val_acc: 0.9150\n",
      "Epoch 215/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9852WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 49s 15ms/sample - loss: 0.1515 - acc: 0.9431\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 77s 318ms/step - loss: 0.0439 - acc: 0.9853 - val_loss: 0.1515 - val_acc: 0.9431\n",
      "Epoch 216/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9840WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 54s 17ms/sample - loss: 0.1755 - acc: 0.9463\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 313ms/step - loss: 0.0430 - acc: 0.9840 - val_loss: 0.1755 - val_acc: 0.9463\n",
      "Epoch 217/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9831WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 17ms/sample - loss: 0.2117 - acc: 0.9388\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 75s 310ms/step - loss: 0.0458 - acc: 0.9831 - val_loss: 0.2117 - val_acc: 0.9388\n",
      "Epoch 218/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9840WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 17ms/sample - loss: 0.2331 - acc: 0.9216\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 74s 307ms/step - loss: 0.0480 - acc: 0.9841 - val_loss: 0.2331 - val_acc: 0.9216\n",
      "Epoch 219/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9846WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 55s 17ms/sample - loss: 0.1723 - acc: 0.9594\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 77s 316ms/step - loss: 0.0413 - acc: 0.9847 - val_loss: 0.1723 - val_acc: 0.9594\n",
      "Epoch 220/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9850WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2359 - acc: 0.9116\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 78s 321ms/step - loss: 0.0458 - acc: 0.9851 - val_loss: 0.2359 - val_acc: 0.9116\n",
      "Epoch 221/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9856WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1135 - acc: 0.9550\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 77s 316ms/step - loss: 0.0388 - acc: 0.9855 - val_loss: 0.1135 - val_acc: 0.9550\n",
      "Epoch 222/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9846WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 17ms/sample - loss: 0.2560 - acc: 0.9144\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 313ms/step - loss: 0.0456 - acc: 0.9847 - val_loss: 0.2560 - val_acc: 0.9144\n",
      "Epoch 223/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9842WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 54s 17ms/sample - loss: 0.1872 - acc: 0.9550\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 81s 337ms/step - loss: 0.0404 - acc: 0.9843 - val_loss: 0.1872 - val_acc: 0.9550\n",
      "Epoch 224/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9835WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 55s 17ms/sample - loss: 0.1847 - acc: 0.9422\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 79s 325ms/step - loss: 0.0428 - acc: 0.9836 - val_loss: 0.1847 - val_acc: 0.9422\n",
      "Epoch 225/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9884WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.1922 - acc: 0.9316\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 75s 312ms/step - loss: 0.0379 - acc: 0.9884 - val_loss: 0.1922 - val_acc: 0.9316\n",
      "Epoch 226/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9842WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 51s 16ms/sample - loss: 0.1957 - acc: 0.9400\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 316ms/step - loss: 0.0436 - acc: 0.9843 - val_loss: 0.1957 - val_acc: 0.9400\n",
      "Epoch 227/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9849WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2087 - acc: 0.9244\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 76s 314ms/step - loss: 0.0458 - acc: 0.9850 - val_loss: 0.2087 - val_acc: 0.9244\n",
      "Epoch 228/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9890WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 54s 17ms/sample - loss: 0.1574 - acc: 0.9575\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 75s 309ms/step - loss: 0.0347 - acc: 0.9890 - val_loss: 0.1574 - val_acc: 0.9575\n",
      "Epoch 229/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9891WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 16ms/sample - loss: 0.2151 - acc: 0.9309\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 77s 319ms/step - loss: 0.0353 - acc: 0.9891 - val_loss: 0.2151 - val_acc: 0.9309\n",
      "Epoch 230/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9876WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.1831 - acc: 0.9500\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 77s 320ms/step - loss: 0.0388 - acc: 0.9877 - val_loss: 0.1831 - val_acc: 0.9500\n",
      "Epoch 231/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9847WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2675 - acc: 0.9256\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 79s 325ms/step - loss: 0.0415 - acc: 0.9848 - val_loss: 0.2675 - val_acc: 0.9256\n",
      "Epoch 232/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9855WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 58s 18ms/sample - loss: 0.1886 - acc: 0.9250\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 78s 323ms/step - loss: 0.0408 - acc: 0.9855 - val_loss: 0.1886 - val_acc: 0.9250\n",
      "Epoch 233/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9870WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 55s 17ms/sample - loss: 0.2577 - acc: 0.9275\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 77s 317ms/step - loss: 0.0393 - acc: 0.9871 - val_loss: 0.2577 - val_acc: 0.9275\n",
      "Epoch 234/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9835WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2291 - acc: 0.9269\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 77s 317ms/step - loss: 0.0466 - acc: 0.9836 - val_loss: 0.2291 - val_acc: 0.9269\n",
      "Epoch 235/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9856WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 17ms/sample - loss: 0.2556 - acc: 0.9284\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 74s 307ms/step - loss: 0.0386 - acc: 0.9856 - val_loss: 0.2556 - val_acc: 0.9284\n",
      "Epoch 236/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9873WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 59s 19ms/sample - loss: 0.2698 - acc: 0.9062\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 81s 334ms/step - loss: 0.0388 - acc: 0.9874 - val_loss: 0.2698 - val_acc: 0.9062\n",
      "Epoch 237/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9865WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 54s 17ms/sample - loss: 0.1055 - acc: 0.9684\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 75s 310ms/step - loss: 0.0382 - acc: 0.9866 - val_loss: 0.1055 - val_acc: 0.9684\n",
      "Epoch 238/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9870WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 55s 17ms/sample - loss: 0.1138 - acc: 0.9625\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 77s 318ms/step - loss: 0.0369 - acc: 0.9871 - val_loss: 0.1138 - val_acc: 0.9625\n",
      "Epoch 239/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9837WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 17ms/sample - loss: 0.1306 - acc: 0.9619\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 74s 308ms/step - loss: 0.0454 - acc: 0.9836 - val_loss: 0.1306 - val_acc: 0.9619\n",
      "Epoch 240/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9841WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 56s 17ms/sample - loss: 0.1701 - acc: 0.9472\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 78s 320ms/step - loss: 0.0457 - acc: 0.9842 - val_loss: 0.1701 - val_acc: 0.9472\n",
      "Epoch 241/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9898WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.3263 - acc: 0.9128\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.07135\n",
      "242/242 [==============================] - 79s 327ms/step - loss: 0.0338 - acc: 0.9899 - val_loss: 0.3263 - val_acc: 0.9128\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3183 [==============================] - 6s 2ms/sample - loss: 0.2094 - acc: 0.9359\n",
      "Loss: 0.20936278329750782 Accuracy: 0.9359095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 8):\n",
    "    base = 'vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_2d_cnn_custom_ch_64_DO(conv_num=i)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit_generator(data_generator,\n",
    "            steps_per_epoch=len(x_train)//batch_size,\n",
    "            epochs=10000,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks = [checkpointer, early_stopping],\n",
    "            workers=8, \n",
    "            use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 55302     \n",
      "=================================================================\n",
      "Total params: 1,596,742\n",
      "Trainable params: 1,596,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 3s 992us/sample - loss: 0.1987 - acc: 0.9259\n",
      "Loss: 0.19873026720746448 Accuracy: 0.9258561\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 13830     \n",
      "=================================================================\n",
      "Total params: 3,193,926\n",
      "Trainable params: 3,193,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 4s 1ms/sample - loss: 0.2345 - acc: 0.9243\n",
      "Loss: 0.23450051145549095 Accuracy: 0.9242853\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 6,469,702\n",
      "Trainable params: 6,469,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3183 [==============================] - 4s 1ms/sample - loss: 0.2094 - acc: 0.9359\n",
      "Loss: 0.20936278329750782 Accuracy: 0.9359095\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = 'vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(5, 8):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 55302     \n",
      "=================================================================\n",
      "Total params: 1,596,742\n",
      "Trainable params: 1,596,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 3s 905us/sample - loss: 0.1547 - acc: 0.9510\n",
      "Loss: 0.15470825378676836 Accuracy: 0.9509896\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 13830     \n",
      "=================================================================\n",
      "Total params: 3,193,926\n",
      "Trainable params: 3,193,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 4s 1ms/sample - loss: 0.1593 - acc: 0.9494\n",
      "Loss: 0.15934825810319106 Accuracy: 0.9494188\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 6,469,702\n",
      "Trainable params: 6,469,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3183 [==============================] - 4s 1ms/sample - loss: 0.2102 - acc: 0.9466\n",
      "Loss: 0.21024965254886605 Accuracy: 0.94659126\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 8):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_64_DO_050_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 13830     \n",
      "=================================================================\n",
      "Total params: 3,193,926\n",
      "Trainable params: 3,193,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 4s 1ms/sample - loss: 0.2345 - acc: 0.9243\n",
      "Loss: 0.23450051145549095 Accuracy: 0.9242853\n"
     ]
    }
   ],
   "source": [
    "i = 6\n",
    "model_name = base+'_{}_conv'.format(i)\n",
    "print()\n",
    "print(model_name, 'Model')\n",
    "model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "model = load_model(model_filename)\n",
    "model.summary()\n",
    "\n",
    "[loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[  29    1    0    0    0    0]\n",
      " [   1 1018    0   22    1    8]\n",
      " [   0   10   50   18    0    0]\n",
      " [   3  147   18 1770    1    1]\n",
      " [   0    1    0    0   32    1]\n",
      " [   0    7    0    0    1   43]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bed       0.88      0.97      0.92        30\n",
      "        bird       0.86      0.97      0.91      1050\n",
      "         cat       0.74      0.64      0.68        78\n",
      "         dog       0.98      0.91      0.94      1940\n",
      "       house       0.91      0.94      0.93        34\n",
      "        tree       0.81      0.84      0.83        51\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      3183\n",
      "   macro avg       0.86      0.88      0.87      3183\n",
      "weighted avg       0.93      0.92      0.92      3183\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd781ebcc0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_real = np.argmax(y_test, axis=1)\n",
    "confusion_mat = confusion_matrix(y_real, y_pred)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_mat)\n",
    "print()\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_real, y_pred, target_names=y_list))\n",
    "print()\n",
    "\n",
    "# labels = y_table.T[0]\n",
    "plt.figure(figsize=(4,4), dpi=100)\n",
    "plt.xticks(np.arange(len(y_list)), y_list)\n",
    "plt.yticks(np.arange(len(y_list)), y_list)\n",
    "plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.bone_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bed', 'bird', 'cat', 'dog', 'house', 'tree'], dtype='<U5')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
