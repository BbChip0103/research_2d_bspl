{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from  tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mnist.load_data(path='mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data[0][0][:,:,:,np.newaxis]\n",
    "y_data = data[0][1]\n",
    "x_test = data[1][0][:,:,:,np.newaxis]\n",
    "y_test = data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40200, 28, 28, 1),\n",
       " (40200, 10),\n",
       " (19800, 28, 28, 1),\n",
       " (19800, 10),\n",
       " (10000, 28, 28, 1),\n",
       " (10000, 10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.33, random_state=42, stratify=y_data)\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list = np.unique(data[1][1])\n",
    "y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_list.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    for i in range(conv_num):\n",
    "        x = Conv2D(kernel_size=3, filters=32*(2**(i//2)), strides=(1,1), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling2D(pool_size=2, strides=(2,2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "  \n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 63,050\n",
      "Trainable params: 63,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                15690     \n",
      "=================================================================\n",
      "Total params: 25,258\n",
      "Trainable params: 25,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 38,314\n",
      "Trainable params: 38,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 67,562\n",
      "Trainable params: 67,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 140,138\n",
      "Trainable params: 140,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model = build_2d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40200 samples, validate on 19800 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 6.4962 - acc: 0.5537\n",
      "Epoch 00001: val_loss improved from inf to 1.64379, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/001-1.6438.hdf5\n",
      "40200/40200 [==============================] - 6s 151us/sample - loss: 6.4955 - acc: 0.5537 - val_loss: 1.6438 - val_acc: 0.8726\n",
      "Epoch 2/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 2.2532 - acc: 0.8321\n",
      "Epoch 00002: val_loss improved from 1.64379 to 1.17090, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/002-1.1709.hdf5\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 2.2410 - acc: 0.8329 - val_loss: 1.1709 - val_acc: 0.9090\n",
      "Epoch 3/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 1.6320 - acc: 0.8768\n",
      "Epoch 00003: val_loss improved from 1.17090 to 1.01943, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/003-1.0194.hdf5\n",
      "40200/40200 [==============================] - 4s 103us/sample - loss: 1.6315 - acc: 0.8767 - val_loss: 1.0194 - val_acc: 0.9227\n",
      "Epoch 4/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 1.2559 - acc: 0.9016\n",
      "Epoch 00004: val_loss improved from 1.01943 to 0.77260, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/004-0.7726.hdf5\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 1.2536 - acc: 0.9019 - val_loss: 0.7726 - val_acc: 0.9384\n",
      "Epoch 5/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 1.0028 - acc: 0.9191\n",
      "Epoch 00005: val_loss improved from 0.77260 to 0.63096, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/005-0.6310.hdf5\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 1.0001 - acc: 0.9193 - val_loss: 0.6310 - val_acc: 0.9469\n",
      "Epoch 6/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.8480 - acc: 0.9294\n",
      "Epoch 00006: val_loss improved from 0.63096 to 0.53923, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/006-0.5392.hdf5\n",
      "40200/40200 [==============================] - 4s 97us/sample - loss: 0.8480 - acc: 0.9294 - val_loss: 0.5392 - val_acc: 0.9534\n",
      "Epoch 7/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.7122 - acc: 0.9379\n",
      "Epoch 00007: val_loss improved from 0.53923 to 0.43369, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/007-0.4337.hdf5\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.7101 - acc: 0.9381 - val_loss: 0.4337 - val_acc: 0.9605\n",
      "Epoch 8/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.6090 - acc: 0.9450\n",
      "Epoch 00008: val_loss improved from 0.43369 to 0.39090, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/008-0.3909.hdf5\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.6081 - acc: 0.9450 - val_loss: 0.3909 - val_acc: 0.9632\n",
      "Epoch 9/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.5263 - acc: 0.9498\n",
      "Epoch 00009: val_loss improved from 0.39090 to 0.34054, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/009-0.3405.hdf5\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.5252 - acc: 0.9500 - val_loss: 0.3405 - val_acc: 0.9672\n",
      "Epoch 10/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.4709 - acc: 0.9529\n",
      "Epoch 00010: val_loss improved from 0.34054 to 0.30065, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/010-0.3007.hdf5\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.4703 - acc: 0.9529 - val_loss: 0.3007 - val_acc: 0.9694\n",
      "Epoch 11/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.4197 - acc: 0.9563\n",
      "Epoch 00011: val_loss improved from 0.30065 to 0.26847, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/011-0.2685.hdf5\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.4187 - acc: 0.9564 - val_loss: 0.2685 - val_acc: 0.9712\n",
      "Epoch 12/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.9604\n",
      "Epoch 00012: val_loss improved from 0.26847 to 0.24354, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/012-0.2435.hdf5\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.3556 - acc: 0.9604 - val_loss: 0.2435 - val_acc: 0.9721\n",
      "Epoch 13/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.9608\n",
      "Epoch 00013: val_loss improved from 0.24354 to 0.21414, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/013-0.2141.hdf5\n",
      "40200/40200 [==============================] - 4s 103us/sample - loss: 0.3375 - acc: 0.9609 - val_loss: 0.2141 - val_acc: 0.9736\n",
      "Epoch 14/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9646\n",
      "Epoch 00014: val_loss improved from 0.21414 to 0.19364, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/014-0.1936.hdf5\n",
      "40200/40200 [==============================] - 4s 106us/sample - loss: 0.2981 - acc: 0.9646 - val_loss: 0.1936 - val_acc: 0.9749\n",
      "Epoch 15/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9651\n",
      "Epoch 00015: val_loss improved from 0.19364 to 0.18189, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/015-0.1819.hdf5\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.2705 - acc: 0.9651 - val_loss: 0.1819 - val_acc: 0.9770\n",
      "Epoch 16/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9661\n",
      "Epoch 00016: val_loss improved from 0.18189 to 0.16463, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/016-0.1646.hdf5\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.2417 - acc: 0.9662 - val_loss: 0.1646 - val_acc: 0.9774\n",
      "Epoch 17/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9682\n",
      "Epoch 00017: val_loss improved from 0.16463 to 0.15972, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/017-0.1597.hdf5\n",
      "40200/40200 [==============================] - 4s 106us/sample - loss: 0.2080 - acc: 0.9682 - val_loss: 0.1597 - val_acc: 0.9769\n",
      "Epoch 18/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9701\n",
      "Epoch 00018: val_loss improved from 0.15972 to 0.14551, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/018-0.1455.hdf5\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.1939 - acc: 0.9701 - val_loss: 0.1455 - val_acc: 0.9772\n",
      "Epoch 19/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9694\n",
      "Epoch 00019: val_loss improved from 0.14551 to 0.13006, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/019-0.1301.hdf5\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.1797 - acc: 0.9694 - val_loss: 0.1301 - val_acc: 0.9785\n",
      "Epoch 20/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9721\n",
      "Epoch 00020: val_loss improved from 0.13006 to 0.12166, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/020-0.1217.hdf5\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.1504 - acc: 0.9722 - val_loss: 0.1217 - val_acc: 0.9794\n",
      "Epoch 21/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9725\n",
      "Epoch 00021: val_loss improved from 0.12166 to 0.11413, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/021-0.1141.hdf5\n",
      "40200/40200 [==============================] - 4s 95us/sample - loss: 0.1435 - acc: 0.9726 - val_loss: 0.1141 - val_acc: 0.9799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9740\n",
      "Epoch 00022: val_loss improved from 0.11413 to 0.10725, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/022-0.1073.hdf5\n",
      "40200/40200 [==============================] - 4s 98us/sample - loss: 0.1243 - acc: 0.9739 - val_loss: 0.1073 - val_acc: 0.9808\n",
      "Epoch 23/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9750\n",
      "Epoch 00023: val_loss improved from 0.10725 to 0.10232, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/023-0.1023.hdf5\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.1117 - acc: 0.9750 - val_loss: 0.1023 - val_acc: 0.9806\n",
      "Epoch 24/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9753\n",
      "Epoch 00024: val_loss improved from 0.10232 to 0.09678, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/024-0.0968.hdf5\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.1043 - acc: 0.9753 - val_loss: 0.0968 - val_acc: 0.9813\n",
      "Epoch 25/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9779\n",
      "Epoch 00025: val_loss improved from 0.09678 to 0.09279, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/025-0.0928.hdf5\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0933 - acc: 0.9778 - val_loss: 0.0928 - val_acc: 0.9811\n",
      "Epoch 26/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9781\n",
      "Epoch 00026: val_loss improved from 0.09279 to 0.08960, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/026-0.0896.hdf5\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0863 - acc: 0.9781 - val_loss: 0.0896 - val_acc: 0.9819\n",
      "Epoch 27/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9793\n",
      "Epoch 00027: val_loss improved from 0.08960 to 0.08806, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/027-0.0881.hdf5\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0819 - acc: 0.9792 - val_loss: 0.0881 - val_acc: 0.9807\n",
      "Epoch 28/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9799\n",
      "Epoch 00028: val_loss improved from 0.08806 to 0.08533, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/028-0.0853.hdf5\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0741 - acc: 0.9800 - val_loss: 0.0853 - val_acc: 0.9812\n",
      "Epoch 29/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9810\n",
      "Epoch 00029: val_loss improved from 0.08533 to 0.08492, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/029-0.0849.hdf5\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0701 - acc: 0.9810 - val_loss: 0.0849 - val_acc: 0.9808\n",
      "Epoch 30/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9803\n",
      "Epoch 00030: val_loss improved from 0.08492 to 0.08114, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/030-0.0811.hdf5\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0682 - acc: 0.9803 - val_loss: 0.0811 - val_acc: 0.9823\n",
      "Epoch 31/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9825\n",
      "Epoch 00031: val_loss improved from 0.08114 to 0.07962, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/031-0.0796.hdf5\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0620 - acc: 0.9825 - val_loss: 0.0796 - val_acc: 0.9817\n",
      "Epoch 32/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9831\n",
      "Epoch 00032: val_loss improved from 0.07962 to 0.07791, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/032-0.0779.hdf5\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0578 - acc: 0.9831 - val_loss: 0.0779 - val_acc: 0.9817\n",
      "Epoch 33/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9836\n",
      "Epoch 00033: val_loss improved from 0.07791 to 0.07704, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/033-0.0770.hdf5\n",
      "40200/40200 [==============================] - 4s 103us/sample - loss: 0.0545 - acc: 0.9837 - val_loss: 0.0770 - val_acc: 0.9820\n",
      "Epoch 34/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9834\n",
      "Epoch 00034: val_loss improved from 0.07704 to 0.07657, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/034-0.0766.hdf5\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0546 - acc: 0.9833 - val_loss: 0.0766 - val_acc: 0.9828\n",
      "Epoch 35/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9845\n",
      "Epoch 00035: val_loss improved from 0.07657 to 0.07391, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/035-0.0739.hdf5\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0496 - acc: 0.9846 - val_loss: 0.0739 - val_acc: 0.9828\n",
      "Epoch 36/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9850\n",
      "Epoch 00036: val_loss did not improve from 0.07391\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0503 - acc: 0.9849 - val_loss: 0.0761 - val_acc: 0.9823\n",
      "Epoch 37/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9858\n",
      "Epoch 00037: val_loss did not improve from 0.07391\n",
      "40200/40200 [==============================] - 4s 98us/sample - loss: 0.0463 - acc: 0.9858 - val_loss: 0.0756 - val_acc: 0.9825\n",
      "Epoch 38/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9853\n",
      "Epoch 00038: val_loss did not improve from 0.07391\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0459 - acc: 0.9853 - val_loss: 0.0762 - val_acc: 0.9816\n",
      "Epoch 39/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9872\n",
      "Epoch 00039: val_loss did not improve from 0.07391\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0419 - acc: 0.9870 - val_loss: 0.0753 - val_acc: 0.9823\n",
      "Epoch 40/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9866\n",
      "Epoch 00040: val_loss did not improve from 0.07391\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0427 - acc: 0.9866 - val_loss: 0.0747 - val_acc: 0.9827\n",
      "Epoch 41/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9868\n",
      "Epoch 00041: val_loss improved from 0.07391 to 0.07270, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/041-0.0727.hdf5\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0428 - acc: 0.9868 - val_loss: 0.0727 - val_acc: 0.9824\n",
      "Epoch 42/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9877\n",
      "Epoch 00042: val_loss improved from 0.07270 to 0.07230, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/042-0.0723.hdf5\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0381 - acc: 0.9877 - val_loss: 0.0723 - val_acc: 0.9832\n",
      "Epoch 43/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9871\n",
      "Epoch 00043: val_loss did not improve from 0.07230\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0400 - acc: 0.9872 - val_loss: 0.0731 - val_acc: 0.9836\n",
      "Epoch 44/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9884\n",
      "Epoch 00044: val_loss did not improve from 0.07230\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0371 - acc: 0.9885 - val_loss: 0.0726 - val_acc: 0.9839\n",
      "Epoch 45/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9879\n",
      "Epoch 00045: val_loss did not improve from 0.07230\n",
      "40200/40200 [==============================] - 4s 103us/sample - loss: 0.0357 - acc: 0.9880 - val_loss: 0.0751 - val_acc: 0.9829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9875\n",
      "Epoch 00046: val_loss did not improve from 0.07230\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0373 - acc: 0.9875 - val_loss: 0.0766 - val_acc: 0.9822\n",
      "Epoch 47/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9894\n",
      "Epoch 00047: val_loss did not improve from 0.07230\n",
      "40200/40200 [==============================] - 4s 103us/sample - loss: 0.0327 - acc: 0.9894 - val_loss: 0.0728 - val_acc: 0.9831\n",
      "Epoch 48/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9895\n",
      "Epoch 00048: val_loss improved from 0.07230 to 0.07221, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/048-0.0722.hdf5\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0331 - acc: 0.9895 - val_loss: 0.0722 - val_acc: 0.9828\n",
      "Epoch 49/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9891\n",
      "Epoch 00049: val_loss did not improve from 0.07221\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0340 - acc: 0.9891 - val_loss: 0.0733 - val_acc: 0.9833\n",
      "Epoch 50/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9891\n",
      "Epoch 00050: val_loss did not improve from 0.07221\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0327 - acc: 0.9891 - val_loss: 0.0738 - val_acc: 0.9828\n",
      "Epoch 51/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9892\n",
      "Epoch 00051: val_loss did not improve from 0.07221\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0335 - acc: 0.9891 - val_loss: 0.0735 - val_acc: 0.9826\n",
      "Epoch 52/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9891\n",
      "Epoch 00052: val_loss did not improve from 0.07221\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0322 - acc: 0.9892 - val_loss: 0.0726 - val_acc: 0.9832\n",
      "Epoch 53/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9894\n",
      "Epoch 00053: val_loss did not improve from 0.07221\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0329 - acc: 0.9894 - val_loss: 0.0736 - val_acc: 0.9825\n",
      "Epoch 54/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9888\n",
      "Epoch 00054: val_loss did not improve from 0.07221\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0328 - acc: 0.9888 - val_loss: 0.0727 - val_acc: 0.9832\n",
      "Epoch 55/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9900\n",
      "Epoch 00055: val_loss did not improve from 0.07221\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0288 - acc: 0.9901 - val_loss: 0.0731 - val_acc: 0.9831\n",
      "Epoch 56/500\n",
      "39488/40200 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9900\n",
      "Epoch 00056: val_loss did not improve from 0.07221\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0293 - acc: 0.9900 - val_loss: 0.0737 - val_acc: 0.9826\n",
      "Epoch 57/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9901\n",
      "Epoch 00057: val_loss improved from 0.07221 to 0.07120, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/057-0.0712.hdf5\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0303 - acc: 0.9902 - val_loss: 0.0712 - val_acc: 0.9835\n",
      "Epoch 58/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9901\n",
      "Epoch 00058: val_loss did not improve from 0.07120\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0307 - acc: 0.9901 - val_loss: 0.0719 - val_acc: 0.9835\n",
      "Epoch 59/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9901\n",
      "Epoch 00059: val_loss did not improve from 0.07120\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0304 - acc: 0.9900 - val_loss: 0.0725 - val_acc: 0.9828\n",
      "Epoch 60/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9900\n",
      "Epoch 00060: val_loss did not improve from 0.07120\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0296 - acc: 0.9900 - val_loss: 0.0728 - val_acc: 0.9829\n",
      "Epoch 61/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9914\n",
      "Epoch 00061: val_loss did not improve from 0.07120\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0265 - acc: 0.9914 - val_loss: 0.0744 - val_acc: 0.9835\n",
      "Epoch 62/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9902\n",
      "Epoch 00062: val_loss did not improve from 0.07120\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0311 - acc: 0.9902 - val_loss: 0.0727 - val_acc: 0.9838\n",
      "Epoch 63/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9908\n",
      "Epoch 00063: val_loss did not improve from 0.07120\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0279 - acc: 0.9908 - val_loss: 0.0735 - val_acc: 0.9835\n",
      "Epoch 64/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9910\n",
      "Epoch 00064: val_loss did not improve from 0.07120\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0276 - acc: 0.9910 - val_loss: 0.0726 - val_acc: 0.9834\n",
      "Epoch 65/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9910\n",
      "Epoch 00065: val_loss improved from 0.07120 to 0.07092, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv_checkpoint/065-0.0709.hdf5\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0266 - acc: 0.9910 - val_loss: 0.0709 - val_acc: 0.9834\n",
      "Epoch 66/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9913\n",
      "Epoch 00066: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0271 - acc: 0.9913 - val_loss: 0.0740 - val_acc: 0.9830\n",
      "Epoch 67/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9916\n",
      "Epoch 00067: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0264 - acc: 0.9916 - val_loss: 0.0734 - val_acc: 0.9834\n",
      "Epoch 68/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9905\n",
      "Epoch 00068: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 103us/sample - loss: 0.0275 - acc: 0.9905 - val_loss: 0.0718 - val_acc: 0.9833\n",
      "Epoch 69/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9914\n",
      "Epoch 00069: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0262 - acc: 0.9915 - val_loss: 0.0758 - val_acc: 0.9829\n",
      "Epoch 70/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9918\n",
      "Epoch 00070: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0253 - acc: 0.9918 - val_loss: 0.0728 - val_acc: 0.9836\n",
      "Epoch 71/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9917\n",
      "Epoch 00071: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.0249 - acc: 0.9917 - val_loss: 0.0722 - val_acc: 0.9838\n",
      "Epoch 72/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9913\n",
      "Epoch 00072: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 89us/sample - loss: 0.0250 - acc: 0.9913 - val_loss: 0.0755 - val_acc: 0.9836\n",
      "Epoch 73/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9919\n",
      "Epoch 00073: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.0253 - acc: 0.9919 - val_loss: 0.0726 - val_acc: 0.9833\n",
      "Epoch 74/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9916\n",
      "Epoch 00074: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 106us/sample - loss: 0.0261 - acc: 0.9916 - val_loss: 0.0733 - val_acc: 0.9830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9919\n",
      "Epoch 00075: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0241 - acc: 0.9920 - val_loss: 0.0732 - val_acc: 0.9823\n",
      "Epoch 76/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9918\n",
      "Epoch 00076: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0248 - acc: 0.9919 - val_loss: 0.0765 - val_acc: 0.9825\n",
      "Epoch 77/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9910\n",
      "Epoch 00077: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 103us/sample - loss: 0.0263 - acc: 0.9910 - val_loss: 0.0732 - val_acc: 0.9835\n",
      "Epoch 78/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9912\n",
      "Epoch 00078: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0249 - acc: 0.9912 - val_loss: 0.0748 - val_acc: 0.9833\n",
      "Epoch 79/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9923\n",
      "Epoch 00079: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0225 - acc: 0.9923 - val_loss: 0.0735 - val_acc: 0.9833\n",
      "Epoch 80/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9918\n",
      "Epoch 00080: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0245 - acc: 0.9918 - val_loss: 0.0736 - val_acc: 0.9833\n",
      "Epoch 81/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9916\n",
      "Epoch 00081: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.0241 - acc: 0.9916 - val_loss: 0.0761 - val_acc: 0.9833\n",
      "Epoch 82/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9922\n",
      "Epoch 00082: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0243 - acc: 0.9922 - val_loss: 0.0729 - val_acc: 0.9832\n",
      "Epoch 83/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9923\n",
      "Epoch 00083: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0233 - acc: 0.9924 - val_loss: 0.0745 - val_acc: 0.9828\n",
      "Epoch 84/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9921\n",
      "Epoch 00084: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0234 - acc: 0.9920 - val_loss: 0.0719 - val_acc: 0.9833\n",
      "Epoch 85/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9920\n",
      "Epoch 00085: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0239 - acc: 0.9921 - val_loss: 0.0742 - val_acc: 0.9838\n",
      "Epoch 86/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9926\n",
      "Epoch 00086: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0224 - acc: 0.9926 - val_loss: 0.0722 - val_acc: 0.9836\n",
      "Epoch 87/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9927\n",
      "Epoch 00087: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 91us/sample - loss: 0.0221 - acc: 0.9927 - val_loss: 0.0741 - val_acc: 0.9837\n",
      "Epoch 88/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9922\n",
      "Epoch 00088: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0230 - acc: 0.9922 - val_loss: 0.0748 - val_acc: 0.9839\n",
      "Epoch 89/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9922\n",
      "Epoch 00089: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0228 - acc: 0.9921 - val_loss: 0.0747 - val_acc: 0.9832\n",
      "Epoch 90/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9923\n",
      "Epoch 00090: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0241 - acc: 0.9923 - val_loss: 0.0740 - val_acc: 0.9840\n",
      "Epoch 91/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9923\n",
      "Epoch 00091: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0234 - acc: 0.9924 - val_loss: 0.0741 - val_acc: 0.9838\n",
      "Epoch 92/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9918\n",
      "Epoch 00092: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.0235 - acc: 0.9918 - val_loss: 0.0740 - val_acc: 0.9829\n",
      "Epoch 93/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9927\n",
      "Epoch 00093: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.0221 - acc: 0.9927 - val_loss: 0.0759 - val_acc: 0.9838\n",
      "Epoch 94/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9923\n",
      "Epoch 00094: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.0216 - acc: 0.9923 - val_loss: 0.0743 - val_acc: 0.9832\n",
      "Epoch 95/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9927\n",
      "Epoch 00095: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0224 - acc: 0.9927 - val_loss: 0.0750 - val_acc: 0.9842\n",
      "Epoch 96/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9926\n",
      "Epoch 00096: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0218 - acc: 0.9926 - val_loss: 0.0741 - val_acc: 0.9835\n",
      "Epoch 97/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9926\n",
      "Epoch 00097: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0222 - acc: 0.9926 - val_loss: 0.0759 - val_acc: 0.9827\n",
      "Epoch 98/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9929\n",
      "Epoch 00098: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0198 - acc: 0.9929 - val_loss: 0.0752 - val_acc: 0.9834\n",
      "Epoch 99/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9925\n",
      "Epoch 00099: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0219 - acc: 0.9926 - val_loss: 0.0762 - val_acc: 0.9832\n",
      "Epoch 100/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9925\n",
      "Epoch 00100: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0224 - acc: 0.9925 - val_loss: 0.0766 - val_acc: 0.9834\n",
      "Epoch 101/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9932\n",
      "Epoch 00101: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0200 - acc: 0.9932 - val_loss: 0.0790 - val_acc: 0.9839\n",
      "Epoch 102/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9923\n",
      "Epoch 00102: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 89us/sample - loss: 0.0227 - acc: 0.9923 - val_loss: 0.0780 - val_acc: 0.9830\n",
      "Epoch 103/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9925\n",
      "Epoch 00103: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0221 - acc: 0.9925 - val_loss: 0.0763 - val_acc: 0.9834\n",
      "Epoch 104/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9929\n",
      "Epoch 00104: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0207 - acc: 0.9929 - val_loss: 0.0781 - val_acc: 0.9832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9924\n",
      "Epoch 00105: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0221 - acc: 0.9924 - val_loss: 0.0761 - val_acc: 0.9835\n",
      "Epoch 106/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9928\n",
      "Epoch 00106: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0220 - acc: 0.9928 - val_loss: 0.0761 - val_acc: 0.9837\n",
      "Epoch 107/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9922\n",
      "Epoch 00107: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0216 - acc: 0.9922 - val_loss: 0.0784 - val_acc: 0.9831\n",
      "Epoch 108/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9929\n",
      "Epoch 00108: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0215 - acc: 0.9929 - val_loss: 0.0755 - val_acc: 0.9833\n",
      "Epoch 109/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9934\n",
      "Epoch 00109: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0199 - acc: 0.9933 - val_loss: 0.0769 - val_acc: 0.9837\n",
      "Epoch 110/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9930\n",
      "Epoch 00110: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.0204 - acc: 0.9930 - val_loss: 0.0748 - val_acc: 0.9839\n",
      "Epoch 111/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9933\n",
      "Epoch 00111: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0204 - acc: 0.9932 - val_loss: 0.0767 - val_acc: 0.9838\n",
      "Epoch 112/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9936\n",
      "Epoch 00112: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0191 - acc: 0.9936 - val_loss: 0.0742 - val_acc: 0.9838\n",
      "Epoch 113/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9929\n",
      "Epoch 00113: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0209 - acc: 0.9929 - val_loss: 0.0754 - val_acc: 0.9829\n",
      "Epoch 114/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9933\n",
      "Epoch 00114: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0209 - acc: 0.9933 - val_loss: 0.0771 - val_acc: 0.9843\n",
      "Epoch 115/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9933\n",
      "Epoch 00115: val_loss did not improve from 0.07092\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0204 - acc: 0.9932 - val_loss: 0.0782 - val_acc: 0.9835\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 78us/sample - loss: 0.0674 - acc: 0.9837\n",
      "Loss: 0.06743853561100914 Accuracy: 0.9837\n",
      "\n",
      "Train on 40200 samples, validate on 19800 samples\n",
      "Epoch 1/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 7.1327 - acc: 0.4702\n",
      "Epoch 00001: val_loss improved from inf to 0.90392, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/001-0.9039.hdf5\n",
      "40200/40200 [==============================] - 5s 129us/sample - loss: 7.0928 - acc: 0.4724 - val_loss: 0.9039 - val_acc: 0.8624\n",
      "Epoch 2/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 1.4070 - acc: 0.7634\n",
      "Epoch 00002: val_loss improved from 0.90392 to 0.34181, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/002-0.3418.hdf5\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 1.4007 - acc: 0.7640 - val_loss: 0.3418 - val_acc: 0.8994\n",
      "Epoch 3/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.6573 - acc: 0.8194\n",
      "Epoch 00003: val_loss improved from 0.34181 to 0.23871, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/003-0.2387.hdf5\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.6570 - acc: 0.8195 - val_loss: 0.2387 - val_acc: 0.9263\n",
      "Epoch 4/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.4539 - acc: 0.8667\n",
      "Epoch 00004: val_loss improved from 0.23871 to 0.18319, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/004-0.1832.hdf5\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.4538 - acc: 0.8667 - val_loss: 0.1832 - val_acc: 0.9421\n",
      "Epoch 5/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.3550 - acc: 0.8925\n",
      "Epoch 00005: val_loss improved from 0.18319 to 0.15311, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/005-0.1531.hdf5\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.3544 - acc: 0.8926 - val_loss: 0.1531 - val_acc: 0.9515\n",
      "Epoch 6/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9115\n",
      "Epoch 00006: val_loss improved from 0.15311 to 0.13264, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/006-0.1326.hdf5\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.2957 - acc: 0.9114 - val_loss: 0.1326 - val_acc: 0.9594\n",
      "Epoch 7/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9241\n",
      "Epoch 00007: val_loss improved from 0.13264 to 0.11374, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/007-0.1137.hdf5\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.2458 - acc: 0.9241 - val_loss: 0.1137 - val_acc: 0.9640\n",
      "Epoch 8/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9348\n",
      "Epoch 00008: val_loss improved from 0.11374 to 0.10187, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/008-0.1019.hdf5\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.2166 - acc: 0.9346 - val_loss: 0.1019 - val_acc: 0.9686\n",
      "Epoch 9/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9428\n",
      "Epoch 00009: val_loss improved from 0.10187 to 0.08942, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/009-0.0894.hdf5\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.1878 - acc: 0.9428 - val_loss: 0.0894 - val_acc: 0.9723\n",
      "Epoch 10/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.1621 - acc: 0.9486\n",
      "Epoch 00010: val_loss improved from 0.08942 to 0.08421, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/010-0.0842.hdf5\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.1632 - acc: 0.9483 - val_loss: 0.0842 - val_acc: 0.9738\n",
      "Epoch 11/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9542\n",
      "Epoch 00011: val_loss improved from 0.08421 to 0.07933, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/011-0.0793.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.1504 - acc: 0.9542 - val_loss: 0.0793 - val_acc: 0.9748\n",
      "Epoch 12/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9578\n",
      "Epoch 00012: val_loss improved from 0.07933 to 0.07241, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/012-0.0724.hdf5\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.1365 - acc: 0.9577 - val_loss: 0.0724 - val_acc: 0.9767\n",
      "Epoch 13/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9622\n",
      "Epoch 00013: val_loss improved from 0.07241 to 0.06851, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/013-0.0685.hdf5\n",
      "40200/40200 [==============================] - 4s 111us/sample - loss: 0.1229 - acc: 0.9623 - val_loss: 0.0685 - val_acc: 0.9784\n",
      "Epoch 14/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9624\n",
      "Epoch 00014: val_loss improved from 0.06851 to 0.06514, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/014-0.0651.hdf5\n",
      "40200/40200 [==============================] - 5s 114us/sample - loss: 0.1197 - acc: 0.9624 - val_loss: 0.0651 - val_acc: 0.9795\n",
      "Epoch 15/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9668\n",
      "Epoch 00015: val_loss improved from 0.06514 to 0.05989, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/015-0.0599.hdf5\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.1076 - acc: 0.9668 - val_loss: 0.0599 - val_acc: 0.9816\n",
      "Epoch 16/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9679\n",
      "Epoch 00016: val_loss improved from 0.05989 to 0.05914, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/016-0.0591.hdf5\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.1019 - acc: 0.9679 - val_loss: 0.0591 - val_acc: 0.9821\n",
      "Epoch 17/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9710\n",
      "Epoch 00017: val_loss improved from 0.05914 to 0.05696, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/017-0.0570.hdf5\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0928 - acc: 0.9710 - val_loss: 0.0570 - val_acc: 0.9827\n",
      "Epoch 18/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9708\n",
      "Epoch 00018: val_loss improved from 0.05696 to 0.05615, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/018-0.0561.hdf5\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0899 - acc: 0.9708 - val_loss: 0.0561 - val_acc: 0.9830\n",
      "Epoch 19/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9718\n",
      "Epoch 00019: val_loss did not improve from 0.05615\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0858 - acc: 0.9718 - val_loss: 0.0575 - val_acc: 0.9828\n",
      "Epoch 20/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9744\n",
      "Epoch 00020: val_loss improved from 0.05615 to 0.05310, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/020-0.0531.hdf5\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0808 - acc: 0.9744 - val_loss: 0.0531 - val_acc: 0.9842\n",
      "Epoch 21/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9740\n",
      "Epoch 00021: val_loss did not improve from 0.05310\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0787 - acc: 0.9740 - val_loss: 0.0551 - val_acc: 0.9830\n",
      "Epoch 22/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9749\n",
      "Epoch 00022: val_loss improved from 0.05310 to 0.05017, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/022-0.0502.hdf5\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0777 - acc: 0.9749 - val_loss: 0.0502 - val_acc: 0.9851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9778\n",
      "Epoch 00023: val_loss did not improve from 0.05017\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0726 - acc: 0.9777 - val_loss: 0.0532 - val_acc: 0.9839\n",
      "Epoch 24/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9780\n",
      "Epoch 00024: val_loss improved from 0.05017 to 0.04889, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/024-0.0489.hdf5\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0696 - acc: 0.9780 - val_loss: 0.0489 - val_acc: 0.9847\n",
      "Epoch 25/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9786\n",
      "Epoch 00025: val_loss did not improve from 0.04889\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0646 - acc: 0.9786 - val_loss: 0.0495 - val_acc: 0.9853\n",
      "Epoch 26/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9797\n",
      "Epoch 00026: val_loss did not improve from 0.04889\n",
      "40200/40200 [==============================] - 4s 106us/sample - loss: 0.0644 - acc: 0.9797 - val_loss: 0.0496 - val_acc: 0.9856\n",
      "Epoch 27/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9797\n",
      "Epoch 00027: val_loss improved from 0.04889 to 0.04764, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/027-0.0476.hdf5\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0619 - acc: 0.9797 - val_loss: 0.0476 - val_acc: 0.9861\n",
      "Epoch 28/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9806\n",
      "Epoch 00028: val_loss did not improve from 0.04764\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0595 - acc: 0.9804 - val_loss: 0.0500 - val_acc: 0.9854\n",
      "Epoch 29/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9811\n",
      "Epoch 00029: val_loss did not improve from 0.04764\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0583 - acc: 0.9812 - val_loss: 0.0477 - val_acc: 0.9858\n",
      "Epoch 30/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9818\n",
      "Epoch 00030: val_loss did not improve from 0.04764\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0570 - acc: 0.9818 - val_loss: 0.0497 - val_acc: 0.9848\n",
      "Epoch 31/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9809\n",
      "Epoch 00031: val_loss improved from 0.04764 to 0.04761, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/031-0.0476.hdf5\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0564 - acc: 0.9808 - val_loss: 0.0476 - val_acc: 0.9867\n",
      "Epoch 32/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9823\n",
      "Epoch 00032: val_loss improved from 0.04761 to 0.04547, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/032-0.0455.hdf5\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0534 - acc: 0.9823 - val_loss: 0.0455 - val_acc: 0.9871\n",
      "Epoch 33/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9823\n",
      "Epoch 00033: val_loss did not improve from 0.04547\n",
      "40200/40200 [==============================] - 5s 130us/sample - loss: 0.0530 - acc: 0.9823 - val_loss: 0.0467 - val_acc: 0.9860\n",
      "Epoch 34/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9833\n",
      "Epoch 00034: val_loss did not improve from 0.04547\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0510 - acc: 0.9833 - val_loss: 0.0457 - val_acc: 0.9867\n",
      "Epoch 35/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9840\n",
      "Epoch 00035: val_loss did not improve from 0.04547\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0484 - acc: 0.9839 - val_loss: 0.0477 - val_acc: 0.9865\n",
      "Epoch 36/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9842\n",
      "Epoch 00036: val_loss did not improve from 0.04547\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0475 - acc: 0.9842 - val_loss: 0.0465 - val_acc: 0.9867\n",
      "Epoch 37/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9840\n",
      "Epoch 00037: val_loss improved from 0.04547 to 0.04366, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/037-0.0437.hdf5\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0476 - acc: 0.9841 - val_loss: 0.0437 - val_acc: 0.9871\n",
      "Epoch 38/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9844\n",
      "Epoch 00038: val_loss did not improve from 0.04366\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.0485 - acc: 0.9844 - val_loss: 0.0448 - val_acc: 0.9863\n",
      "Epoch 39/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9836\n",
      "Epoch 00039: val_loss did not improve from 0.04366\n",
      "40200/40200 [==============================] - 4s 111us/sample - loss: 0.0496 - acc: 0.9836 - val_loss: 0.0439 - val_acc: 0.9872\n",
      "Epoch 40/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9848\n",
      "Epoch 00040: val_loss did not improve from 0.04366\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0458 - acc: 0.9848 - val_loss: 0.0440 - val_acc: 0.9867\n",
      "Epoch 41/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9856\n",
      "Epoch 00041: val_loss did not improve from 0.04366\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0417 - acc: 0.9856 - val_loss: 0.0444 - val_acc: 0.9860\n",
      "Epoch 42/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9867\n",
      "Epoch 00042: val_loss did not improve from 0.04366\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0417 - acc: 0.9866 - val_loss: 0.0437 - val_acc: 0.9871\n",
      "Epoch 43/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9868\n",
      "Epoch 00043: val_loss did not improve from 0.04366\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0385 - acc: 0.9868 - val_loss: 0.0480 - val_acc: 0.9864\n",
      "Epoch 44/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9870\n",
      "Epoch 00044: val_loss improved from 0.04366 to 0.04310, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/044-0.0431.hdf5\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0393 - acc: 0.9870 - val_loss: 0.0431 - val_acc: 0.9876\n",
      "Epoch 45/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9863\n",
      "Epoch 00045: val_loss improved from 0.04310 to 0.04274, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/045-0.0427.hdf5\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0407 - acc: 0.9862 - val_loss: 0.0427 - val_acc: 0.9874\n",
      "Epoch 46/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9858\n",
      "Epoch 00046: val_loss did not improve from 0.04274\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0426 - acc: 0.9858 - val_loss: 0.0435 - val_acc: 0.9875\n",
      "Epoch 47/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9874\n",
      "Epoch 00047: val_loss did not improve from 0.04274\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0362 - acc: 0.9874 - val_loss: 0.0442 - val_acc: 0.9879\n",
      "Epoch 48/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9874\n",
      "Epoch 00048: val_loss did not improve from 0.04274\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0384 - acc: 0.9874 - val_loss: 0.0444 - val_acc: 0.9873\n",
      "Epoch 49/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9883\n",
      "Epoch 00049: val_loss improved from 0.04274 to 0.04202, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/049-0.0420.hdf5\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0372 - acc: 0.9883 - val_loss: 0.0420 - val_acc: 0.9872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9877\n",
      "Epoch 00050: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0367 - acc: 0.9878 - val_loss: 0.0444 - val_acc: 0.9876\n",
      "Epoch 51/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9874\n",
      "Epoch 00051: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0385 - acc: 0.9874 - val_loss: 0.0432 - val_acc: 0.9877\n",
      "Epoch 52/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9883\n",
      "Epoch 00052: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 5s 115us/sample - loss: 0.0365 - acc: 0.9883 - val_loss: 0.0456 - val_acc: 0.9876\n",
      "Epoch 53/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9875\n",
      "Epoch 00053: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0371 - acc: 0.9875 - val_loss: 0.0436 - val_acc: 0.9877\n",
      "Epoch 54/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9876\n",
      "Epoch 00054: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0352 - acc: 0.9876 - val_loss: 0.0432 - val_acc: 0.9878\n",
      "Epoch 55/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9883\n",
      "Epoch 00055: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0335 - acc: 0.9883 - val_loss: 0.0425 - val_acc: 0.9882\n",
      "Epoch 56/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9886\n",
      "Epoch 00056: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0347 - acc: 0.9886 - val_loss: 0.0439 - val_acc: 0.9878\n",
      "Epoch 57/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9881\n",
      "Epoch 00057: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0344 - acc: 0.9882 - val_loss: 0.0439 - val_acc: 0.9874\n",
      "Epoch 58/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9892\n",
      "Epoch 00058: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0332 - acc: 0.9892 - val_loss: 0.0441 - val_acc: 0.9881\n",
      "Epoch 59/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9892\n",
      "Epoch 00059: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0325 - acc: 0.9892 - val_loss: 0.0431 - val_acc: 0.9879\n",
      "Epoch 60/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9886\n",
      "Epoch 00060: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 5s 113us/sample - loss: 0.0327 - acc: 0.9885 - val_loss: 0.0439 - val_acc: 0.9880\n",
      "Epoch 61/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9893\n",
      "Epoch 00061: val_loss did not improve from 0.04202\n",
      "40200/40200 [==============================] - 5s 113us/sample - loss: 0.0333 - acc: 0.9893 - val_loss: 0.0457 - val_acc: 0.9874\n",
      "Epoch 62/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9881\n",
      "Epoch 00062: val_loss improved from 0.04202 to 0.04201, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/062-0.0420.hdf5\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0355 - acc: 0.9881 - val_loss: 0.0420 - val_acc: 0.9884\n",
      "Epoch 63/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9895\n",
      "Epoch 00063: val_loss did not improve from 0.04201\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0322 - acc: 0.9896 - val_loss: 0.0430 - val_acc: 0.9872\n",
      "Epoch 64/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9899\n",
      "Epoch 00064: val_loss did not improve from 0.04201\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0317 - acc: 0.9899 - val_loss: 0.0429 - val_acc: 0.9877\n",
      "Epoch 65/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9899\n",
      "Epoch 00065: val_loss improved from 0.04201 to 0.04195, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/065-0.0419.hdf5\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0304 - acc: 0.9899 - val_loss: 0.0419 - val_acc: 0.9885\n",
      "Epoch 66/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9893\n",
      "Epoch 00066: val_loss did not improve from 0.04195\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0301 - acc: 0.9893 - val_loss: 0.0434 - val_acc: 0.9881\n",
      "Epoch 67/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9896\n",
      "Epoch 00067: val_loss did not improve from 0.04195\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0292 - acc: 0.9897 - val_loss: 0.0461 - val_acc: 0.9875\n",
      "Epoch 68/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9904\n",
      "Epoch 00068: val_loss did not improve from 0.04195\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0283 - acc: 0.9905 - val_loss: 0.0438 - val_acc: 0.9885\n",
      "Epoch 69/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9901\n",
      "Epoch 00069: val_loss did not improve from 0.04195\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0290 - acc: 0.9901 - val_loss: 0.0428 - val_acc: 0.9879\n",
      "Epoch 70/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9895\n",
      "Epoch 00070: val_loss did not improve from 0.04195\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0304 - acc: 0.9895 - val_loss: 0.0437 - val_acc: 0.9883\n",
      "Epoch 71/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9900\n",
      "Epoch 00071: val_loss did not improve from 0.04195\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0298 - acc: 0.9900 - val_loss: 0.0424 - val_acc: 0.9888\n",
      "Epoch 72/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9905\n",
      "Epoch 00072: val_loss improved from 0.04195 to 0.04194, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/072-0.0419.hdf5\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0279 - acc: 0.9905 - val_loss: 0.0419 - val_acc: 0.9891\n",
      "Epoch 73/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9908\n",
      "Epoch 00073: val_loss improved from 0.04194 to 0.04107, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/073-0.0411.hdf5\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0280 - acc: 0.9907 - val_loss: 0.0411 - val_acc: 0.9885\n",
      "Epoch 74/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9907\n",
      "Epoch 00074: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0282 - acc: 0.9907 - val_loss: 0.0438 - val_acc: 0.9885\n",
      "Epoch 75/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9905\n",
      "Epoch 00075: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0278 - acc: 0.9905 - val_loss: 0.0428 - val_acc: 0.9886\n",
      "Epoch 76/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9904\n",
      "Epoch 00076: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0291 - acc: 0.9904 - val_loss: 0.0448 - val_acc: 0.9882\n",
      "Epoch 77/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9911\n",
      "Epoch 00077: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0273 - acc: 0.9911 - val_loss: 0.0424 - val_acc: 0.9891\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9911\n",
      "Epoch 00078: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0272 - acc: 0.9911 - val_loss: 0.0446 - val_acc: 0.9884\n",
      "Epoch 79/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9912\n",
      "Epoch 00079: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0264 - acc: 0.9912 - val_loss: 0.0417 - val_acc: 0.9888\n",
      "Epoch 80/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9910\n",
      "Epoch 00080: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0260 - acc: 0.9910 - val_loss: 0.0440 - val_acc: 0.9878\n",
      "Epoch 81/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9904\n",
      "Epoch 00081: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0273 - acc: 0.9904 - val_loss: 0.0423 - val_acc: 0.9888\n",
      "Epoch 82/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9911\n",
      "Epoch 00082: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0267 - acc: 0.9910 - val_loss: 0.0422 - val_acc: 0.9890\n",
      "Epoch 83/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9913\n",
      "Epoch 00083: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0251 - acc: 0.9914 - val_loss: 0.0427 - val_acc: 0.9890\n",
      "Epoch 84/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9913\n",
      "Epoch 00084: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0250 - acc: 0.9913 - val_loss: 0.0420 - val_acc: 0.9887\n",
      "Epoch 85/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9906\n",
      "Epoch 00085: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0273 - acc: 0.9906 - val_loss: 0.0435 - val_acc: 0.9875\n",
      "Epoch 86/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9917\n",
      "Epoch 00086: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0240 - acc: 0.9917 - val_loss: 0.0449 - val_acc: 0.9886\n",
      "Epoch 87/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9912\n",
      "Epoch 00087: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0254 - acc: 0.9912 - val_loss: 0.0422 - val_acc: 0.9890\n",
      "Epoch 88/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9917\n",
      "Epoch 00088: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0255 - acc: 0.9917 - val_loss: 0.0445 - val_acc: 0.9882\n",
      "Epoch 89/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9910\n",
      "Epoch 00089: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 114us/sample - loss: 0.0253 - acc: 0.9910 - val_loss: 0.0425 - val_acc: 0.9886\n",
      "Epoch 90/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9917\n",
      "Epoch 00090: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0234 - acc: 0.9917 - val_loss: 0.0412 - val_acc: 0.9891\n",
      "Epoch 91/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9915\n",
      "Epoch 00091: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0248 - acc: 0.9915 - val_loss: 0.0433 - val_acc: 0.9892\n",
      "Epoch 92/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9921\n",
      "Epoch 00092: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0242 - acc: 0.9921 - val_loss: 0.0444 - val_acc: 0.9883\n",
      "Epoch 93/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9914\n",
      "Epoch 00093: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0250 - acc: 0.9914 - val_loss: 0.0442 - val_acc: 0.9884\n",
      "Epoch 94/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9914\n",
      "Epoch 00094: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0250 - acc: 0.9914 - val_loss: 0.0426 - val_acc: 0.9885\n",
      "Epoch 95/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9917\n",
      "Epoch 00095: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0234 - acc: 0.9917 - val_loss: 0.0432 - val_acc: 0.9888\n",
      "Epoch 96/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9920\n",
      "Epoch 00096: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0247 - acc: 0.9920 - val_loss: 0.0420 - val_acc: 0.9893\n",
      "Epoch 97/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9918\n",
      "Epoch 00097: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0255 - acc: 0.9918 - val_loss: 0.0433 - val_acc: 0.9891\n",
      "Epoch 98/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9917\n",
      "Epoch 00098: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0240 - acc: 0.9917 - val_loss: 0.0423 - val_acc: 0.9891\n",
      "Epoch 99/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9914\n",
      "Epoch 00099: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0244 - acc: 0.9913 - val_loss: 0.0441 - val_acc: 0.9885\n",
      "Epoch 100/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9923\n",
      "Epoch 00100: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0231 - acc: 0.9923 - val_loss: 0.0439 - val_acc: 0.9888\n",
      "Epoch 101/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9924\n",
      "Epoch 00101: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0229 - acc: 0.9924 - val_loss: 0.0444 - val_acc: 0.9886\n",
      "Epoch 102/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9919\n",
      "Epoch 00102: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 113us/sample - loss: 0.0242 - acc: 0.9919 - val_loss: 0.0448 - val_acc: 0.9886\n",
      "Epoch 103/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9924\n",
      "Epoch 00103: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.0240 - acc: 0.9924 - val_loss: 0.0428 - val_acc: 0.9896\n",
      "Epoch 104/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9925\n",
      "Epoch 00104: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 116us/sample - loss: 0.0223 - acc: 0.9925 - val_loss: 0.0453 - val_acc: 0.9887\n",
      "Epoch 105/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9924\n",
      "Epoch 00105: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 116us/sample - loss: 0.0227 - acc: 0.9924 - val_loss: 0.0441 - val_acc: 0.9895\n",
      "Epoch 106/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9924\n",
      "Epoch 00106: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0221 - acc: 0.9925 - val_loss: 0.0447 - val_acc: 0.9889\n",
      "Epoch 107/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9928\n",
      "Epoch 00107: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0225 - acc: 0.9928 - val_loss: 0.0439 - val_acc: 0.9893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9922\n",
      "Epoch 00108: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0218 - acc: 0.9921 - val_loss: 0.0432 - val_acc: 0.9888\n",
      "Epoch 109/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9924\n",
      "Epoch 00109: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0218 - acc: 0.9924 - val_loss: 0.0437 - val_acc: 0.9890\n",
      "Epoch 110/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9922\n",
      "Epoch 00110: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0227 - acc: 0.9921 - val_loss: 0.0438 - val_acc: 0.9891\n",
      "Epoch 111/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9923\n",
      "Epoch 00111: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0216 - acc: 0.9924 - val_loss: 0.0462 - val_acc: 0.9881\n",
      "Epoch 112/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9930\n",
      "Epoch 00112: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0204 - acc: 0.9930 - val_loss: 0.0425 - val_acc: 0.9890\n",
      "Epoch 113/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9928\n",
      "Epoch 00113: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0205 - acc: 0.9928 - val_loss: 0.0448 - val_acc: 0.9890\n",
      "Epoch 114/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9920\n",
      "Epoch 00114: val_loss did not improve from 0.04107\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0229 - acc: 0.9920 - val_loss: 0.0444 - val_acc: 0.9890\n",
      "Epoch 115/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9920\n",
      "Epoch 00115: val_loss improved from 0.04107 to 0.04089, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv_checkpoint/115-0.0409.hdf5\n",
      "40200/40200 [==============================] - 4s 112us/sample - loss: 0.0237 - acc: 0.9920 - val_loss: 0.0409 - val_acc: 0.9896\n",
      "Epoch 116/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9925\n",
      "Epoch 00116: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 112us/sample - loss: 0.0224 - acc: 0.9925 - val_loss: 0.0423 - val_acc: 0.9890\n",
      "Epoch 117/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9923\n",
      "Epoch 00117: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0228 - acc: 0.9923 - val_loss: 0.0451 - val_acc: 0.9889\n",
      "Epoch 118/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9922\n",
      "Epoch 00118: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0230 - acc: 0.9922 - val_loss: 0.0463 - val_acc: 0.9877\n",
      "Epoch 119/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9925\n",
      "Epoch 00119: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0217 - acc: 0.9925 - val_loss: 0.0451 - val_acc: 0.9888\n",
      "Epoch 120/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9924\n",
      "Epoch 00120: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0228 - acc: 0.9924 - val_loss: 0.0425 - val_acc: 0.9890\n",
      "Epoch 121/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9927\n",
      "Epoch 00121: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0217 - acc: 0.9926 - val_loss: 0.0434 - val_acc: 0.9888\n",
      "Epoch 122/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9928\n",
      "Epoch 00122: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0212 - acc: 0.9929 - val_loss: 0.0424 - val_acc: 0.9891\n",
      "Epoch 123/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9930\n",
      "Epoch 00123: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0213 - acc: 0.9930 - val_loss: 0.0450 - val_acc: 0.9891\n",
      "Epoch 124/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9932\n",
      "Epoch 00124: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0188 - acc: 0.9932 - val_loss: 0.0442 - val_acc: 0.9887\n",
      "Epoch 125/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9933\n",
      "Epoch 00125: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0197 - acc: 0.9933 - val_loss: 0.0462 - val_acc: 0.9891\n",
      "Epoch 126/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9926\n",
      "Epoch 00126: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0215 - acc: 0.9926 - val_loss: 0.0455 - val_acc: 0.9886\n",
      "Epoch 127/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9931\n",
      "Epoch 00127: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0206 - acc: 0.9932 - val_loss: 0.0447 - val_acc: 0.9889\n",
      "Epoch 128/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9931\n",
      "Epoch 00128: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0205 - acc: 0.9930 - val_loss: 0.0430 - val_acc: 0.9894\n",
      "Epoch 129/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9928\n",
      "Epoch 00129: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0212 - acc: 0.9928 - val_loss: 0.0431 - val_acc: 0.9896\n",
      "Epoch 130/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9938\n",
      "Epoch 00130: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0184 - acc: 0.9938 - val_loss: 0.0439 - val_acc: 0.9889\n",
      "Epoch 131/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9926\n",
      "Epoch 00131: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0212 - acc: 0.9926 - val_loss: 0.0446 - val_acc: 0.9889\n",
      "Epoch 132/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9936\n",
      "Epoch 00132: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0191 - acc: 0.9936 - val_loss: 0.0441 - val_acc: 0.9896\n",
      "Epoch 133/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9924\n",
      "Epoch 00133: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0215 - acc: 0.9923 - val_loss: 0.0423 - val_acc: 0.9896\n",
      "Epoch 134/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9937\n",
      "Epoch 00134: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0189 - acc: 0.9937 - val_loss: 0.0448 - val_acc: 0.9890\n",
      "Epoch 135/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9929\n",
      "Epoch 00135: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0196 - acc: 0.9929 - val_loss: 0.0465 - val_acc: 0.9888\n",
      "Epoch 136/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9939\n",
      "Epoch 00136: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0180 - acc: 0.9939 - val_loss: 0.0452 - val_acc: 0.9891\n",
      "Epoch 137/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9927\n",
      "Epoch 00137: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0195 - acc: 0.9927 - val_loss: 0.0444 - val_acc: 0.9893\n",
      "Epoch 138/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9938\n",
      "Epoch 00138: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0191 - acc: 0.9938 - val_loss: 0.0470 - val_acc: 0.9885\n",
      "Epoch 139/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9935\n",
      "Epoch 00139: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 116us/sample - loss: 0.0186 - acc: 0.9935 - val_loss: 0.0460 - val_acc: 0.9892\n",
      "Epoch 140/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9937\n",
      "Epoch 00140: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0184 - acc: 0.9937 - val_loss: 0.0462 - val_acc: 0.9891\n",
      "Epoch 141/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9928\n",
      "Epoch 00141: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0198 - acc: 0.9927 - val_loss: 0.0428 - val_acc: 0.9889\n",
      "Epoch 142/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9935\n",
      "Epoch 00142: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0436 - val_acc: 0.9889\n",
      "Epoch 143/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9937\n",
      "Epoch 00143: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0187 - acc: 0.9936 - val_loss: 0.0455 - val_acc: 0.9889\n",
      "Epoch 144/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9939\n",
      "Epoch 00144: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0179 - acc: 0.9939 - val_loss: 0.0467 - val_acc: 0.9888\n",
      "Epoch 145/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9937\n",
      "Epoch 00145: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0446 - val_acc: 0.9895\n",
      "Epoch 146/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9935\n",
      "Epoch 00146: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0195 - acc: 0.9935 - val_loss: 0.0450 - val_acc: 0.9888\n",
      "Epoch 147/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9939\n",
      "Epoch 00147: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0182 - acc: 0.9939 - val_loss: 0.0430 - val_acc: 0.9892\n",
      "Epoch 148/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9939\n",
      "Epoch 00148: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0176 - acc: 0.9939 - val_loss: 0.0463 - val_acc: 0.9887\n",
      "Epoch 149/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9931\n",
      "Epoch 00149: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0196 - acc: 0.9931 - val_loss: 0.0448 - val_acc: 0.9898\n",
      "Epoch 150/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9941\n",
      "Epoch 00150: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0175 - acc: 0.9940 - val_loss: 0.0428 - val_acc: 0.9894\n",
      "Epoch 151/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9937\n",
      "Epoch 00151: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0184 - acc: 0.9937 - val_loss: 0.0434 - val_acc: 0.9887\n",
      "Epoch 152/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9937\n",
      "Epoch 00152: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0190 - acc: 0.9936 - val_loss: 0.0444 - val_acc: 0.9890\n",
      "Epoch 153/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9937\n",
      "Epoch 00153: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 113us/sample - loss: 0.0193 - acc: 0.9937 - val_loss: 0.0449 - val_acc: 0.9897\n",
      "Epoch 154/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9943\n",
      "Epoch 00154: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0165 - acc: 0.9943 - val_loss: 0.0433 - val_acc: 0.9894\n",
      "Epoch 155/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9940\n",
      "Epoch 00155: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0175 - acc: 0.9940 - val_loss: 0.0440 - val_acc: 0.9897\n",
      "Epoch 156/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9943\n",
      "Epoch 00156: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 116us/sample - loss: 0.0166 - acc: 0.9943 - val_loss: 0.0437 - val_acc: 0.9895\n",
      "Epoch 157/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9932\n",
      "Epoch 00157: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0208 - acc: 0.9931 - val_loss: 0.0447 - val_acc: 0.9899\n",
      "Epoch 158/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9936\n",
      "Epoch 00158: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0197 - acc: 0.9936 - val_loss: 0.0452 - val_acc: 0.9889\n",
      "Epoch 159/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9941\n",
      "Epoch 00159: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0181 - acc: 0.9941 - val_loss: 0.0481 - val_acc: 0.9891\n",
      "Epoch 160/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9938\n",
      "Epoch 00160: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0182 - acc: 0.9938 - val_loss: 0.0461 - val_acc: 0.9887\n",
      "Epoch 161/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9937\n",
      "Epoch 00161: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0170 - acc: 0.9938 - val_loss: 0.0449 - val_acc: 0.9898\n",
      "Epoch 162/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9936\n",
      "Epoch 00162: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0175 - acc: 0.9936 - val_loss: 0.0431 - val_acc: 0.9903\n",
      "Epoch 163/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9939\n",
      "Epoch 00163: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0182 - acc: 0.9939 - val_loss: 0.0449 - val_acc: 0.9892\n",
      "Epoch 164/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9946\n",
      "Epoch 00164: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0165 - acc: 0.9946 - val_loss: 0.0453 - val_acc: 0.9890\n",
      "Epoch 165/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9938\n",
      "Epoch 00165: val_loss did not improve from 0.04089\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0185 - acc: 0.9938 - val_loss: 0.0456 - val_acc: 0.9892\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 90us/sample - loss: 0.0343 - acc: 0.9891\n",
      "Loss: 0.034315878265905396 Accuracy: 0.9891\n",
      "\n",
      "Train on 40200 samples, validate on 19800 samples\n",
      "Epoch 1/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 3.3498 - acc: 0.5511\n",
      "Epoch 00001: val_loss improved from inf to 0.32809, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/001-0.3281.hdf5\n",
      "40200/40200 [==============================] - 6s 161us/sample - loss: 3.3494 - acc: 0.5511 - val_loss: 0.3281 - val_acc: 0.8973\n",
      "Epoch 2/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.8423\n",
      "Epoch 00002: val_loss improved from 0.32809 to 0.18597, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/002-0.1860.hdf5\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.4956 - acc: 0.8424 - val_loss: 0.1860 - val_acc: 0.9411\n",
      "Epoch 3/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9014\n",
      "Epoch 00003: val_loss improved from 0.18597 to 0.13358, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/003-0.1336.hdf5\n",
      "40200/40200 [==============================] - 5s 133us/sample - loss: 0.3111 - acc: 0.9015 - val_loss: 0.1336 - val_acc: 0.9572\n",
      "Epoch 4/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9265\n",
      "Epoch 00004: val_loss improved from 0.13358 to 0.10764, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/004-0.1076.hdf5\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.2277 - acc: 0.9268 - val_loss: 0.1076 - val_acc: 0.9653\n",
      "Epoch 5/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9435\n",
      "Epoch 00005: val_loss improved from 0.10764 to 0.09351, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/005-0.0935.hdf5\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.1789 - acc: 0.9434 - val_loss: 0.0935 - val_acc: 0.9699\n",
      "Epoch 6/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9528\n",
      "Epoch 00006: val_loss improved from 0.09351 to 0.08039, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/006-0.0804.hdf5\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.1464 - acc: 0.9529 - val_loss: 0.0804 - val_acc: 0.9746\n",
      "Epoch 7/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9589\n",
      "Epoch 00007: val_loss improved from 0.08039 to 0.07349, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/007-0.0735.hdf5\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.1274 - acc: 0.9589 - val_loss: 0.0735 - val_acc: 0.9771\n",
      "Epoch 8/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9653\n",
      "Epoch 00008: val_loss improved from 0.07349 to 0.06550, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/008-0.0655.hdf5\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.1093 - acc: 0.9654 - val_loss: 0.0655 - val_acc: 0.9791\n",
      "Epoch 9/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9690\n",
      "Epoch 00009: val_loss improved from 0.06550 to 0.06076, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/009-0.0608.hdf5\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0965 - acc: 0.9691 - val_loss: 0.0608 - val_acc: 0.9802\n",
      "Epoch 10/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9712\n",
      "Epoch 00010: val_loss improved from 0.06076 to 0.05672, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/010-0.0567.hdf5\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0879 - acc: 0.9713 - val_loss: 0.0567 - val_acc: 0.9819\n",
      "Epoch 11/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9739\n",
      "Epoch 00011: val_loss improved from 0.05672 to 0.05225, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/011-0.0522.hdf5\n",
      "40200/40200 [==============================] - 5s 128us/sample - loss: 0.0781 - acc: 0.9740 - val_loss: 0.0522 - val_acc: 0.9838\n",
      "Epoch 12/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9764\n",
      "Epoch 00012: val_loss did not improve from 0.05225\n",
      "40200/40200 [==============================] - 5s 135us/sample - loss: 0.0732 - acc: 0.9764 - val_loss: 0.0525 - val_acc: 0.9836\n",
      "Epoch 13/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9786\n",
      "Epoch 00013: val_loss improved from 0.05225 to 0.04977, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/013-0.0498.hdf5\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0663 - acc: 0.9787 - val_loss: 0.0498 - val_acc: 0.9846\n",
      "Epoch 14/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9814\n",
      "Epoch 00014: val_loss improved from 0.04977 to 0.04714, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/014-0.0471.hdf5\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0591 - acc: 0.9814 - val_loss: 0.0471 - val_acc: 0.9853\n",
      "Epoch 15/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9824\n",
      "Epoch 00015: val_loss improved from 0.04714 to 0.04563, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/015-0.0456.hdf5\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0531 - acc: 0.9825 - val_loss: 0.0456 - val_acc: 0.9857\n",
      "Epoch 16/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9824\n",
      "Epoch 00016: val_loss improved from 0.04563 to 0.04472, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/016-0.0447.hdf5\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0533 - acc: 0.9824 - val_loss: 0.0447 - val_acc: 0.9858\n",
      "Epoch 17/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9839\n",
      "Epoch 00017: val_loss improved from 0.04472 to 0.04454, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/017-0.0445.hdf5\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0501 - acc: 0.9839 - val_loss: 0.0445 - val_acc: 0.9857\n",
      "Epoch 18/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9855\n",
      "Epoch 00018: val_loss did not improve from 0.04454\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0437 - acc: 0.9855 - val_loss: 0.0453 - val_acc: 0.9863\n",
      "Epoch 19/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9860\n",
      "Epoch 00019: val_loss improved from 0.04454 to 0.04169, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/019-0.0417.hdf5\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0435 - acc: 0.9860 - val_loss: 0.0417 - val_acc: 0.9868\n",
      "Epoch 20/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9867\n",
      "Epoch 00020: val_loss did not improve from 0.04169\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0415 - acc: 0.9867 - val_loss: 0.0432 - val_acc: 0.9863\n",
      "Epoch 21/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9868\n",
      "Epoch 00021: val_loss did not improve from 0.04169\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0401 - acc: 0.9867 - val_loss: 0.0463 - val_acc: 0.9862\n",
      "Epoch 22/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9885\n",
      "Epoch 00022: val_loss improved from 0.04169 to 0.04027, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/022-0.0403.hdf5\n",
      "40200/40200 [==============================] - 5s 133us/sample - loss: 0.0368 - acc: 0.9885 - val_loss: 0.0403 - val_acc: 0.9868\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9881\n",
      "Epoch 00023: val_loss did not improve from 0.04027\n",
      "40200/40200 [==============================] - 5s 134us/sample - loss: 0.0358 - acc: 0.9881 - val_loss: 0.0430 - val_acc: 0.9868\n",
      "Epoch 24/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9889\n",
      "Epoch 00024: val_loss improved from 0.04027 to 0.04011, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/024-0.0401.hdf5\n",
      "40200/40200 [==============================] - 6s 137us/sample - loss: 0.0327 - acc: 0.9889 - val_loss: 0.0401 - val_acc: 0.9879\n",
      "Epoch 25/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9892\n",
      "Epoch 00025: val_loss improved from 0.04011 to 0.03724, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/025-0.0372.hdf5\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0323 - acc: 0.9892 - val_loss: 0.0372 - val_acc: 0.9884\n",
      "Epoch 26/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9895\n",
      "Epoch 00026: val_loss did not improve from 0.03724\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0300 - acc: 0.9895 - val_loss: 0.0375 - val_acc: 0.9886\n",
      "Epoch 27/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9911\n",
      "Epoch 00027: val_loss did not improve from 0.03724\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0268 - acc: 0.9911 - val_loss: 0.0374 - val_acc: 0.9886\n",
      "Epoch 28/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9905\n",
      "Epoch 00028: val_loss did not improve from 0.03724\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0275 - acc: 0.9905 - val_loss: 0.0401 - val_acc: 0.9878\n",
      "Epoch 29/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9908\n",
      "Epoch 00029: val_loss did not improve from 0.03724\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0261 - acc: 0.9909 - val_loss: 0.0396 - val_acc: 0.9884\n",
      "Epoch 30/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9913\n",
      "Epoch 00030: val_loss improved from 0.03724 to 0.03676, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/030-0.0368.hdf5\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0251 - acc: 0.9913 - val_loss: 0.0368 - val_acc: 0.9890\n",
      "Epoch 31/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9920\n",
      "Epoch 00031: val_loss did not improve from 0.03676\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0235 - acc: 0.9921 - val_loss: 0.0385 - val_acc: 0.9887\n",
      "Epoch 32/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9920\n",
      "Epoch 00032: val_loss did not improve from 0.03676\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0223 - acc: 0.9920 - val_loss: 0.0377 - val_acc: 0.9887\n",
      "Epoch 33/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9924\n",
      "Epoch 00033: val_loss did not improve from 0.03676\n",
      "40200/40200 [==============================] - 5s 131us/sample - loss: 0.0234 - acc: 0.9924 - val_loss: 0.0377 - val_acc: 0.9883\n",
      "Epoch 34/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9917\n",
      "Epoch 00034: val_loss did not improve from 0.03676\n",
      "40200/40200 [==============================] - 5s 133us/sample - loss: 0.0225 - acc: 0.9918 - val_loss: 0.0392 - val_acc: 0.9884\n",
      "Epoch 35/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9934\n",
      "Epoch 00035: val_loss improved from 0.03676 to 0.03658, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/035-0.0366.hdf5\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0195 - acc: 0.9934 - val_loss: 0.0366 - val_acc: 0.9900\n",
      "Epoch 36/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9931\n",
      "Epoch 00036: val_loss did not improve from 0.03658\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0199 - acc: 0.9931 - val_loss: 0.0373 - val_acc: 0.9887\n",
      "Epoch 37/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9934\n",
      "Epoch 00037: val_loss did not improve from 0.03658\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0191 - acc: 0.9934 - val_loss: 0.0401 - val_acc: 0.9886\n",
      "Epoch 38/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9934\n",
      "Epoch 00038: val_loss did not improve from 0.03658\n",
      "40200/40200 [==============================] - 6s 137us/sample - loss: 0.0197 - acc: 0.9934 - val_loss: 0.0368 - val_acc: 0.9888\n",
      "Epoch 39/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9930\n",
      "Epoch 00039: val_loss improved from 0.03658 to 0.03627, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/039-0.0363.hdf5\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0198 - acc: 0.9930 - val_loss: 0.0363 - val_acc: 0.9893\n",
      "Epoch 40/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9936\n",
      "Epoch 00040: val_loss did not improve from 0.03627\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0186 - acc: 0.9936 - val_loss: 0.0440 - val_acc: 0.9877\n",
      "Epoch 41/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9945\n",
      "Epoch 00041: val_loss did not improve from 0.03627\n",
      "40200/40200 [==============================] - 6s 137us/sample - loss: 0.0169 - acc: 0.9945 - val_loss: 0.0368 - val_acc: 0.9894\n",
      "Epoch 42/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9942\n",
      "Epoch 00042: val_loss did not improve from 0.03627\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0164 - acc: 0.9943 - val_loss: 0.0391 - val_acc: 0.9902\n",
      "Epoch 43/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9939\n",
      "Epoch 00043: val_loss improved from 0.03627 to 0.03580, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/043-0.0358.hdf5\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0178 - acc: 0.9940 - val_loss: 0.0358 - val_acc: 0.9897\n",
      "Epoch 44/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9942\n",
      "Epoch 00044: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 5s 127us/sample - loss: 0.0165 - acc: 0.9943 - val_loss: 0.0371 - val_acc: 0.9894\n",
      "Epoch 45/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9944\n",
      "Epoch 00045: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 5s 130us/sample - loss: 0.0171 - acc: 0.9944 - val_loss: 0.0378 - val_acc: 0.9895\n",
      "Epoch 46/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9943\n",
      "Epoch 00046: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0167 - acc: 0.9943 - val_loss: 0.0376 - val_acc: 0.9894\n",
      "Epoch 47/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9950\n",
      "Epoch 00047: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0143 - acc: 0.9950 - val_loss: 0.0379 - val_acc: 0.9903\n",
      "Epoch 48/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9956\n",
      "Epoch 00048: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0131 - acc: 0.9956 - val_loss: 0.0390 - val_acc: 0.9894\n",
      "Epoch 49/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9950\n",
      "Epoch 00049: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0149 - acc: 0.9950 - val_loss: 0.0373 - val_acc: 0.9903\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9952\n",
      "Epoch 00050: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 137us/sample - loss: 0.0147 - acc: 0.9952 - val_loss: 0.0391 - val_acc: 0.9893\n",
      "Epoch 51/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9952\n",
      "Epoch 00051: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0143 - acc: 0.9952 - val_loss: 0.0376 - val_acc: 0.9897\n",
      "Epoch 52/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9955\n",
      "Epoch 00052: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0127 - acc: 0.9955 - val_loss: 0.0382 - val_acc: 0.9900\n",
      "Epoch 53/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9957\n",
      "Epoch 00053: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 5s 137us/sample - loss: 0.0125 - acc: 0.9957 - val_loss: 0.0395 - val_acc: 0.9898\n",
      "Epoch 54/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9959\n",
      "Epoch 00054: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0126 - acc: 0.9958 - val_loss: 0.0409 - val_acc: 0.9896\n",
      "Epoch 55/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9959\n",
      "Epoch 00055: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 5s 133us/sample - loss: 0.0118 - acc: 0.9959 - val_loss: 0.0432 - val_acc: 0.9888\n",
      "Epoch 56/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9952\n",
      "Epoch 00056: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 5s 129us/sample - loss: 0.0133 - acc: 0.9952 - val_loss: 0.0390 - val_acc: 0.9898\n",
      "Epoch 57/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9963\n",
      "Epoch 00057: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 137us/sample - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0390 - val_acc: 0.9908\n",
      "Epoch 58/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9957\n",
      "Epoch 00058: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0118 - acc: 0.9957 - val_loss: 0.0441 - val_acc: 0.9892\n",
      "Epoch 59/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9961\n",
      "Epoch 00059: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0112 - acc: 0.9961 - val_loss: 0.0382 - val_acc: 0.9894\n",
      "Epoch 60/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9956\n",
      "Epoch 00060: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0129 - acc: 0.9956 - val_loss: 0.0366 - val_acc: 0.9907\n",
      "Epoch 61/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9962\n",
      "Epoch 00061: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 137us/sample - loss: 0.0116 - acc: 0.9962 - val_loss: 0.0384 - val_acc: 0.9905\n",
      "Epoch 62/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9962\n",
      "Epoch 00062: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0114 - acc: 0.9962 - val_loss: 0.0374 - val_acc: 0.9907\n",
      "Epoch 63/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9958\n",
      "Epoch 00063: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.0121 - acc: 0.9958 - val_loss: 0.0409 - val_acc: 0.9899\n",
      "Epoch 64/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9962\n",
      "Epoch 00064: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 6s 137us/sample - loss: 0.0110 - acc: 0.9962 - val_loss: 0.0385 - val_acc: 0.9902\n",
      "Epoch 65/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9965\n",
      "Epoch 00065: val_loss did not improve from 0.03580\n",
      "40200/40200 [==============================] - 5s 133us/sample - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0372 - val_acc: 0.9909\n",
      "Epoch 66/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9963\n",
      "Epoch 00066: val_loss improved from 0.03580 to 0.03553, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/066-0.0355.hdf5\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.0101 - acc: 0.9963 - val_loss: 0.0355 - val_acc: 0.9905\n",
      "Epoch 67/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9962\n",
      "Epoch 00067: val_loss improved from 0.03553 to 0.03536, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv_checkpoint/067-0.0354.hdf5\n",
      "40200/40200 [==============================] - 5s 132us/sample - loss: 0.0110 - acc: 0.9962 - val_loss: 0.0354 - val_acc: 0.9911\n",
      "Epoch 68/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9962\n",
      "Epoch 00068: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0112 - acc: 0.9961 - val_loss: 0.0377 - val_acc: 0.9905\n",
      "Epoch 69/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9967\n",
      "Epoch 00069: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0096 - acc: 0.9967 - val_loss: 0.0401 - val_acc: 0.9899\n",
      "Epoch 70/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9965\n",
      "Epoch 00070: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0102 - acc: 0.9965 - val_loss: 0.0361 - val_acc: 0.9907\n",
      "Epoch 71/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9958\n",
      "Epoch 00071: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 130us/sample - loss: 0.0124 - acc: 0.9958 - val_loss: 0.0367 - val_acc: 0.9906\n",
      "Epoch 72/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9972\n",
      "Epoch 00072: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.0397 - val_acc: 0.9906\n",
      "Epoch 73/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9966\n",
      "Epoch 00073: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0099 - acc: 0.9966 - val_loss: 0.0358 - val_acc: 0.9907\n",
      "Epoch 74/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9974\n",
      "Epoch 00074: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0087 - acc: 0.9974 - val_loss: 0.0384 - val_acc: 0.9904\n",
      "Epoch 75/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9974\n",
      "Epoch 00075: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0415 - val_acc: 0.9906\n",
      "Epoch 76/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9969\n",
      "Epoch 00076: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0098 - acc: 0.9969 - val_loss: 0.0388 - val_acc: 0.9908\n",
      "Epoch 77/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9968\n",
      "Epoch 00077: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 129us/sample - loss: 0.0089 - acc: 0.9968 - val_loss: 0.0374 - val_acc: 0.9908\n",
      "Epoch 78/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9967\n",
      "Epoch 00078: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 129us/sample - loss: 0.0100 - acc: 0.9967 - val_loss: 0.0362 - val_acc: 0.9913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9970\n",
      "Epoch 00079: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0381 - val_acc: 0.9912\n",
      "Epoch 80/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9975\n",
      "Epoch 00080: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0078 - acc: 0.9975 - val_loss: 0.0373 - val_acc: 0.9914\n",
      "Epoch 81/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9974\n",
      "Epoch 00081: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0385 - val_acc: 0.9907\n",
      "Epoch 82/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9964\n",
      "Epoch 00082: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0107 - acc: 0.9964 - val_loss: 0.0378 - val_acc: 0.9907\n",
      "Epoch 83/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9969\n",
      "Epoch 00083: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0085 - acc: 0.9969 - val_loss: 0.0368 - val_acc: 0.9908\n",
      "Epoch 84/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9974\n",
      "Epoch 00084: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 127us/sample - loss: 0.0079 - acc: 0.9974 - val_loss: 0.0384 - val_acc: 0.9906\n",
      "Epoch 85/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9973\n",
      "Epoch 00085: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 127us/sample - loss: 0.0083 - acc: 0.9973 - val_loss: 0.0400 - val_acc: 0.9907\n",
      "Epoch 86/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9969\n",
      "Epoch 00086: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 133us/sample - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0396 - val_acc: 0.9906\n",
      "Epoch 87/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9974\n",
      "Epoch 00087: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 146us/sample - loss: 0.0074 - acc: 0.9974 - val_loss: 0.0399 - val_acc: 0.9907\n",
      "Epoch 88/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9974\n",
      "Epoch 00088: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 131us/sample - loss: 0.0080 - acc: 0.9974 - val_loss: 0.0386 - val_acc: 0.9910\n",
      "Epoch 89/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9972\n",
      "Epoch 00089: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 134us/sample - loss: 0.0079 - acc: 0.9972 - val_loss: 0.0425 - val_acc: 0.9904\n",
      "Epoch 90/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9972\n",
      "Epoch 00090: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0074 - acc: 0.9972 - val_loss: 0.0400 - val_acc: 0.9911\n",
      "Epoch 91/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9972\n",
      "Epoch 00091: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0085 - acc: 0.9971 - val_loss: 0.0390 - val_acc: 0.9914\n",
      "Epoch 92/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9969\n",
      "Epoch 00092: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 145us/sample - loss: 0.0090 - acc: 0.9969 - val_loss: 0.0398 - val_acc: 0.9906\n",
      "Epoch 93/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9976\n",
      "Epoch 00093: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0072 - acc: 0.9976 - val_loss: 0.0411 - val_acc: 0.9913\n",
      "Epoch 94/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9974\n",
      "Epoch 00094: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0439 - val_acc: 0.9904\n",
      "Epoch 95/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 00095: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0073 - acc: 0.9977 - val_loss: 0.0415 - val_acc: 0.9910\n",
      "Epoch 96/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 00096: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0406 - val_acc: 0.9906\n",
      "Epoch 97/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9978\n",
      "Epoch 00097: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0416 - val_acc: 0.9904\n",
      "Epoch 98/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9975\n",
      "Epoch 00098: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 134us/sample - loss: 0.0083 - acc: 0.9975 - val_loss: 0.0409 - val_acc: 0.9906\n",
      "Epoch 99/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9977\n",
      "Epoch 00099: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0066 - acc: 0.9977 - val_loss: 0.0401 - val_acc: 0.9916\n",
      "Epoch 100/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9972\n",
      "Epoch 00100: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0078 - acc: 0.9972 - val_loss: 0.0432 - val_acc: 0.9906\n",
      "Epoch 101/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9972\n",
      "Epoch 00101: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0081 - acc: 0.9972 - val_loss: 0.0367 - val_acc: 0.9914\n",
      "Epoch 102/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9976\n",
      "Epoch 00102: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0382 - val_acc: 0.9908\n",
      "Epoch 103/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9969\n",
      "Epoch 00103: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0090 - acc: 0.9969 - val_loss: 0.0446 - val_acc: 0.9899\n",
      "Epoch 104/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9977\n",
      "Epoch 00104: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0065 - acc: 0.9977 - val_loss: 0.0403 - val_acc: 0.9912\n",
      "Epoch 105/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 00105: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 132us/sample - loss: 0.0071 - acc: 0.9977 - val_loss: 0.0444 - val_acc: 0.9901\n",
      "Epoch 106/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9974\n",
      "Epoch 00106: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 145us/sample - loss: 0.0076 - acc: 0.9974 - val_loss: 0.0402 - val_acc: 0.9910\n",
      "Epoch 107/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9975\n",
      "Epoch 00107: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0068 - acc: 0.9975 - val_loss: 0.0383 - val_acc: 0.9911\n",
      "Epoch 108/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 00108: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0067 - acc: 0.9977 - val_loss: 0.0407 - val_acc: 0.9910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9984\n",
      "Epoch 00109: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 145us/sample - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0404 - val_acc: 0.9913\n",
      "Epoch 110/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9976\n",
      "Epoch 00110: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0408 - val_acc: 0.9905\n",
      "Epoch 111/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9979\n",
      "Epoch 00111: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 127us/sample - loss: 0.0062 - acc: 0.9979 - val_loss: 0.0410 - val_acc: 0.9912\n",
      "Epoch 112/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9978\n",
      "Epoch 00112: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0064 - acc: 0.9978 - val_loss: 0.0424 - val_acc: 0.9915\n",
      "Epoch 113/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9979\n",
      "Epoch 00113: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 137us/sample - loss: 0.0066 - acc: 0.9979 - val_loss: 0.0410 - val_acc: 0.9910\n",
      "Epoch 114/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9981\n",
      "Epoch 00114: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0404 - val_acc: 0.9909\n",
      "Epoch 115/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9976\n",
      "Epoch 00115: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0396 - val_acc: 0.9915\n",
      "Epoch 116/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9976\n",
      "Epoch 00116: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0072 - acc: 0.9976 - val_loss: 0.0403 - val_acc: 0.9910\n",
      "Epoch 117/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9984\n",
      "Epoch 00117: val_loss did not improve from 0.03536\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0397 - val_acc: 0.9909\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 103us/sample - loss: 0.0267 - acc: 0.9921\n",
      "Loss: 0.026718628816661476 Accuracy: 0.9921\n",
      "\n",
      "Train on 40200 samples, validate on 19800 samples\n",
      "Epoch 1/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 3.2240 - acc: 0.5584\n",
      "Epoch 00001: val_loss improved from inf to 0.33497, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/001-0.3350.hdf5\n",
      "40200/40200 [==============================] - 8s 198us/sample - loss: 3.2106 - acc: 0.5596 - val_loss: 0.3350 - val_acc: 0.9013\n",
      "Epoch 2/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.4776 - acc: 0.8455\n",
      "Epoch 00002: val_loss improved from 0.33497 to 0.17560, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/002-0.1756.hdf5\n",
      "40200/40200 [==============================] - 7s 163us/sample - loss: 0.4774 - acc: 0.8455 - val_loss: 0.1756 - val_acc: 0.9468\n",
      "Epoch 3/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9031\n",
      "Epoch 00003: val_loss improved from 0.17560 to 0.12671, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/003-0.1267.hdf5\n",
      "40200/40200 [==============================] - 6s 145us/sample - loss: 0.3033 - acc: 0.9033 - val_loss: 0.1267 - val_acc: 0.9609\n",
      "Epoch 4/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9317\n",
      "Epoch 00004: val_loss improved from 0.12671 to 0.10252, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/004-0.1025.hdf5\n",
      "40200/40200 [==============================] - 6s 160us/sample - loss: 0.2195 - acc: 0.9317 - val_loss: 0.1025 - val_acc: 0.9682\n",
      "Epoch 5/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9465\n",
      "Epoch 00005: val_loss improved from 0.10252 to 0.08229, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/005-0.0823.hdf5\n",
      "40200/40200 [==============================] - 7s 162us/sample - loss: 0.1738 - acc: 0.9466 - val_loss: 0.0823 - val_acc: 0.9739\n",
      "Epoch 6/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9541\n",
      "Epoch 00006: val_loss improved from 0.08229 to 0.07219, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/006-0.0722.hdf5\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.1518 - acc: 0.9542 - val_loss: 0.0722 - val_acc: 0.9770\n",
      "Epoch 7/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9596\n",
      "Epoch 00007: val_loss improved from 0.07219 to 0.06835, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/007-0.0683.hdf5\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.1283 - acc: 0.9596 - val_loss: 0.0683 - val_acc: 0.9784\n",
      "Epoch 8/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9653\n",
      "Epoch 00008: val_loss improved from 0.06835 to 0.06332, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/008-0.0633.hdf5\n",
      "40200/40200 [==============================] - 6s 158us/sample - loss: 0.1108 - acc: 0.9652 - val_loss: 0.0633 - val_acc: 0.9792\n",
      "Epoch 9/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9686\n",
      "Epoch 00009: val_loss improved from 0.06332 to 0.05456, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/009-0.0546.hdf5\n",
      "40200/40200 [==============================] - 7s 163us/sample - loss: 0.0998 - acc: 0.9687 - val_loss: 0.0546 - val_acc: 0.9828\n",
      "Epoch 10/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9729\n",
      "Epoch 00010: val_loss improved from 0.05456 to 0.05063, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/010-0.0506.hdf5\n",
      "40200/40200 [==============================] - 6s 160us/sample - loss: 0.0859 - acc: 0.9728 - val_loss: 0.0506 - val_acc: 0.9838\n",
      "Epoch 11/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9754\n",
      "Epoch 00011: val_loss did not improve from 0.05063\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.0791 - acc: 0.9754 - val_loss: 0.0531 - val_acc: 0.9833\n",
      "Epoch 12/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9768\n",
      "Epoch 00012: val_loss did not improve from 0.05063\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0728 - acc: 0.9768 - val_loss: 0.0524 - val_acc: 0.9844\n",
      "Epoch 13/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9787\n",
      "Epoch 00013: val_loss improved from 0.05063 to 0.04891, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/013-0.0489.hdf5\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0678 - acc: 0.9787 - val_loss: 0.0489 - val_acc: 0.9842\n",
      "Epoch 14/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9800\n",
      "Epoch 00014: val_loss improved from 0.04891 to 0.04788, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/014-0.0479.hdf5\n",
      "40200/40200 [==============================] - 6s 162us/sample - loss: 0.0626 - acc: 0.9801 - val_loss: 0.0479 - val_acc: 0.9849\n",
      "Epoch 15/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9829\n",
      "Epoch 00015: val_loss did not improve from 0.04788\n",
      "40200/40200 [==============================] - 6s 159us/sample - loss: 0.0558 - acc: 0.9829 - val_loss: 0.0482 - val_acc: 0.9858\n",
      "Epoch 16/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9841\n",
      "Epoch 00016: val_loss improved from 0.04788 to 0.04539, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/016-0.0454.hdf5\n",
      "40200/40200 [==============================] - 7s 169us/sample - loss: 0.0511 - acc: 0.9840 - val_loss: 0.0454 - val_acc: 0.9861\n",
      "Epoch 17/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9851\n",
      "Epoch 00017: val_loss improved from 0.04539 to 0.03988, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/017-0.0399.hdf5\n",
      "40200/40200 [==============================] - 6s 161us/sample - loss: 0.0473 - acc: 0.9851 - val_loss: 0.0399 - val_acc: 0.9876\n",
      "Epoch 18/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9860\n",
      "Epoch 00018: val_loss did not improve from 0.03988\n",
      "40200/40200 [==============================] - 6s 158us/sample - loss: 0.0440 - acc: 0.9860 - val_loss: 0.0433 - val_acc: 0.9869\n",
      "Epoch 19/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9856\n",
      "Epoch 00019: val_loss did not improve from 0.03988\n",
      "40200/40200 [==============================] - 6s 162us/sample - loss: 0.0443 - acc: 0.9856 - val_loss: 0.0450 - val_acc: 0.9871\n",
      "Epoch 20/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9869\n",
      "Epoch 00020: val_loss improved from 0.03988 to 0.03949, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/020-0.0395.hdf5\n",
      "40200/40200 [==============================] - 7s 163us/sample - loss: 0.0413 - acc: 0.9869 - val_loss: 0.0395 - val_acc: 0.9890\n",
      "Epoch 21/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9878\n",
      "Epoch 00021: val_loss did not improve from 0.03949\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.0366 - acc: 0.9878 - val_loss: 0.0430 - val_acc: 0.9878\n",
      "Epoch 22/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9890\n",
      "Epoch 00022: val_loss improved from 0.03949 to 0.03906, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/022-0.0391.hdf5\n",
      "40200/40200 [==============================] - 6s 148us/sample - loss: 0.0357 - acc: 0.9890 - val_loss: 0.0391 - val_acc: 0.9886\n",
      "Epoch 23/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9888\n",
      "Epoch 00023: val_loss did not improve from 0.03906\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0327 - acc: 0.9888 - val_loss: 0.0394 - val_acc: 0.9883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9907\n",
      "Epoch 00024: val_loss improved from 0.03906 to 0.03813, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/024-0.0381.hdf5\n",
      "40200/40200 [==============================] - 7s 163us/sample - loss: 0.0301 - acc: 0.9907 - val_loss: 0.0381 - val_acc: 0.9888\n",
      "Epoch 25/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9899\n",
      "Epoch 00025: val_loss improved from 0.03813 to 0.03729, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/025-0.0373.hdf5\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.0285 - acc: 0.9899 - val_loss: 0.0373 - val_acc: 0.9886\n",
      "Epoch 26/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9910\n",
      "Epoch 00026: val_loss did not improve from 0.03729\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.0268 - acc: 0.9909 - val_loss: 0.0389 - val_acc: 0.9884\n",
      "Epoch 27/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9914\n",
      "Epoch 00027: val_loss did not improve from 0.03729\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.0280 - acc: 0.9913 - val_loss: 0.0393 - val_acc: 0.9895\n",
      "Epoch 28/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9923\n",
      "Epoch 00028: val_loss did not improve from 0.03729\n",
      "40200/40200 [==============================] - 6s 158us/sample - loss: 0.0237 - acc: 0.9924 - val_loss: 0.0437 - val_acc: 0.9879\n",
      "Epoch 29/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9915\n",
      "Epoch 00029: val_loss did not improve from 0.03729\n",
      "40200/40200 [==============================] - 7s 162us/sample - loss: 0.0248 - acc: 0.9915 - val_loss: 0.0386 - val_acc: 0.9889\n",
      "Epoch 30/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9918\n",
      "Epoch 00030: val_loss did not improve from 0.03729\n",
      "40200/40200 [==============================] - 7s 162us/sample - loss: 0.0240 - acc: 0.9918 - val_loss: 0.0387 - val_acc: 0.9889\n",
      "Epoch 31/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9919\n",
      "Epoch 00031: val_loss improved from 0.03729 to 0.03569, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv_checkpoint/031-0.0357.hdf5\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0234 - acc: 0.9919 - val_loss: 0.0357 - val_acc: 0.9897\n",
      "Epoch 32/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9935\n",
      "Epoch 00032: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 153us/sample - loss: 0.0197 - acc: 0.9936 - val_loss: 0.0368 - val_acc: 0.9903\n",
      "Epoch 33/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9937\n",
      "Epoch 00033: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 7s 163us/sample - loss: 0.0208 - acc: 0.9936 - val_loss: 0.0369 - val_acc: 0.9894\n",
      "Epoch 34/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9940\n",
      "Epoch 00034: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 160us/sample - loss: 0.0183 - acc: 0.9940 - val_loss: 0.0406 - val_acc: 0.9887\n",
      "Epoch 35/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9941\n",
      "Epoch 00035: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 158us/sample - loss: 0.0188 - acc: 0.9940 - val_loss: 0.0360 - val_acc: 0.9906\n",
      "Epoch 36/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9933\n",
      "Epoch 00036: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 157us/sample - loss: 0.0196 - acc: 0.9933 - val_loss: 0.0379 - val_acc: 0.9903\n",
      "Epoch 37/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9939\n",
      "Epoch 00037: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 7s 162us/sample - loss: 0.0183 - acc: 0.9939 - val_loss: 0.0363 - val_acc: 0.9908\n",
      "Epoch 38/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9945\n",
      "Epoch 00038: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 7s 164us/sample - loss: 0.0165 - acc: 0.9945 - val_loss: 0.0378 - val_acc: 0.9898\n",
      "Epoch 39/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9945\n",
      "Epoch 00039: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 158us/sample - loss: 0.0163 - acc: 0.9945 - val_loss: 0.0359 - val_acc: 0.9909\n",
      "Epoch 40/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9944\n",
      "Epoch 00040: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 157us/sample - loss: 0.0163 - acc: 0.9944 - val_loss: 0.0396 - val_acc: 0.9905\n",
      "Epoch 41/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9952\n",
      "Epoch 00041: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0147 - acc: 0.9952 - val_loss: 0.0382 - val_acc: 0.9906\n",
      "Epoch 42/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9956\n",
      "Epoch 00042: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 7s 164us/sample - loss: 0.0132 - acc: 0.9956 - val_loss: 0.0382 - val_acc: 0.9908\n",
      "Epoch 43/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9956\n",
      "Epoch 00043: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0137 - acc: 0.9956 - val_loss: 0.0367 - val_acc: 0.9906\n",
      "Epoch 44/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9947\n",
      "Epoch 00044: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 150us/sample - loss: 0.0151 - acc: 0.9947 - val_loss: 0.0408 - val_acc: 0.9899\n",
      "Epoch 45/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9956\n",
      "Epoch 00045: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 161us/sample - loss: 0.0127 - acc: 0.9956 - val_loss: 0.0387 - val_acc: 0.9909\n",
      "Epoch 46/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9961\n",
      "Epoch 00046: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0119 - acc: 0.9961 - val_loss: 0.0387 - val_acc: 0.9910\n",
      "Epoch 47/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9956\n",
      "Epoch 00047: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0121 - acc: 0.9956 - val_loss: 0.0439 - val_acc: 0.9900\n",
      "Epoch 48/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9958\n",
      "Epoch 00048: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.0140 - acc: 0.9958 - val_loss: 0.0409 - val_acc: 0.9905\n",
      "Epoch 49/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9963\n",
      "Epoch 00049: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0110 - acc: 0.9963 - val_loss: 0.0377 - val_acc: 0.9912\n",
      "Epoch 50/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9956\n",
      "Epoch 00050: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 137us/sample - loss: 0.0133 - acc: 0.9955 - val_loss: 0.0401 - val_acc: 0.9905\n",
      "Epoch 51/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9961\n",
      "Epoch 00051: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 5s 128us/sample - loss: 0.0107 - acc: 0.9961 - val_loss: 0.0383 - val_acc: 0.9906\n",
      "Epoch 52/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9962\n",
      "Epoch 00052: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 148us/sample - loss: 0.0106 - acc: 0.9963 - val_loss: 0.0388 - val_acc: 0.9906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9967\n",
      "Epoch 00053: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0105 - acc: 0.9965 - val_loss: 0.0395 - val_acc: 0.9906\n",
      "Epoch 54/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9961\n",
      "Epoch 00054: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.0117 - acc: 0.9962 - val_loss: 0.0390 - val_acc: 0.9903\n",
      "Epoch 55/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9970\n",
      "Epoch 00055: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0389 - val_acc: 0.9911\n",
      "Epoch 56/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9967\n",
      "Epoch 00056: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0093 - acc: 0.9967 - val_loss: 0.0427 - val_acc: 0.9906\n",
      "Epoch 57/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9967\n",
      "Epoch 00057: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 161us/sample - loss: 0.0093 - acc: 0.9967 - val_loss: 0.0432 - val_acc: 0.9905\n",
      "Epoch 58/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9967\n",
      "Epoch 00058: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 161us/sample - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0419 - val_acc: 0.9903\n",
      "Epoch 59/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9968\n",
      "Epoch 00059: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 7s 163us/sample - loss: 0.0094 - acc: 0.9969 - val_loss: 0.0416 - val_acc: 0.9907\n",
      "Epoch 60/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9968\n",
      "Epoch 00060: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 159us/sample - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0387 - val_acc: 0.9911\n",
      "Epoch 61/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9969\n",
      "Epoch 00061: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 146us/sample - loss: 0.0094 - acc: 0.9969 - val_loss: 0.0407 - val_acc: 0.9913\n",
      "Epoch 62/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9967\n",
      "Epoch 00062: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 161us/sample - loss: 0.0095 - acc: 0.9967 - val_loss: 0.0429 - val_acc: 0.9903\n",
      "Epoch 63/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9978\n",
      "Epoch 00063: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 7s 162us/sample - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0428 - val_acc: 0.9919\n",
      "Epoch 64/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9967\n",
      "Epoch 00064: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.0094 - acc: 0.9967 - val_loss: 0.0450 - val_acc: 0.9903\n",
      "Epoch 65/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9970\n",
      "Epoch 00065: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 160us/sample - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0414 - val_acc: 0.9910\n",
      "Epoch 66/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9968\n",
      "Epoch 00066: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 161us/sample - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0424 - val_acc: 0.9904\n",
      "Epoch 67/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9972\n",
      "Epoch 00067: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 7s 163us/sample - loss: 0.0078 - acc: 0.9972 - val_loss: 0.0415 - val_acc: 0.9917\n",
      "Epoch 68/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9973\n",
      "Epoch 00068: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0077 - acc: 0.9973 - val_loss: 0.0425 - val_acc: 0.9915\n",
      "Epoch 69/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9976\n",
      "Epoch 00069: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0067 - acc: 0.9976 - val_loss: 0.0437 - val_acc: 0.9910\n",
      "Epoch 70/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9973\n",
      "Epoch 00070: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 5s 134us/sample - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0420 - val_acc: 0.9910\n",
      "Epoch 71/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9974\n",
      "Epoch 00071: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 5s 131us/sample - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0415 - val_acc: 0.9912\n",
      "Epoch 72/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9977\n",
      "Epoch 00072: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0398 - val_acc: 0.9914\n",
      "Epoch 73/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9976\n",
      "Epoch 00073: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 159us/sample - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0393 - val_acc: 0.9913\n",
      "Epoch 74/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9974\n",
      "Epoch 00074: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 161us/sample - loss: 0.0091 - acc: 0.9973 - val_loss: 0.0416 - val_acc: 0.9915\n",
      "Epoch 75/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9979\n",
      "Epoch 00075: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 7s 166us/sample - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0399 - val_acc: 0.9912\n",
      "Epoch 76/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9979\n",
      "Epoch 00076: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 7s 164us/sample - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0427 - val_acc: 0.9915\n",
      "Epoch 77/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9978\n",
      "Epoch 00077: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 159us/sample - loss: 0.0074 - acc: 0.9978 - val_loss: 0.0456 - val_acc: 0.9907\n",
      "Epoch 78/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9978\n",
      "Epoch 00078: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 7s 165us/sample - loss: 0.0068 - acc: 0.9978 - val_loss: 0.0397 - val_acc: 0.9909\n",
      "Epoch 79/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9981\n",
      "Epoch 00079: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 161us/sample - loss: 0.0062 - acc: 0.9981 - val_loss: 0.0488 - val_acc: 0.9904\n",
      "Epoch 80/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9975\n",
      "Epoch 00080: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 147us/sample - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0408 - val_acc: 0.9914\n",
      "Epoch 81/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9979\n",
      "Epoch 00081: val_loss did not improve from 0.03569\n",
      "40200/40200 [==============================] - 6s 159us/sample - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0403 - val_acc: 0.9910\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 128us/sample - loss: 0.0272 - acc: 0.9917\n",
      "Loss: 0.02717794110944369 Accuracy: 0.9917\n",
      "\n",
      "Train on 40200 samples, validate on 19800 samples\n",
      "Epoch 1/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 1.2112 - acc: 0.7194\n",
      "Epoch 00001: val_loss improved from inf to 0.17966, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/001-0.1797.hdf5\n",
      "40200/40200 [==============================] - 10s 253us/sample - loss: 1.2110 - acc: 0.7195 - val_loss: 0.1797 - val_acc: 0.9446\n",
      "Epoch 2/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9294\n",
      "Epoch 00002: val_loss improved from 0.17966 to 0.10921, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/002-0.1092.hdf5\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.2336 - acc: 0.9294 - val_loss: 0.1092 - val_acc: 0.9655\n",
      "Epoch 3/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9532\n",
      "Epoch 00003: val_loss improved from 0.10921 to 0.08552, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/003-0.0855.hdf5\n",
      "40200/40200 [==============================] - 7s 183us/sample - loss: 0.1554 - acc: 0.9531 - val_loss: 0.0855 - val_acc: 0.9729\n",
      "Epoch 4/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9650\n",
      "Epoch 00004: val_loss improved from 0.08552 to 0.06952, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/004-0.0695.hdf5\n",
      "40200/40200 [==============================] - 7s 182us/sample - loss: 0.1164 - acc: 0.9650 - val_loss: 0.0695 - val_acc: 0.9780\n",
      "Epoch 5/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9701\n",
      "Epoch 00005: val_loss did not improve from 0.06952\n",
      "40200/40200 [==============================] - 7s 183us/sample - loss: 0.0967 - acc: 0.9701 - val_loss: 0.0702 - val_acc: 0.9787\n",
      "Epoch 6/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9767\n",
      "Epoch 00006: val_loss improved from 0.06952 to 0.05862, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/006-0.0586.hdf5\n",
      "40200/40200 [==============================] - 7s 168us/sample - loss: 0.0786 - acc: 0.9767 - val_loss: 0.0586 - val_acc: 0.9810\n",
      "Epoch 7/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9782\n",
      "Epoch 00007: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0711 - acc: 0.9782 - val_loss: 0.0593 - val_acc: 0.9820\n",
      "Epoch 8/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9821\n",
      "Epoch 00008: val_loss improved from 0.05862 to 0.05340, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/008-0.0534.hdf5\n",
      "40200/40200 [==============================] - 7s 177us/sample - loss: 0.0590 - acc: 0.9821 - val_loss: 0.0534 - val_acc: 0.9832\n",
      "Epoch 9/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9840\n",
      "Epoch 00009: val_loss improved from 0.05340 to 0.04908, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/009-0.0491.hdf5\n",
      "40200/40200 [==============================] - 7s 180us/sample - loss: 0.0527 - acc: 0.9840 - val_loss: 0.0491 - val_acc: 0.9852\n",
      "Epoch 10/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9856\n",
      "Epoch 00010: val_loss improved from 0.04908 to 0.04902, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/010-0.0490.hdf5\n",
      "40200/40200 [==============================] - 7s 185us/sample - loss: 0.0444 - acc: 0.9856 - val_loss: 0.0490 - val_acc: 0.9854\n",
      "Epoch 11/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9876\n",
      "Epoch 00011: val_loss improved from 0.04902 to 0.04503, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/011-0.0450.hdf5\n",
      "40200/40200 [==============================] - 7s 184us/sample - loss: 0.0410 - acc: 0.9876 - val_loss: 0.0450 - val_acc: 0.9866\n",
      "Epoch 12/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9891\n",
      "Epoch 00012: val_loss improved from 0.04503 to 0.04468, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/012-0.0447.hdf5\n",
      "40200/40200 [==============================] - 7s 183us/sample - loss: 0.0341 - acc: 0.9891 - val_loss: 0.0447 - val_acc: 0.9873\n",
      "Epoch 13/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9899\n",
      "Epoch 00013: val_loss improved from 0.04468 to 0.04262, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/013-0.0426.hdf5\n",
      "40200/40200 [==============================] - 7s 178us/sample - loss: 0.0316 - acc: 0.9899 - val_loss: 0.0426 - val_acc: 0.9882\n",
      "Epoch 14/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9905\n",
      "Epoch 00014: val_loss did not improve from 0.04262\n",
      "40200/40200 [==============================] - 7s 172us/sample - loss: 0.0298 - acc: 0.9906 - val_loss: 0.0533 - val_acc: 0.9843\n",
      "Epoch 15/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9921\n",
      "Epoch 00015: val_loss did not improve from 0.04262\n",
      "40200/40200 [==============================] - 7s 168us/sample - loss: 0.0257 - acc: 0.9920 - val_loss: 0.0435 - val_acc: 0.9880\n",
      "Epoch 16/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9932\n",
      "Epoch 00016: val_loss did not improve from 0.04262\n",
      "40200/40200 [==============================] - 7s 178us/sample - loss: 0.0223 - acc: 0.9932 - val_loss: 0.0469 - val_acc: 0.9866\n",
      "Epoch 17/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9933\n",
      "Epoch 00017: val_loss did not improve from 0.04262\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0203 - acc: 0.9933 - val_loss: 0.0461 - val_acc: 0.9880\n",
      "Epoch 18/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9940\n",
      "Epoch 00018: val_loss did not improve from 0.04262\n",
      "40200/40200 [==============================] - 7s 177us/sample - loss: 0.0197 - acc: 0.9940 - val_loss: 0.0573 - val_acc: 0.9865\n",
      "Epoch 19/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9938\n",
      "Epoch 00019: val_loss did not improve from 0.04262\n",
      "40200/40200 [==============================] - 7s 178us/sample - loss: 0.0186 - acc: 0.9938 - val_loss: 0.0461 - val_acc: 0.9878\n",
      "Epoch 20/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9951\n",
      "Epoch 00020: val_loss did not improve from 0.04262\n",
      "40200/40200 [==============================] - 7s 176us/sample - loss: 0.0156 - acc: 0.9951 - val_loss: 0.0807 - val_acc: 0.9787\n",
      "Epoch 21/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9951\n",
      "Epoch 00021: val_loss improved from 0.04262 to 0.04144, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/021-0.0414.hdf5\n",
      "40200/40200 [==============================] - 7s 182us/sample - loss: 0.0161 - acc: 0.9951 - val_loss: 0.0414 - val_acc: 0.9896\n",
      "Epoch 22/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9961\n",
      "Epoch 00022: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 181us/sample - loss: 0.0123 - acc: 0.9961 - val_loss: 0.0529 - val_acc: 0.9865\n",
      "Epoch 23/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9956\n",
      "Epoch 00023: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 6s 152us/sample - loss: 0.0131 - acc: 0.9956 - val_loss: 0.0527 - val_acc: 0.9874\n",
      "Epoch 24/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9962\n",
      "Epoch 00024: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 6s 160us/sample - loss: 0.0123 - acc: 0.9963 - val_loss: 0.0586 - val_acc: 0.9876\n",
      "Epoch 25/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9968\n",
      "Epoch 00025: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 177us/sample - loss: 0.0096 - acc: 0.9968 - val_loss: 0.0466 - val_acc: 0.9883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9971\n",
      "Epoch 00026: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 182us/sample - loss: 0.0104 - acc: 0.9971 - val_loss: 0.0452 - val_acc: 0.9894\n",
      "Epoch 27/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 00027: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 183us/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0471 - val_acc: 0.9893\n",
      "Epoch 28/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9973\n",
      "Epoch 00028: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 178us/sample - loss: 0.0087 - acc: 0.9973 - val_loss: 0.0454 - val_acc: 0.9906\n",
      "Epoch 29/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9975\n",
      "Epoch 00029: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 183us/sample - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0510 - val_acc: 0.9885\n",
      "Epoch 30/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9975\n",
      "Epoch 00030: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0453 - val_acc: 0.9905\n",
      "Epoch 31/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9974\n",
      "Epoch 00031: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 178us/sample - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0440 - val_acc: 0.9901\n",
      "Epoch 32/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 00032: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 172us/sample - loss: 0.0076 - acc: 0.9977 - val_loss: 0.0477 - val_acc: 0.9893\n",
      "Epoch 33/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9977\n",
      "Epoch 00033: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0075 - acc: 0.9977 - val_loss: 0.0466 - val_acc: 0.9902\n",
      "Epoch 34/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9979\n",
      "Epoch 00034: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0433 - val_acc: 0.9912\n",
      "Epoch 35/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9978\n",
      "Epoch 00035: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 175us/sample - loss: 0.0063 - acc: 0.9978 - val_loss: 0.0560 - val_acc: 0.9884\n",
      "Epoch 36/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9975\n",
      "Epoch 00036: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0500 - val_acc: 0.9895\n",
      "Epoch 37/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9986\n",
      "Epoch 00037: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 181us/sample - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0588 - val_acc: 0.9880\n",
      "Epoch 38/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9972\n",
      "Epoch 00038: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 167us/sample - loss: 0.0082 - acc: 0.9972 - val_loss: 0.0507 - val_acc: 0.9895\n",
      "Epoch 39/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9985\n",
      "Epoch 00039: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 171us/sample - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0537 - val_acc: 0.9904\n",
      "Epoch 40/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9983\n",
      "Epoch 00040: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 166us/sample - loss: 0.0048 - acc: 0.9983 - val_loss: 0.0608 - val_acc: 0.9884\n",
      "Epoch 41/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9983\n",
      "Epoch 00041: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0056 - acc: 0.9983 - val_loss: 0.0499 - val_acc: 0.9901\n",
      "Epoch 42/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9978\n",
      "Epoch 00042: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 181us/sample - loss: 0.0068 - acc: 0.9978 - val_loss: 0.0615 - val_acc: 0.9879\n",
      "Epoch 43/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9983\n",
      "Epoch 00043: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 178us/sample - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0448 - val_acc: 0.9903\n",
      "Epoch 44/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 00044: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0477 - val_acc: 0.9915\n",
      "Epoch 45/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9986\n",
      "Epoch 00045: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 180us/sample - loss: 0.0039 - acc: 0.9986 - val_loss: 0.0545 - val_acc: 0.9887\n",
      "Epoch 46/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9984\n",
      "Epoch 00046: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0047 - acc: 0.9984 - val_loss: 0.0522 - val_acc: 0.9897\n",
      "Epoch 47/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9989\n",
      "Epoch 00047: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 177us/sample - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0521 - val_acc: 0.9909\n",
      "Epoch 48/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9991\n",
      "Epoch 00048: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 173us/sample - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0588 - val_acc: 0.9894\n",
      "Epoch 49/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9979\n",
      "Epoch 00049: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0055 - acc: 0.9980 - val_loss: 0.0488 - val_acc: 0.9911\n",
      "Epoch 50/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9989\n",
      "Epoch 00050: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 180us/sample - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0502 - val_acc: 0.9912\n",
      "Epoch 51/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9986\n",
      "Epoch 00051: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 182us/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0570 - val_acc: 0.9889\n",
      "Epoch 52/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9991\n",
      "Epoch 00052: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 180us/sample - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0718 - val_acc: 0.9877\n",
      "Epoch 53/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9981\n",
      "Epoch 00053: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0581 - val_acc: 0.9900\n",
      "Epoch 54/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9990\n",
      "Epoch 00054: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 181us/sample - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0550 - val_acc: 0.9898\n",
      "Epoch 55/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9987\n",
      "Epoch 00055: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 170us/sample - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0581 - val_acc: 0.9896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 00056: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 177us/sample - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0536 - val_acc: 0.9909\n",
      "Epoch 57/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9986\n",
      "Epoch 00057: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 167us/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0611 - val_acc: 0.9892\n",
      "Epoch 58/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9988\n",
      "Epoch 00058: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0572 - val_acc: 0.9898\n",
      "Epoch 59/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9984\n",
      "Epoch 00059: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 186us/sample - loss: 0.0041 - acc: 0.9984 - val_loss: 0.0530 - val_acc: 0.9910\n",
      "Epoch 60/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 00060: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 181us/sample - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0534 - val_acc: 0.9910\n",
      "Epoch 61/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9987\n",
      "Epoch 00061: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 181us/sample - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0519 - val_acc: 0.9910\n",
      "Epoch 62/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00062: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 180us/sample - loss: 0.0040 - acc: 0.9991 - val_loss: 0.0561 - val_acc: 0.9894\n",
      "Epoch 63/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 00063: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 182us/sample - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0497 - val_acc: 0.9915\n",
      "Epoch 64/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9992\n",
      "Epoch 00064: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 181us/sample - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0634 - val_acc: 0.9893\n",
      "Epoch 65/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9992\n",
      "Epoch 00065: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 169us/sample - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0608 - val_acc: 0.9897\n",
      "Epoch 66/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9985\n",
      "Epoch 00066: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 179us/sample - loss: 0.0055 - acc: 0.9985 - val_loss: 0.0563 - val_acc: 0.9900\n",
      "Epoch 67/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 00067: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 177us/sample - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0668 - val_acc: 0.9865\n",
      "Epoch 68/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9987\n",
      "Epoch 00068: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 177us/sample - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0564 - val_acc: 0.9906\n",
      "Epoch 69/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9990\n",
      "Epoch 00069: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 181us/sample - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0565 - val_acc: 0.9898\n",
      "Epoch 70/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 00070: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 183us/sample - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0588 - val_acc: 0.9894\n",
      "Epoch 71/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 00071: val_loss did not improve from 0.04144\n",
      "40200/40200 [==============================] - 7s 172us/sample - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0581 - val_acc: 0.9905\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 131us/sample - loss: 0.0339 - acc: 0.9901\n",
      "Loss: 0.033898328276310716 Accuracy: 0.9901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    base = 'vis_2D_CNN_custom_ch_32_DO_050_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_2d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train, y_train, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val, y_val], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 63,050\n",
      "Trainable params: 63,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 97us/sample - loss: 0.0674 - acc: 0.9837\n",
      "Loss: 0.06743853561100914 Accuracy: 0.9837\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                15690     \n",
      "=================================================================\n",
      "Total params: 25,258\n",
      "Trainable params: 25,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 118us/sample - loss: 0.0343 - acc: 0.9891\n",
      "Loss: 0.034315878265905396 Accuracy: 0.9891\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 38,314\n",
      "Trainable params: 38,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 129us/sample - loss: 0.0267 - acc: 0.9921\n",
      "Loss: 0.026718628816661476 Accuracy: 0.9921\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 67,562\n",
      "Trainable params: 67,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 132us/sample - loss: 0.0272 - acc: 0.9917\n",
      "Loss: 0.02717794110944369 Accuracy: 0.9917\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 140,138\n",
      "Trainable params: 140,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 145us/sample - loss: 0.0339 - acc: 0.9901\n",
      "Loss: 0.033898328276310716 Accuracy: 0.9901\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = 'vis_2D_CNN_custom_ch_32_DO_050_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 6):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 63,050\n",
      "Trainable params: 63,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 127us/sample - loss: 0.0684 - acc: 0.9850\n",
      "Loss: 0.06835483827059506 Accuracy: 0.985\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                15690     \n",
      "=================================================================\n",
      "Total params: 25,258\n",
      "Trainable params: 25,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 119us/sample - loss: 0.0368 - acc: 0.9902\n",
      "Loss: 0.036768381542957744 Accuracy: 0.9902\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 38,314\n",
      "Trainable params: 38,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 137us/sample - loss: 0.0300 - acc: 0.9927\n",
      "Loss: 0.029953074127161746 Accuracy: 0.9927\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 67,562\n",
      "Trainable params: 67,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 148us/sample - loss: 0.0316 - acc: 0.9909\n",
      "Loss: 0.03163852475772169 Accuracy: 0.9909\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_050_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 140,138\n",
      "Trainable params: 140,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 159us/sample - loss: 0.0491 - acc: 0.9915\n",
      "Loss: 0.04913012339402099 Accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
