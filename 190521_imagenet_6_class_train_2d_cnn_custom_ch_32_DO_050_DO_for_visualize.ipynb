{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical, multi_gpu_model, Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.load(path.join(data_dir, 'imagenet_6_class_172_train_data.npz'))\n",
    "# val_data = np.load(path.join(data_dir, 'imagenet_6_class_172_val_data.npz'))\n",
    "\n",
    "x_train = np.load(path.join(data_dir, 'imagenet_6_class_172_x_train.npy'))\n",
    "y_train = np.load(path.join(data_dir, 'imagenet_6_class_172_y_train.npy'))\n",
    "x_val = np.load(path.join(data_dir, 'imagenet_6_class_172_x_val.npy'))\n",
    "y_val = np.load(path.join(data_dir, 'imagenet_6_class_172_y_val.npy'))\n",
    "y_list = np.load(path.join(data_dir, 'imagenet_6_class_172_y_list.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((235111, 172, 172, 3),\n",
       " (235111,),\n",
       " (3183, 172, 172, 3),\n",
       " (3183,),\n",
       " (3183, 172, 172, 3),\n",
       " (3183,),\n",
       " (6,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train = train_data['x_data']\n",
    "# y_train = train_data['y_data']\n",
    "# x_val = val_data['x_data']\n",
    "# y_val = val_data['y_data']\n",
    "x_test = x_val\n",
    "y_test = y_val\n",
    "# y_list = val_data['y_list']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((235111, 172, 172, 3),\n",
       " (235111, 6),\n",
       " (3183, 172, 172, 3),\n",
       " (3183, 6),\n",
       " (3183, 172, 172, 3),\n",
       " (3183, 6))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = y_val\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['bed', 'bird', 'cat', 'dog', 'house', 'tree'], dtype='<U5'),\n",
       " array([  2690.,  72641.,   6500., 149006.,   1611.,   2663.],\n",
       "       dtype=float32),\n",
       " array([  30., 1050.,   78., 1940.,   34.,   51.], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list, y_train.sum(axis=0), y_val.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train[0].shape\n",
    "output_size = len(y_list)\n",
    "\n",
    "# input_shape = (172,172,3)\n",
    "# output_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    for i in range(conv_num):\n",
    "        x = Conv2D(kernel_size=5, filters=32*(2**(i//2)), strides=(1,1), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling2D(pool_size=2, strides=(2,2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "  \n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 236672)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 236672)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 1420038   \n",
      "=================================================================\n",
      "Total params: 1,422,470\n",
      "Trainable params: 1,422,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 59168)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 59168)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 355014    \n",
      "=================================================================\n",
      "Total params: 383,078\n",
      "Trainable params: 383,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 185862    \n",
      "=================================================================\n",
      "Total params: 265,190\n",
      "Trainable params: 265,190\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 46470     \n",
      "=================================================================\n",
      "Total params: 228,262\n",
      "Trainable params: 228,262\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 11, 11, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 27654     \n",
      "=================================================================\n",
      "Total params: 414,374\n",
      "Trainable params: 414,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 11, 11, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 6, 6, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 6918      \n",
      "=================================================================\n",
      "Total params: 803,366\n",
      "Trainable params: 803,366\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 11, 11, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 6, 6, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 3, 3, 256)         819456    \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 1,622,054\n",
      "Trainable params: 1,622,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 8):\n",
    "    model = build_2d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalanceDataGenerator(Sequence):\n",
    "    def __init__(self, x_data, y_data, batch_size, shuffle=True):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_size = int(np.sum(y_data, axis=0).min())\n",
    "        self.data_shape = x_data.shape[1:]\n",
    "        self.y_label = self.y_data.argmax(axis=1)\n",
    "        self.labels = np.unique(self.y_label)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.labels) * self.sample_size / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.zeros((len(self.labels), self.sample_size))\n",
    "        for i, label in enumerate(self.labels):\n",
    "            y_index = np.argwhere(self.y_label==label).squeeze()\n",
    "            if self.shuffle == True:\n",
    "                self.indexes[i] = np.random.choice(y_index, \n",
    "                                   self.sample_size, \n",
    "                                   replace=False)\n",
    "            else:\n",
    "                self.indexes[i] = y_index[:self.sample_size]\n",
    "                \n",
    "        self.indexes = self.indexes.flatten().astype(np.int32)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "                \n",
    "    def __getitem__(self, batch_idx):\n",
    "        indices = self.indexes[batch_idx*self.batch_size: (batch_idx+1)*self.batch_size]\n",
    "        return self.x_data[indices], self.y_data[indices]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "data_generator = BalanceDataGenerator(x_train, y_train,\n",
    "                                      batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.5522 - acc: 0.3385WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 1.7990 - acc: 0.2100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.79902, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/001-1.7990.hdf5\n",
      "242/242 [==============================] - 29s 119ms/step - loss: 1.5512 - acc: 0.3388 - val_loss: 1.7990 - val_acc: 0.2100\n",
      "Epoch 2/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 1.2630 - acc: 0.4943WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 1.2678 - acc: 0.5225\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.79902 to 1.26777, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/002-1.2678.hdf5\n",
      "242/242 [==============================] - 50s 207ms/step - loss: 1.2622 - acc: 0.4945 - val_loss: 1.2678 - val_acc: 0.5225\n",
      "Epoch 3/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 1.1344 - acc: 0.5561WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 1.1917 - acc: 0.5425\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.26777 to 1.19165, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/003-1.1917.hdf5\n",
      "242/242 [==============================] - 39s 159ms/step - loss: 1.1349 - acc: 0.5560 - val_loss: 1.1917 - val_acc: 0.5425\n",
      "Epoch 4/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 1.0812 - acc: 0.5805WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 1.0866 - acc: 0.5775\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.19165 to 1.08664, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/004-1.0866.hdf5\n",
      "242/242 [==============================] - 49s 202ms/step - loss: 1.0813 - acc: 0.5810 - val_loss: 1.0866 - val_acc: 0.5775\n",
      "Epoch 5/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 1.0297 - acc: 0.6022WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 1.4449 - acc: 0.4300\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.08664\n",
      "242/242 [==============================] - 75s 311ms/step - loss: 1.0285 - acc: 0.6027 - val_loss: 1.4449 - val_acc: 0.4300\n",
      "Epoch 6/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.9885 - acc: 0.6154WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.9486 - acc: 0.6650\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.08664 to 0.94862, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/006-0.9486.hdf5\n",
      "242/242 [==============================] - 47s 194ms/step - loss: 0.9874 - acc: 0.6157 - val_loss: 0.9486 - val_acc: 0.6650\n",
      "Epoch 7/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.9355 - acc: 0.6411WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 1.0992 - acc: 0.5925\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.94862\n",
      "242/242 [==============================] - 47s 194ms/step - loss: 0.9348 - acc: 0.6414 - val_loss: 1.0992 - val_acc: 0.5925\n",
      "Epoch 8/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.9070 - acc: 0.6512WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 1.2999 - acc: 0.5081\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.94862\n",
      "242/242 [==============================] - 25s 104ms/step - loss: 0.9077 - acc: 0.6508 - val_loss: 1.2999 - val_acc: 0.5081\n",
      "Epoch 9/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8610 - acc: 0.6762WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 1.0027 - acc: 0.6094\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.94862\n",
      "242/242 [==============================] - 69s 284ms/step - loss: 0.8611 - acc: 0.6764 - val_loss: 1.0027 - val_acc: 0.6094\n",
      "Epoch 10/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8194 - acc: 0.6857WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.8060 - acc: 0.7075\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.94862 to 0.80597, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/010-0.8060.hdf5\n",
      "242/242 [==============================] - 49s 201ms/step - loss: 0.8195 - acc: 0.6857 - val_loss: 0.8060 - val_acc: 0.7075\n",
      "Epoch 11/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8045 - acc: 0.6903WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 26s 8ms/sample - loss: 0.8314 - acc: 0.6875\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.80597\n",
      "242/242 [==============================] - 73s 300ms/step - loss: 0.8038 - acc: 0.6906 - val_loss: 0.8314 - val_acc: 0.6875\n",
      "Epoch 12/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7971 - acc: 0.6974WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 1.0999 - acc: 0.5775\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.80597\n",
      "242/242 [==============================] - 75s 309ms/step - loss: 0.7970 - acc: 0.6976 - val_loss: 1.0999 - val_acc: 0.5775\n",
      "Epoch 13/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7689 - acc: 0.7089WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.7587 - acc: 0.7394\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.80597 to 0.75869, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/013-0.7587.hdf5\n",
      "242/242 [==============================] - 42s 174ms/step - loss: 0.7692 - acc: 0.7086 - val_loss: 0.7587 - val_acc: 0.7394\n",
      "Epoch 14/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7356 - acc: 0.7181WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 1.0013 - acc: 0.6022\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.75869\n",
      "242/242 [==============================] - 28s 116ms/step - loss: 0.7351 - acc: 0.7181 - val_loss: 1.0013 - val_acc: 0.6022\n",
      "Epoch 15/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7073 - acc: 0.7287WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 6s 2ms/sample - loss: 0.7767 - acc: 0.6975\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.75869\n",
      "242/242 [==============================] - 19s 77ms/step - loss: 0.7068 - acc: 0.7286 - val_loss: 0.7767 - val_acc: 0.6975\n",
      "Epoch 16/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7130 - acc: 0.7230WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.6685 - acc: 0.7738\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.75869 to 0.66854, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/016-0.6685.hdf5\n",
      "242/242 [==============================] - 21s 88ms/step - loss: 0.7121 - acc: 0.7237 - val_loss: 0.6685 - val_acc: 0.7738\n",
      "Epoch 17/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.7381WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.6956 - acc: 0.7475\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.66854\n",
      "242/242 [==============================] - 19s 78ms/step - loss: 0.6905 - acc: 0.7385 - val_loss: 0.6956 - val_acc: 0.7475\n",
      "Epoch 18/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.6875 - acc: 0.7375WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.6566 - acc: 0.7700\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.66854 to 0.65665, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/018-0.6566.hdf5\n",
      "242/242 [==============================] - 19s 79ms/step - loss: 0.6874 - acc: 0.7372 - val_loss: 0.6566 - val_acc: 0.7700\n",
      "Epoch 19/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.6792 - acc: 0.7390WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.8971 - acc: 0.6522\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.65665\n",
      "242/242 [==============================] - 20s 82ms/step - loss: 0.6779 - acc: 0.7396 - val_loss: 0.8971 - val_acc: 0.6522\n",
      "Epoch 20/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.6466 - acc: 0.7519WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.8144 - acc: 0.7028\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.65665\n",
      "242/242 [==============================] - 21s 86ms/step - loss: 0.6460 - acc: 0.7523 - val_loss: 0.8144 - val_acc: 0.7028\n",
      "Epoch 21/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.6326 - acc: 0.7626WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.5061 - acc: 0.8319\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.65665 to 0.50610, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/021-0.5061.hdf5\n",
      "242/242 [==============================] - 19s 78ms/step - loss: 0.6329 - acc: 0.7624 - val_loss: 0.5061 - val_acc: 0.8319\n",
      "Epoch 22/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6352 - acc: 0.7574WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.7766 - acc: 0.7150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.50610\n",
      "242/242 [==============================] - 19s 78ms/step - loss: 0.6351 - acc: 0.7574 - val_loss: 0.7766 - val_acc: 0.7150\n",
      "Epoch 23/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6181 - acc: 0.7662WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4844 - acc: 0.8453\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.50610 to 0.48444, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/023-0.4844.hdf5\n",
      "242/242 [==============================] - 21s 87ms/step - loss: 0.6183 - acc: 0.7659 - val_loss: 0.4844 - val_acc: 0.8453\n",
      "Epoch 24/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.6139 - acc: 0.7632WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.5049 - acc: 0.8300\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48444\n",
      "242/242 [==============================] - 19s 77ms/step - loss: 0.6136 - acc: 0.7634 - val_loss: 0.5049 - val_acc: 0.8300\n",
      "Epoch 25/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5990 - acc: 0.7729WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4493 - acc: 0.8662\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.48444 to 0.44930, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/025-0.4493.hdf5\n",
      "242/242 [==============================] - 20s 83ms/step - loss: 0.5985 - acc: 0.7735 - val_loss: 0.4493 - val_acc: 0.8662\n",
      "Epoch 26/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.7732WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.4952 - acc: 0.8550\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.44930\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.6018 - acc: 0.7732 - val_loss: 0.4952 - val_acc: 0.8550\n",
      "Epoch 27/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5769 - acc: 0.7808WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.6636 - acc: 0.7600\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.44930\n",
      "242/242 [==============================] - 19s 78ms/step - loss: 0.5765 - acc: 0.7810 - val_loss: 0.6636 - val_acc: 0.7600\n",
      "Epoch 28/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.7774- ETA:WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5793 - acc: 0.8016\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.44930\n",
      "242/242 [==============================] - 21s 89ms/step - loss: 0.5798 - acc: 0.7772 - val_loss: 0.5793 - val_acc: 0.8016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5787 - acc: 0.7812WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.6325 - acc: 0.7650\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.44930\n",
      "242/242 [==============================] - 23s 94ms/step - loss: 0.5796 - acc: 0.7811 - val_loss: 0.6325 - val_acc: 0.7650\n",
      "Epoch 30/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5610 - acc: 0.7852WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 6s 2ms/sample - loss: 0.6354 - acc: 0.7969\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.44930\n",
      "242/242 [==============================] - 19s 79ms/step - loss: 0.5613 - acc: 0.7849 - val_loss: 0.6354 - val_acc: 0.7969\n",
      "Epoch 31/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5655 - acc: 0.7817WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.6812 - acc: 0.7697 2s - loss: 0.6449 -\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.44930\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.5654 - acc: 0.7815 - val_loss: 0.6812 - val_acc: 0.7697\n",
      "Epoch 32/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7934WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.4821 - acc: 0.8363\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.44930\n",
      "242/242 [==============================] - 19s 79ms/step - loss: 0.5366 - acc: 0.7936 - val_loss: 0.4821 - val_acc: 0.8363\n",
      "Epoch 33/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5414 - acc: 0.7917WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.6896 - acc: 0.7513\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.44930\n",
      "242/242 [==============================] - 19s 79ms/step - loss: 0.5400 - acc: 0.7921 - val_loss: 0.6896 - val_acc: 0.7513\n",
      "Epoch 34/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7949WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.7958 - acc: 0.7250\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.44930\n",
      "242/242 [==============================] - 19s 79ms/step - loss: 0.5346 - acc: 0.7948 - val_loss: 0.7958 - val_acc: 0.7250\n",
      "Epoch 35/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.7988WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.6374 - acc: 0.7775\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.44930\n",
      "242/242 [==============================] - 19s 77ms/step - loss: 0.5292 - acc: 0.7985 - val_loss: 0.6374 - val_acc: 0.7775\n",
      "Epoch 36/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5144 - acc: 0.8043WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.3834 - acc: 0.8825\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.44930 to 0.38344, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/036-0.3834.hdf5\n",
      "242/242 [==============================] - 24s 98ms/step - loss: 0.5134 - acc: 0.8048 - val_loss: 0.3834 - val_acc: 0.8825\n",
      "Epoch 37/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8093WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4843 - acc: 0.8334\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 21s 87ms/step - loss: 0.5086 - acc: 0.8089 - val_loss: 0.4843 - val_acc: 0.8334\n",
      "Epoch 38/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5070 - acc: 0.8058WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.5716 - acc: 0.8203\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 20s 81ms/step - loss: 0.5068 - acc: 0.8059 - val_loss: 0.5716 - val_acc: 0.8203\n",
      "Epoch 39/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.8074WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.5648 - acc: 0.8163\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.5098 - acc: 0.8081 - val_loss: 0.5648 - val_acc: 0.8163\n",
      "Epoch 40/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4963 - acc: 0.8155WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.6602 - acc: 0.7713\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 22s 89ms/step - loss: 0.4960 - acc: 0.8157 - val_loss: 0.6602 - val_acc: 0.7713\n",
      "Epoch 41/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4914 - acc: 0.8130WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 6s 2ms/sample - loss: 0.6364 - acc: 0.7775\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 18s 75ms/step - loss: 0.4910 - acc: 0.8131 - val_loss: 0.6364 - val_acc: 0.7775\n",
      "Epoch 42/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4924 - acc: 0.8150WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.5023 - acc: 0.8344\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 19s 79ms/step - loss: 0.4914 - acc: 0.8155 - val_loss: 0.5023 - val_acc: 0.8344\n",
      "Epoch 43/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4981 - acc: 0.8146WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.4612 - acc: 0.8400\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 20s 82ms/step - loss: 0.4990 - acc: 0.8144 - val_loss: 0.4612 - val_acc: 0.8400\n",
      "Epoch 44/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4642 - acc: 0.8290WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.7248 - acc: 0.7509\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 24s 98ms/step - loss: 0.4645 - acc: 0.8288 - val_loss: 0.7248 - val_acc: 0.7509\n",
      "Epoch 45/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4836 - acc: 0.8134WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.5886 - acc: 0.7950\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.4838 - acc: 0.8134 - val_loss: 0.5886 - val_acc: 0.7950\n",
      "Epoch 46/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4769 - acc: 0.8192WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 6s 2ms/sample - loss: 0.5267 - acc: 0.8225\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 19s 79ms/step - loss: 0.4778 - acc: 0.8185 - val_loss: 0.5267 - val_acc: 0.8225\n",
      "Epoch 47/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.8286WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5574 - acc: 0.8016\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 20s 84ms/step - loss: 0.4503 - acc: 0.8287 - val_loss: 0.5574 - val_acc: 0.8016\n",
      "Epoch 48/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.8250WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.6145 - acc: 0.79255s - loss: 0\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 21s 87ms/step - loss: 0.4604 - acc: 0.8245 - val_loss: 0.6145 - val_acc: 0.7925\n",
      "Epoch 49/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4538 - acc: 0.8287WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.4456 - acc: 0.8475\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 19s 79ms/step - loss: 0.4533 - acc: 0.8289 - val_loss: 0.4456 - val_acc: 0.8475\n",
      "Epoch 50/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4513 - acc: 0.8288WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.5110 - acc: 0.8341\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 21s 86ms/step - loss: 0.4514 - acc: 0.8287 - val_loss: 0.5110 - val_acc: 0.8341\n",
      "Epoch 51/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4514 - acc: 0.8283WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.5644 - acc: 0.8150\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.38344\n",
      "242/242 [==============================] - 20s 81ms/step - loss: 0.4521 - acc: 0.8282 - val_loss: 0.5644 - val_acc: 0.8150\n",
      "Epoch 52/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4255 - acc: 0.8363WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3546 - acc: 0.8875\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.38344 to 0.35462, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/052-0.3546.hdf5\n",
      "242/242 [==============================] - 22s 90ms/step - loss: 0.4254 - acc: 0.8364 - val_loss: 0.3546 - val_acc: 0.8875\n",
      "Epoch 53/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4384 - acc: 0.8353WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.4193 - acc: 0.8537\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.35462\n",
      "242/242 [==============================] - 29s 122ms/step - loss: 0.4379 - acc: 0.8353 - val_loss: 0.4193 - val_acc: 0.8537\n",
      "Epoch 54/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4396 - acc: 0.8304WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.3521 - acc: 0.8800\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.35462 to 0.35206, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/054-0.3521.hdf5\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.4405 - acc: 0.8301 - val_loss: 0.3521 - val_acc: 0.8800\n",
      "Epoch 55/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8405WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4760 - acc: 0.8334\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 22s 92ms/step - loss: 0.4161 - acc: 0.8404 - val_loss: 0.4760 - val_acc: 0.8334\n",
      "Epoch 56/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8484WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.4947 - acc: 0.8328\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.4096 - acc: 0.8481 - val_loss: 0.4947 - val_acc: 0.8328\n",
      "Epoch 57/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8456WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5040 - acc: 0.8375\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 22s 90ms/step - loss: 0.4031 - acc: 0.8454 - val_loss: 0.5040 - val_acc: 0.8375\n",
      "Epoch 58/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8471WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5141 - acc: 0.8100\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 21s 85ms/step - loss: 0.4085 - acc: 0.8472 - val_loss: 0.5141 - val_acc: 0.8100\n",
      "Epoch 59/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8472WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.4600 - acc: 0.8409\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 20s 81ms/step - loss: 0.4067 - acc: 0.8469 - val_loss: 0.4600 - val_acc: 0.8409\n",
      "Epoch 60/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3974 - acc: 0.8490WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.7988 - acc: 0.7275\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 20s 81ms/step - loss: 0.3985 - acc: 0.8486 - val_loss: 0.7988 - val_acc: 0.7275\n",
      "Epoch 61/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8426WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.5380 - acc: 0.8184\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 23s 94ms/step - loss: 0.4115 - acc: 0.8423 - val_loss: 0.5380 - val_acc: 0.8184\n",
      "Epoch 62/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8537WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 6s 2ms/sample - loss: 0.3738 - acc: 0.8650\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 19s 78ms/step - loss: 0.4003 - acc: 0.8540 - val_loss: 0.3738 - val_acc: 0.8650\n",
      "Epoch 63/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8508WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3690 - acc: 0.8813s may duplicate y\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 24s 99ms/step - loss: 0.3911 - acc: 0.8507 - val_loss: 0.3690 - val_acc: 0.8813\n",
      "Epoch 64/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8510WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.6522 - acc: 0.7650\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 22s 91ms/step - loss: 0.3940 - acc: 0.8511 - val_loss: 0.6522 - val_acc: 0.7650\n",
      "Epoch 65/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8509WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5221 - acc: 0.8138\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 21s 85ms/step - loss: 0.3871 - acc: 0.8508 - val_loss: 0.5221 - val_acc: 0.8138\n",
      "Epoch 66/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8552WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5245 - acc: 0.8175\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 22s 92ms/step - loss: 0.3906 - acc: 0.8554 - val_loss: 0.5245 - val_acc: 0.8175\n",
      "Epoch 67/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8541WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.4394 - acc: 0.8550\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 29s 121ms/step - loss: 0.3947 - acc: 0.8538 - val_loss: 0.4394 - val_acc: 0.8550\n",
      "Epoch 68/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3873 - acc: 0.8546WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.4868 - acc: 0.8375\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 20s 84ms/step - loss: 0.3862 - acc: 0.8550 - val_loss: 0.4868 - val_acc: 0.8375\n",
      "Epoch 69/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3744 - acc: 0.8588WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.4942 - acc: 0.8325\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 21s 86ms/step - loss: 0.3744 - acc: 0.8590 - val_loss: 0.4942 - val_acc: 0.8325\n",
      "Epoch 70/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8595WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5179 - acc: 0.8288\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 22s 90ms/step - loss: 0.3813 - acc: 0.8598 - val_loss: 0.5179 - val_acc: 0.8288\n",
      "Epoch 71/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3598 - acc: 0.8666WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5299 - acc: 0.8128\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 21s 87ms/step - loss: 0.3592 - acc: 0.8667 - val_loss: 0.5299 - val_acc: 0.8128\n",
      "Epoch 72/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8622WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.4387 - acc: 0.8425\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 34s 141ms/step - loss: 0.3651 - acc: 0.8620 - val_loss: 0.4387 - val_acc: 0.8425\n",
      "Epoch 73/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8656WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3841 - acc: 0.8725\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 28s 118ms/step - loss: 0.3588 - acc: 0.8658 - val_loss: 0.3841 - val_acc: 0.8725\n",
      "Epoch 74/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8638WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.4902 - acc: 0.8175\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 22s 90ms/step - loss: 0.3618 - acc: 0.8641 - val_loss: 0.4902 - val_acc: 0.8175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8616WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4156 - acc: 0.8575\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 21s 88ms/step - loss: 0.3666 - acc: 0.8616 - val_loss: 0.4156 - val_acc: 0.8575\n",
      "Epoch 76/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8713WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 6s 2ms/sample - loss: 0.4662 - acc: 0.8341\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.35206\n",
      "242/242 [==============================] - 19s 77ms/step - loss: 0.3485 - acc: 0.8710 - val_loss: 0.4662 - val_acc: 0.8341\n",
      "Epoch 77/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.8695WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3043 - acc: 0.9013\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.35206 to 0.30430, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/077-0.3043.hdf5\n",
      "242/242 [==============================] - 20s 84ms/step - loss: 0.3505 - acc: 0.8693 - val_loss: 0.3043 - val_acc: 0.9013\n",
      "Epoch 78/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3510 - acc: 0.8648WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.3388 - acc: 0.8775\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.30430\n",
      "242/242 [==============================] - 20s 84ms/step - loss: 0.3501 - acc: 0.8651 - val_loss: 0.3388 - val_acc: 0.8775\n",
      "Epoch 79/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8725WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4693 - acc: 0.8375\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.30430\n",
      "242/242 [==============================] - 29s 122ms/step - loss: 0.3402 - acc: 0.8726 - val_loss: 0.4693 - val_acc: 0.8375\n",
      "Epoch 80/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8722WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3226 - acc: 0.9009\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.30430\n",
      "242/242 [==============================] - 24s 98ms/step - loss: 0.3352 - acc: 0.8725 - val_loss: 0.3226 - val_acc: 0.9009\n",
      "Epoch 81/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.8711WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.3788 - acc: 0.8800\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.30430\n",
      "242/242 [==============================] - 19s 79ms/step - loss: 0.3453 - acc: 0.8710 - val_loss: 0.3788 - val_acc: 0.8800\n",
      "Epoch 82/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8661WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.4071 - acc: 0.8466\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.30430\n",
      "242/242 [==============================] - 23s 96ms/step - loss: 0.3554 - acc: 0.8655 - val_loss: 0.4071 - val_acc: 0.8466\n",
      "Epoch 83/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8739WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.2773 - acc: 0.9175\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.30430 to 0.27731, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/083-0.2773.hdf5\n",
      "242/242 [==============================] - 21s 85ms/step - loss: 0.3308 - acc: 0.8740 - val_loss: 0.2773 - val_acc: 0.9175\n",
      "Epoch 84/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8743WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.4265 - acc: 0.8600\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 21s 86ms/step - loss: 0.3320 - acc: 0.8745 - val_loss: 0.4265 - val_acc: 0.8600\n",
      "Epoch 85/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.8733WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3038 - acc: 0.8988\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 22s 92ms/step - loss: 0.3339 - acc: 0.8734 - val_loss: 0.3038 - val_acc: 0.8988\n",
      "Epoch 86/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.8796WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.4235 - acc: 0.8625\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 21s 88ms/step - loss: 0.3209 - acc: 0.8796 - val_loss: 0.4235 - val_acc: 0.8625\n",
      "Epoch 87/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8781WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.7416 - acc: 0.7450\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 22s 90ms/step - loss: 0.3301 - acc: 0.8780 - val_loss: 0.7416 - val_acc: 0.7450\n",
      "Epoch 88/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.8808WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.3002 - acc: 0.9125\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 27s 111ms/step - loss: 0.3143 - acc: 0.8806 - val_loss: 0.3002 - val_acc: 0.9125\n",
      "Epoch 89/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8777WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5231 - acc: 0.8181\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 21s 88ms/step - loss: 0.3280 - acc: 0.8779 - val_loss: 0.5231 - val_acc: 0.8181\n",
      "Epoch 90/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/242 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8803WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.4112 - acc: 0.8616\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 20s 83ms/step - loss: 0.3218 - acc: 0.8803 - val_loss: 0.4112 - val_acc: 0.8616\n",
      "Epoch 91/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.8897WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.4330 - acc: 0.8556\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 24s 99ms/step - loss: 0.2957 - acc: 0.8897 - val_loss: 0.4330 - val_acc: 0.8556\n",
      "Epoch 92/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.8817WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3029 - acc: 0.8919\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 20s 84ms/step - loss: 0.3078 - acc: 0.8816 - val_loss: 0.3029 - val_acc: 0.8919\n",
      "Epoch 93/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2984 - acc: 0.8871WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4327 - acc: 0.8553\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 22s 91ms/step - loss: 0.2981 - acc: 0.8873 - val_loss: 0.4327 - val_acc: 0.8553\n",
      "Epoch 94/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.8903WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5253 - acc: 0.8200\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 21s 87ms/step - loss: 0.2956 - acc: 0.8899 - val_loss: 0.5253 - val_acc: 0.8200\n",
      "Epoch 95/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.8792WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5532 - acc: 0.7903\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 30s 123ms/step - loss: 0.3138 - acc: 0.8791 - val_loss: 0.5532 - val_acc: 0.7903\n",
      "Epoch 96/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.8900WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4406 - acc: 0.8481\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 23s 95ms/step - loss: 0.2942 - acc: 0.8906 - val_loss: 0.4406 - val_acc: 0.8481\n",
      "Epoch 97/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.8835WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.2830 - acc: 0.8984\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 28s 114ms/step - loss: 0.3066 - acc: 0.8833 - val_loss: 0.2830 - val_acc: 0.8984\n",
      "Epoch 98/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.8936WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3299 - acc: 0.8950\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 24s 101ms/step - loss: 0.2887 - acc: 0.8934 - val_loss: 0.3299 - val_acc: 0.8950\n",
      "Epoch 99/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.8911WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4510 - acc: 0.8512\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 21s 87ms/step - loss: 0.2977 - acc: 0.8911 - val_loss: 0.4510 - val_acc: 0.8512\n",
      "Epoch 100/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.8889WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.3246 - acc: 0.8859\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 21s 86ms/step - loss: 0.2966 - acc: 0.8889 - val_loss: 0.3246 - val_acc: 0.8859\n",
      "Epoch 101/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.8874WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3252 - acc: 0.8888\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 23s 96ms/step - loss: 0.2965 - acc: 0.8872 - val_loss: 0.3252 - val_acc: 0.8888\n",
      "Epoch 102/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.8936WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4883 - acc: 0.8306\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.27731\n",
      "242/242 [==============================] - 22s 90ms/step - loss: 0.2846 - acc: 0.8936 - val_loss: 0.4883 - val_acc: 0.8306\n",
      "Epoch 103/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.8956WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.2475 - acc: 0.9294\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.27731 to 0.24751, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/103-0.2475.hdf5\n",
      "242/242 [==============================] - 23s 94ms/step - loss: 0.2842 - acc: 0.8954 - val_loss: 0.2475 - val_acc: 0.9294\n",
      "Epoch 104/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2879 - acc: 0.8923WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3452 - acc: 0.8850\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.24751\n",
      "242/242 [==============================] - 27s 113ms/step - loss: 0.2883 - acc: 0.8923 - val_loss: 0.3452 - val_acc: 0.8850\n",
      "Epoch 105/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/242 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.8934WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.1898 - acc: 0.9419\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.24751 to 0.18983, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv_checkpoint/105-0.1898.hdf5\n",
      "242/242 [==============================] - 21s 87ms/step - loss: 0.2918 - acc: 0.8928 - val_loss: 0.1898 - val_acc: 0.9419\n",
      "Epoch 106/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.8912WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3763 - acc: 0.8850 2s - loss: 0.3521 - \n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 22s 90ms/step - loss: 0.2779 - acc: 0.8911 - val_loss: 0.3763 - val_acc: 0.8850\n",
      "Epoch 107/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.9009WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3893 - acc: 0.8619\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 21s 86ms/step - loss: 0.2705 - acc: 0.9010 - val_loss: 0.3893 - val_acc: 0.8619\n",
      "Epoch 108/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.8985WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5904 - acc: 0.7994\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 24s 100ms/step - loss: 0.2749 - acc: 0.8992 - val_loss: 0.5904 - val_acc: 0.7994\n",
      "Epoch 109/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.8985WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5400 - acc: 0.8119\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 21s 86ms/step - loss: 0.2745 - acc: 0.8986 - val_loss: 0.5400 - val_acc: 0.8119\n",
      "Epoch 110/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.8936WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.3212 - acc: 0.8975\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 23s 97ms/step - loss: 0.2808 - acc: 0.8938 - val_loss: 0.3212 - val_acc: 0.8975\n",
      "Epoch 111/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.8995WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.3747 - acc: 0.8634\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.2730 - acc: 0.8996 - val_loss: 0.3747 - val_acc: 0.8634\n",
      "Epoch 112/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9048WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2603 - acc: 0.9100\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 22s 90ms/step - loss: 0.2567 - acc: 0.9046 - val_loss: 0.2603 - val_acc: 0.9100\n",
      "Epoch 113/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.8935WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.3011 - acc: 0.8975\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 21s 85ms/step - loss: 0.2846 - acc: 0.8936 - val_loss: 0.3011 - val_acc: 0.8975\n",
      "Epoch 114/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9026WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.3403 - acc: 0.8684\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 36s 147ms/step - loss: 0.2643 - acc: 0.9022 - val_loss: 0.3403 - val_acc: 0.8684\n",
      "Epoch 115/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.8996WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3849 - acc: 0.8622\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 23s 96ms/step - loss: 0.2667 - acc: 0.8998 - val_loss: 0.3849 - val_acc: 0.8622\n",
      "Epoch 116/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.8999WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3315 - acc: 0.8975\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 26s 106ms/step - loss: 0.2690 - acc: 0.8998 - val_loss: 0.3315 - val_acc: 0.8975\n",
      "Epoch 117/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9014WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3638 - acc: 0.8712\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 26s 106ms/step - loss: 0.2705 - acc: 0.9019 - val_loss: 0.3638 - val_acc: 0.8712\n",
      "Epoch 118/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2667 - acc: 0.9007WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3787 - acc: 0.8600\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 25s 103ms/step - loss: 0.2666 - acc: 0.9008 - val_loss: 0.3787 - val_acc: 0.8600\n",
      "Epoch 119/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.8995WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.4009 - acc: 0.8453\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 30s 123ms/step - loss: 0.2650 - acc: 0.8995 - val_loss: 0.4009 - val_acc: 0.8453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9000WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 0.3585 - acc: 0.8775\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 20s 84ms/step - loss: 0.2703 - acc: 0.9000 - val_loss: 0.3585 - val_acc: 0.8775\n",
      "Epoch 121/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9003WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3989 - acc: 0.8634\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 34s 141ms/step - loss: 0.2647 - acc: 0.9002 - val_loss: 0.3989 - val_acc: 0.8634\n",
      "Epoch 122/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9062WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3764 - acc: 0.8741\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 22s 90ms/step - loss: 0.2547 - acc: 0.9063 - val_loss: 0.3764 - val_acc: 0.8741\n",
      "Epoch 123/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.9026WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3073 - acc: 0.8916\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 36s 150ms/step - loss: 0.2585 - acc: 0.9030 - val_loss: 0.3073 - val_acc: 0.8916\n",
      "Epoch 124/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2556 - acc: 0.9029WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2873 - acc: 0.9000\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 25s 104ms/step - loss: 0.2552 - acc: 0.9030 - val_loss: 0.2873 - val_acc: 0.9000\n",
      "Epoch 125/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9127WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.3866 - acc: 0.8725\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 23s 96ms/step - loss: 0.2385 - acc: 0.9129 - val_loss: 0.3866 - val_acc: 0.8725\n",
      "Epoch 126/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9071WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3040 - acc: 0.8919\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 27s 111ms/step - loss: 0.2544 - acc: 0.9072 - val_loss: 0.3040 - val_acc: 0.8919\n",
      "Epoch 127/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9110WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2408 - acc: 0.9116\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 23s 95ms/step - loss: 0.2448 - acc: 0.9111 - val_loss: 0.2408 - val_acc: 0.9116\n",
      "Epoch 128/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9126WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2759 - acc: 0.9025\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 30s 126ms/step - loss: 0.2463 - acc: 0.9123 - val_loss: 0.2759 - val_acc: 0.9025\n",
      "Epoch 129/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9094WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3868 - acc: 0.8600\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 22s 92ms/step - loss: 0.2404 - acc: 0.9094 - val_loss: 0.3868 - val_acc: 0.8600\n",
      "Epoch 130/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2391 - acc: 0.9084- WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.2609 - acc: 0.9050\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 40s 165ms/step - loss: 0.2391 - acc: 0.9083 - val_loss: 0.2609 - val_acc: 0.9050\n",
      "Epoch 131/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9052WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3150 - acc: 0.8875\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 30s 126ms/step - loss: 0.2571 - acc: 0.9049 - val_loss: 0.3150 - val_acc: 0.8875\n",
      "Epoch 132/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9072WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.3039 - acc: 0.8925\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 32s 133ms/step - loss: 0.2490 - acc: 0.9074 - val_loss: 0.3039 - val_acc: 0.8925\n",
      "Epoch 133/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9112- ETA: 4s - WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.3245 - acc: 0.8925\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 29s 121ms/step - loss: 0.2401 - acc: 0.9115 - val_loss: 0.3245 - val_acc: 0.8925\n",
      "Epoch 134/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9149WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2734 - acc: 0.9116\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 25s 102ms/step - loss: 0.2324 - acc: 0.9148 - val_loss: 0.2734 - val_acc: 0.9116\n",
      "Epoch 135/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9071WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2733 - acc: 0.9100\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 28s 116ms/step - loss: 0.2523 - acc: 0.9073 - val_loss: 0.2733 - val_acc: 0.9100\n",
      "Epoch 136/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9099WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.2757 - acc: 0.9025\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 25s 105ms/step - loss: 0.2464 - acc: 0.9098 - val_loss: 0.2757 - val_acc: 0.9025\n",
      "Epoch 137/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9144WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.2740 - acc: 0.9053\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 25s 103ms/step - loss: 0.2305 - acc: 0.9143 - val_loss: 0.2740 - val_acc: 0.9053\n",
      "Epoch 138/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2382 - acc: 0.9140WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.2211 - acc: 0.9212\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 36s 151ms/step - loss: 0.2386 - acc: 0.9137 - val_loss: 0.2211 - val_acc: 0.9212\n",
      "Epoch 139/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9082WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.2887 - acc: 0.9069\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 29s 120ms/step - loss: 0.2486 - acc: 0.9083 - val_loss: 0.2887 - val_acc: 0.9069\n",
      "Epoch 140/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9107WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2851 - acc: 0.8994\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 35s 146ms/step - loss: 0.2376 - acc: 0.9108 - val_loss: 0.2851 - val_acc: 0.8994\n",
      "Epoch 141/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9127WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3371 - acc: 0.8722\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 22s 90ms/step - loss: 0.2375 - acc: 0.9128 - val_loss: 0.3371 - val_acc: 0.8722\n",
      "Epoch 142/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9137WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3391 - acc: 0.8856\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 30s 123ms/step - loss: 0.2351 - acc: 0.9137 - val_loss: 0.3391 - val_acc: 0.8856\n",
      "Epoch 143/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9185WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.3403 - acc: 0.8800\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 35s 143ms/step - loss: 0.2313 - acc: 0.9180 - val_loss: 0.3403 - val_acc: 0.8800\n",
      "Epoch 144/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9147WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3747 - acc: 0.8712\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 25s 104ms/step - loss: 0.2379 - acc: 0.9148 - val_loss: 0.3747 - val_acc: 0.8712\n",
      "Epoch 145/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9153WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.3145 - acc: 0.8975\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 39s 159ms/step - loss: 0.2298 - acc: 0.9156 - val_loss: 0.3145 - val_acc: 0.8975\n",
      "Epoch 146/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9152WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.4275 - acc: 0.8612\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 25s 102ms/step - loss: 0.2335 - acc: 0.9154 - val_loss: 0.4275 - val_acc: 0.8612\n",
      "Epoch 147/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9174WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4072 - acc: 0.8734\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 33s 136ms/step - loss: 0.2244 - acc: 0.9176 - val_loss: 0.4072 - val_acc: 0.8734\n",
      "Epoch 148/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9110WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3374 - acc: 0.8850\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 31s 129ms/step - loss: 0.2359 - acc: 0.9112 - val_loss: 0.3374 - val_acc: 0.8850\n",
      "Epoch 149/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9186WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3539 - acc: 0.8825\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 27s 111ms/step - loss: 0.2214 - acc: 0.9187 - val_loss: 0.3539 - val_acc: 0.8825\n",
      "Epoch 150/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9154WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.2076 - acc: 0.9325\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 29s 118ms/step - loss: 0.2239 - acc: 0.9153 - val_loss: 0.2076 - val_acc: 0.9325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9173WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5081 - acc: 0.8313\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 22s 90ms/step - loss: 0.2287 - acc: 0.9172 - val_loss: 0.5081 - val_acc: 0.8313\n",
      "Epoch 152/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9139WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2907 - acc: 0.8841\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 32s 131ms/step - loss: 0.2291 - acc: 0.9138 - val_loss: 0.2907 - val_acc: 0.8841\n",
      "Epoch 153/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9202WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3509 - acc: 0.8772\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 36s 148ms/step - loss: 0.2170 - acc: 0.9201 - val_loss: 0.3509 - val_acc: 0.8772\n",
      "Epoch 154/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9184WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3886 - acc: 0.8550\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 34s 141ms/step - loss: 0.2222 - acc: 0.9181 - val_loss: 0.3886 - val_acc: 0.8550\n",
      "Epoch 155/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9160WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2776 - acc: 0.9000\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.18983\n",
      "242/242 [==============================] - 26s 106ms/step - loss: 0.2174 - acc: 0.9162 - val_loss: 0.2776 - val_acc: 0.9000\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3183 [==============================] - 3s 936us/sample - loss: 0.4856 - acc: 0.8014\n",
      "Loss: 0.48557202282975837 Accuracy: 0.8014452\n",
      "\n",
      "Epoch 1/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.5179 - acc: 0.3585WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 1.3299 - acc: 0.5069\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.32989, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/001-1.3299.hdf5\n",
      "242/242 [==============================] - 23s 96ms/step - loss: 1.5169 - acc: 0.3589 - val_loss: 1.3299 - val_acc: 0.5069\n",
      "Epoch 2/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 1.2701 - acc: 0.4888WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 2ms/sample - loss: 1.4597 - acc: 0.3800\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.32989\n",
      "242/242 [==============================] - 21s 85ms/step - loss: 1.2700 - acc: 0.4888 - val_loss: 1.4597 - val_acc: 0.3800\n",
      "Epoch 3/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 1.1562 - acc: 0.5435WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.9946 - acc: 0.7075\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.32989 to 0.99458, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/003-0.9946.hdf5\n",
      "242/242 [==============================] - 23s 97ms/step - loss: 1.1561 - acc: 0.5433 - val_loss: 0.9946 - val_acc: 0.7075\n",
      "Epoch 4/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.0692 - acc: 0.5910WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 1.0921 - acc: 0.5838\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.99458\n",
      "242/242 [==============================] - 19s 78ms/step - loss: 1.0689 - acc: 0.5915 - val_loss: 1.0921 - val_acc: 0.5838\n",
      "Epoch 5/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.9980 - acc: 0.6193WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 1.0856 - acc: 0.6025\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.99458\n",
      "242/242 [==============================] - 25s 105ms/step - loss: 0.9978 - acc: 0.6194 - val_loss: 1.0856 - val_acc: 0.6025\n",
      "Epoch 6/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.9339 - acc: 0.6354WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 1.1008 - acc: 0.5825\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99458\n",
      "242/242 [==============================] - 29s 119ms/step - loss: 0.9329 - acc: 0.6356 - val_loss: 1.1008 - val_acc: 0.5825\n",
      "Epoch 7/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.8806 - acc: 0.6605WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.6917 - acc: 0.7500\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.99458 to 0.69174, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/007-0.6917.hdf5\n",
      "242/242 [==============================] - 25s 105ms/step - loss: 0.8806 - acc: 0.6609 - val_loss: 0.6917 - val_acc: 0.7500\n",
      "Epoch 8/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8334 - acc: 0.6754WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.9097 - acc: 0.6475\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69174\n",
      "242/242 [==============================] - 31s 128ms/step - loss: 0.8333 - acc: 0.6749 - val_loss: 0.9097 - val_acc: 0.6475\n",
      "Epoch 9/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7891 - acc: 0.6937WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.7738 - acc: 0.7309\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.69174\n",
      "242/242 [==============================] - 22s 92ms/step - loss: 0.7902 - acc: 0.6936 - val_loss: 0.7738 - val_acc: 0.7309\n",
      "Epoch 10/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.7741 - acc: 0.6975WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5591 - acc: 0.8175\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.69174 to 0.55909, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/010-0.5591.hdf5\n",
      "242/242 [==============================] - 26s 108ms/step - loss: 0.7740 - acc: 0.6977 - val_loss: 0.5591 - val_acc: 0.8175\n",
      "Epoch 11/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7501 - acc: 0.7154WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.9055 - acc: 0.6553\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.55909\n",
      "242/242 [==============================] - 28s 115ms/step - loss: 0.7503 - acc: 0.7153 - val_loss: 0.9055 - val_acc: 0.6553\n",
      "Epoch 12/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.7343 - acc: 0.7177WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 1.2140 - acc: 0.5269\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.55909\n",
      "242/242 [==============================] - 25s 105ms/step - loss: 0.7338 - acc: 0.7181 - val_loss: 1.2140 - val_acc: 0.5269\n",
      "Epoch 13/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6981 - acc: 0.7299WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.8605 - acc: 0.6547\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.55909\n",
      "242/242 [==============================] - 29s 118ms/step - loss: 0.6978 - acc: 0.7301 - val_loss: 0.8605 - val_acc: 0.6547\n",
      "Epoch 14/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6921 - acc: 0.7344WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.6383 - acc: 0.7653\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.55909\n",
      "242/242 [==============================] - 28s 118ms/step - loss: 0.6918 - acc: 0.7346 - val_loss: 0.6383 - val_acc: 0.7653\n",
      "Epoch 15/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6777 - acc: 0.7310WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.7104 - acc: 0.7306\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.55909\n",
      "242/242 [==============================] - 27s 110ms/step - loss: 0.6783 - acc: 0.7309 - val_loss: 0.7104 - val_acc: 0.7306\n",
      "Epoch 16/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.6492 - acc: 0.7451WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.9111 - acc: 0.6500\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.55909\n",
      "242/242 [==============================] - 22s 92ms/step - loss: 0.6494 - acc: 0.7450 - val_loss: 0.9111 - val_acc: 0.6500\n",
      "Epoch 17/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.6465 - acc: 0.7520WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.7860 - acc: 0.6875\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.55909\n",
      "242/242 [==============================] - 24s 97ms/step - loss: 0.6466 - acc: 0.7514 - val_loss: 0.7860 - val_acc: 0.6875\n",
      "Epoch 18/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.6415 - acc: 0.7489WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.6881 - acc: 0.7519\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.55909\n",
      "242/242 [==============================] - 22s 89ms/step - loss: 0.6418 - acc: 0.7487 - val_loss: 0.6881 - val_acc: 0.7519\n",
      "Epoch 19/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6144 - acc: 0.7598WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.5468 - acc: 0.8225\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.55909 to 0.54682, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/019-0.5468.hdf5\n",
      "242/242 [==============================] - 23s 97ms/step - loss: 0.6144 - acc: 0.7599 - val_loss: 0.5468 - val_acc: 0.8225\n",
      "Epoch 20/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.7681WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.7293 - acc: 0.7403\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.54682\n",
      "242/242 [==============================] - 20s 84ms/step - loss: 0.6013 - acc: 0.7685 - val_loss: 0.7293 - val_acc: 0.7403\n",
      "Epoch 21/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.7692WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.6208 - acc: 0.7875\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.54682\n",
      "242/242 [==============================] - 29s 121ms/step - loss: 0.6017 - acc: 0.7699 - val_loss: 0.6208 - val_acc: 0.7875\n",
      "Epoch 22/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5800 - acc: 0.7751WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.8168 - acc: 0.7022\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.54682\n",
      "242/242 [==============================] - 23s 95ms/step - loss: 0.5794 - acc: 0.7752 - val_loss: 0.8168 - val_acc: 0.7022\n",
      "Epoch 23/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5737 - acc: 0.7809WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.6105 - acc: 0.7975\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.54682\n",
      "242/242 [==============================] - 27s 110ms/step - loss: 0.5734 - acc: 0.7810 - val_loss: 0.6105 - val_acc: 0.7975\n",
      "Epoch 24/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5477 - acc: 0.7910WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.5434 - acc: 0.8094\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.54682 to 0.54338, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/024-0.5434.hdf5\n",
      "242/242 [==============================] - 23s 96ms/step - loss: 0.5500 - acc: 0.7902 - val_loss: 0.5434 - val_acc: 0.8094\n",
      "Epoch 25/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.7879WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.7127 - acc: 0.7419\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.54338\n",
      "242/242 [==============================] - 24s 99ms/step - loss: 0.5421 - acc: 0.7879 - val_loss: 0.7127 - val_acc: 0.7419\n",
      "Epoch 26/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.7977WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.5030 - acc: 0.8484\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.54338 to 0.50301, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/026-0.5030.hdf5\n",
      "242/242 [==============================] - 25s 105ms/step - loss: 0.5270 - acc: 0.7977 - val_loss: 0.5030 - val_acc: 0.8484\n",
      "Epoch 27/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.7917WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5012 - acc: 0.8219\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.50301 to 0.50119, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/027-0.5012.hdf5\n",
      "242/242 [==============================] - 29s 119ms/step - loss: 0.5339 - acc: 0.7916 - val_loss: 0.5012 - val_acc: 0.8219\n",
      "Epoch 28/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5187 - acc: 0.8020WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.7275 - acc: 0.7181\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.50119\n",
      "242/242 [==============================] - 32s 132ms/step - loss: 0.5194 - acc: 0.8016 - val_loss: 0.7275 - val_acc: 0.7181\n",
      "Epoch 29/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.8060WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.6275 - acc: 0.7725\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.50119\n",
      "242/242 [==============================] - 28s 116ms/step - loss: 0.5021 - acc: 0.8061 - val_loss: 0.6275 - val_acc: 0.7725\n",
      "Epoch 30/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/242 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.8113WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.5500 - acc: 0.8025\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.50119\n",
      "242/242 [==============================] - 24s 100ms/step - loss: 0.5004 - acc: 0.8112 - val_loss: 0.5500 - val_acc: 0.8025\n",
      "Epoch 31/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.8025WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.5523 - acc: 0.8000\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.50119\n",
      "242/242 [==============================] - 34s 140ms/step - loss: 0.5153 - acc: 0.8024 - val_loss: 0.5523 - val_acc: 0.8000\n",
      "Epoch 32/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.8123WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5270 - acc: 0.8050\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.50119\n",
      "242/242 [==============================] - 24s 100ms/step - loss: 0.4836 - acc: 0.8124 - val_loss: 0.5270 - val_acc: 0.8050\n",
      "Epoch 33/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4764 - acc: 0.8180WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.5328 - acc: 0.8209\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.50119\n",
      "242/242 [==============================] - 33s 136ms/step - loss: 0.4753 - acc: 0.8186 - val_loss: 0.5328 - val_acc: 0.8209\n",
      "Epoch 34/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8211WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.4733 - acc: 0.8428\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.50119 to 0.47330, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/034-0.4733.hdf5\n",
      "242/242 [==============================] - 26s 106ms/step - loss: 0.4778 - acc: 0.8213 - val_loss: 0.4733 - val_acc: 0.8428\n",
      "Epoch 35/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4587 - acc: 0.8228WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5094 - acc: 0.8206\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47330\n",
      "242/242 [==============================] - 24s 100ms/step - loss: 0.4588 - acc: 0.8229 - val_loss: 0.5094 - val_acc: 0.8206\n",
      "Epoch 36/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4740 - acc: 0.8156WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4380 - acc: 0.8400 3s - loss: 0.4061\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.47330 to 0.43799, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/036-0.4380.hdf5\n",
      "242/242 [==============================] - 30s 124ms/step - loss: 0.4735 - acc: 0.8158 - val_loss: 0.4380 - val_acc: 0.8400\n",
      "Epoch 37/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4488 - acc: 0.8281WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5530 - acc: 0.8075\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.43799\n",
      "242/242 [==============================] - 32s 132ms/step - loss: 0.4495 - acc: 0.8276 - val_loss: 0.5530 - val_acc: 0.8075\n",
      "Epoch 38/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8315WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.9230 - acc: 0.6650\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.43799\n",
      "242/242 [==============================] - 27s 111ms/step - loss: 0.4471 - acc: 0.8320 - val_loss: 0.9230 - val_acc: 0.6650\n",
      "Epoch 39/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4410 - acc: 0.8319WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3945 - acc: 0.8634\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.43799 to 0.39453, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/039-0.3945.hdf5\n",
      "242/242 [==============================] - 28s 117ms/step - loss: 0.4414 - acc: 0.8317 - val_loss: 0.3945 - val_acc: 0.8634\n",
      "Epoch 40/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4441 - acc: 0.8306WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.4000 - acc: 0.8494\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.39453\n",
      "242/242 [==============================] - 23s 96ms/step - loss: 0.4437 - acc: 0.8307 - val_loss: 0.4000 - val_acc: 0.8494\n",
      "Epoch 41/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8361WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.4397 - acc: 0.8478\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.39453\n",
      "242/242 [==============================] - 39s 162ms/step - loss: 0.4228 - acc: 0.8356 - val_loss: 0.4397 - val_acc: 0.8478\n",
      "Epoch 42/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4299 - acc: 0.8378WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.6846 - acc: 0.7375s may\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.39453\n",
      "242/242 [==============================] - 36s 147ms/step - loss: 0.4299 - acc: 0.8377 - val_loss: 0.6846 - val_acc: 0.7375\n",
      "Epoch 43/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4225 - acc: 0.8405WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5091 - acc: 0.7981\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.39453\n",
      "242/242 [==============================] - 23s 95ms/step - loss: 0.4222 - acc: 0.8408 - val_loss: 0.5091 - val_acc: 0.7981\n",
      "Epoch 44/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8449WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.5586 - acc: 0.7934\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.39453\n",
      "242/242 [==============================] - 36s 150ms/step - loss: 0.4070 - acc: 0.8451 - val_loss: 0.5586 - val_acc: 0.7934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4102 - acc: 0.8426WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5506 - acc: 0.7875\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.39453\n",
      "242/242 [==============================] - 27s 112ms/step - loss: 0.4109 - acc: 0.8424 - val_loss: 0.5506 - val_acc: 0.7875\n",
      "Epoch 46/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.8494WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.3548 - acc: 0.8675\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.39453 to 0.35477, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/046-0.3548.hdf5\n",
      "242/242 [==============================] - 36s 147ms/step - loss: 0.3913 - acc: 0.8492 - val_loss: 0.3548 - val_acc: 0.8675\n",
      "Epoch 47/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8467WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.4668 - acc: 0.8250\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.35477\n",
      "242/242 [==============================] - 27s 110ms/step - loss: 0.3913 - acc: 0.8470 - val_loss: 0.4668 - val_acc: 0.8250\n",
      "Epoch 48/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8586WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4410 - acc: 0.8325\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.35477\n",
      "242/242 [==============================] - 25s 104ms/step - loss: 0.3718 - acc: 0.8583 - val_loss: 0.4410 - val_acc: 0.8325\n",
      "Epoch 49/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3841 - acc: 0.8580WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.4190 - acc: 0.8475\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.35477\n",
      "242/242 [==============================] - 38s 159ms/step - loss: 0.3844 - acc: 0.8579 - val_loss: 0.4190 - val_acc: 0.8475\n",
      "Epoch 50/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8601WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4048 - acc: 0.8628\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.35477\n",
      "242/242 [==============================] - 29s 120ms/step - loss: 0.3700 - acc: 0.8601 - val_loss: 0.4048 - val_acc: 0.8628\n",
      "Epoch 51/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3752 - acc: 0.8577WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.3271 - acc: 0.9000\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.35477 to 0.32712, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/051-0.3271.hdf5\n",
      "242/242 [==============================] - 34s 141ms/step - loss: 0.3754 - acc: 0.8576 - val_loss: 0.3271 - val_acc: 0.9000\n",
      "Epoch 52/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3688 - acc: 0.8613WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.4635 - acc: 0.8450\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.32712\n",
      "242/242 [==============================] - 29s 120ms/step - loss: 0.3684 - acc: 0.8614 - val_loss: 0.4635 - val_acc: 0.8450\n",
      "Epoch 53/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.8612WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.3108 - acc: 0.8900\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.32712 to 0.31076, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/053-0.3108.hdf5\n",
      "242/242 [==============================] - 37s 152ms/step - loss: 0.3704 - acc: 0.8614 - val_loss: 0.3108 - val_acc: 0.8900\n",
      "Epoch 54/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8655WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.5125 - acc: 0.8050\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 35s 145ms/step - loss: 0.3523 - acc: 0.8652 - val_loss: 0.5125 - val_acc: 0.8050\n",
      "Epoch 55/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8640WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.4290 - acc: 0.8475\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 37s 155ms/step - loss: 0.3497 - acc: 0.8642 - val_loss: 0.4290 - val_acc: 0.8475\n",
      "Epoch 56/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3424 - acc: 0.8681WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.4137 - acc: 0.8350\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 36s 149ms/step - loss: 0.3420 - acc: 0.8684 - val_loss: 0.4137 - val_acc: 0.8350\n",
      "Epoch 57/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3468 - acc: 0.8711WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4601 - acc: 0.8425\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 26s 108ms/step - loss: 0.3481 - acc: 0.8708 - val_loss: 0.4601 - val_acc: 0.8425\n",
      "Epoch 58/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3301 - acc: 0.8719WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.5404 - acc: 0.7994\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 33s 136ms/step - loss: 0.3305 - acc: 0.8717 - val_loss: 0.5404 - val_acc: 0.7994\n",
      "Epoch 59/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8774WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.4428 - acc: 0.8150\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 36s 151ms/step - loss: 0.3336 - acc: 0.8777 - val_loss: 0.4428 - val_acc: 0.8150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8704WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.5479 - acc: 0.7825\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 37s 154ms/step - loss: 0.3346 - acc: 0.8707 - val_loss: 0.5479 - val_acc: 0.7825\n",
      "Epoch 61/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.8795WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.4301 - acc: 0.8356\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 30s 123ms/step - loss: 0.3182 - acc: 0.8793 - val_loss: 0.4301 - val_acc: 0.8356\n",
      "Epoch 62/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3198 - acc: 0.8834WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5045 - acc: 0.7900\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 29s 122ms/step - loss: 0.3195 - acc: 0.8834 - val_loss: 0.5045 - val_acc: 0.7900\n",
      "Epoch 63/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.8744WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.3787 - acc: 0.8781\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 34s 142ms/step - loss: 0.3253 - acc: 0.8747 - val_loss: 0.3787 - val_acc: 0.8781\n",
      "Epoch 64/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3239 - acc: 0.8792WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 5ms/sample - loss: 0.3359 - acc: 0.8800\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 35s 144ms/step - loss: 0.3235 - acc: 0.8792 - val_loss: 0.3359 - val_acc: 0.8800\n",
      "Epoch 65/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8790WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4417 - acc: 0.8375\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 28s 117ms/step - loss: 0.3197 - acc: 0.8791 - val_loss: 0.4417 - val_acc: 0.8375\n",
      "Epoch 66/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.8798WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.3553 - acc: 0.8750\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.31076\n",
      "242/242 [==============================] - 36s 150ms/step - loss: 0.3055 - acc: 0.8802 - val_loss: 0.3553 - val_acc: 0.8750\n",
      "Epoch 67/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.8875WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.2751 - acc: 0.8925\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.31076 to 0.27506, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/067-0.2751.hdf5\n",
      "242/242 [==============================] - 33s 136ms/step - loss: 0.3041 - acc: 0.8876 - val_loss: 0.2751 - val_acc: 0.8925\n",
      "Epoch 68/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.8860WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4111 - acc: 0.8456\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 24s 101ms/step - loss: 0.3009 - acc: 0.8860 - val_loss: 0.4111 - val_acc: 0.8456\n",
      "Epoch 69/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.8907WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.7192 - acc: 0.7400\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 30s 123ms/step - loss: 0.2892 - acc: 0.8906 - val_loss: 0.7192 - val_acc: 0.7400\n",
      "Epoch 70/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.8854WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4792 - acc: 0.8325\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 29s 118ms/step - loss: 0.3062 - acc: 0.8856 - val_loss: 0.4792 - val_acc: 0.8325\n",
      "Epoch 71/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.8924WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2751 - acc: 0.9175\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 32s 132ms/step - loss: 0.2890 - acc: 0.8924 - val_loss: 0.2751 - val_acc: 0.9175\n",
      "Epoch 72/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.8907WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3694 - acc: 0.8669\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 40s 166ms/step - loss: 0.2834 - acc: 0.8904 - val_loss: 0.3694 - val_acc: 0.8669\n",
      "Epoch 73/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.8881WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3856 - acc: 0.8500\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 27s 114ms/step - loss: 0.3035 - acc: 0.8876 - val_loss: 0.3856 - val_acc: 0.8500\n",
      "Epoch 74/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.8987WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3784 - acc: 0.8525\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 38s 156ms/step - loss: 0.2718 - acc: 0.8987 - val_loss: 0.3784 - val_acc: 0.8525\n",
      "Epoch 75/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.8951WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4254 - acc: 0.8300\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 26s 109ms/step - loss: 0.2836 - acc: 0.8949 - val_loss: 0.4254 - val_acc: 0.8300\n",
      "Epoch 76/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.8920WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.3564 - acc: 0.8650\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 35s 146ms/step - loss: 0.2764 - acc: 0.8922 - val_loss: 0.3564 - val_acc: 0.8650\n",
      "Epoch 77/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.8972WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.3079 - acc: 0.8913\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 26s 109ms/step - loss: 0.2833 - acc: 0.8970 - val_loss: 0.3079 - val_acc: 0.8913\n",
      "Epoch 78/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.8983WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.4163 - acc: 0.8500\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 27s 112ms/step - loss: 0.2756 - acc: 0.8982 - val_loss: 0.4163 - val_acc: 0.8500\n",
      "Epoch 79/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.8994WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.6092 - acc: 0.7750\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 27s 112ms/step - loss: 0.2705 - acc: 0.8993 - val_loss: 0.6092 - val_acc: 0.7750\n",
      "Epoch 80/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.8972WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.4894 - acc: 0.8178\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.27506\n",
      "242/242 [==============================] - 29s 120ms/step - loss: 0.2685 - acc: 0.8969 - val_loss: 0.4894 - val_acc: 0.8178\n",
      "Epoch 81/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9045WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.2702 - acc: 0.9100\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.27506 to 0.27017, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/081-0.2702.hdf5\n",
      "242/242 [==============================] - 33s 136ms/step - loss: 0.2527 - acc: 0.9040 - val_loss: 0.2702 - val_acc: 0.9100\n",
      "Epoch 82/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9027WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3485 - acc: 0.8850\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.27017\n",
      "242/242 [==============================] - 37s 153ms/step - loss: 0.2641 - acc: 0.9024 - val_loss: 0.3485 - val_acc: 0.8850\n",
      "Epoch 83/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.8995WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.4203 - acc: 0.8550\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.27017\n",
      "242/242 [==============================] - 34s 141ms/step - loss: 0.2607 - acc: 0.8996 - val_loss: 0.4203 - val_acc: 0.8550\n",
      "Epoch 84/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9091WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.4572 - acc: 0.8431\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.27017\n",
      "242/242 [==============================] - 30s 125ms/step - loss: 0.2517 - acc: 0.9092 - val_loss: 0.4572 - val_acc: 0.8431\n",
      "Epoch 85/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9029WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4047 - acc: 0.8575\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.27017\n",
      "242/242 [==============================] - 31s 127ms/step - loss: 0.2545 - acc: 0.9029 - val_loss: 0.4047 - val_acc: 0.8575\n",
      "Epoch 86/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9080WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3764 - acc: 0.8625\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.27017\n",
      "242/242 [==============================] - 36s 147ms/step - loss: 0.2525 - acc: 0.9077 - val_loss: 0.3764 - val_acc: 0.8625\n",
      "Epoch 87/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9106WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2515 - acc: 0.9087\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.27017 to 0.25149, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/087-0.2515.hdf5\n",
      "242/242 [==============================] - 32s 133ms/step - loss: 0.2426 - acc: 0.9108 - val_loss: 0.2515 - val_acc: 0.9087\n",
      "Epoch 88/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9083WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.4662 - acc: 0.8184\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 29s 122ms/step - loss: 0.2513 - acc: 0.9083 - val_loss: 0.4662 - val_acc: 0.8184\n",
      "Epoch 89/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9082WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.4602 - acc: 0.8269\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 33s 135ms/step - loss: 0.2478 - acc: 0.9088 - val_loss: 0.4602 - val_acc: 0.8269\n",
      "Epoch 90/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.9116WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3560 - acc: 0.8706\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 39s 161ms/step - loss: 0.2450 - acc: 0.9114 - val_loss: 0.3560 - val_acc: 0.8706\n",
      "Epoch 91/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9114WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.4419 - acc: 0.8400\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 38s 158ms/step - loss: 0.2408 - acc: 0.9112 - val_loss: 0.4419 - val_acc: 0.8400\n",
      "Epoch 92/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2382 - acc: 0.9114WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 5ms/sample - loss: 0.2632 - acc: 0.9125\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 42s 174ms/step - loss: 0.2385 - acc: 0.9112 - val_loss: 0.2632 - val_acc: 0.9125\n",
      "Epoch 93/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9067WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.3869 - acc: 0.8600\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 27s 112ms/step - loss: 0.2532 - acc: 0.9067 - val_loss: 0.3869 - val_acc: 0.8600\n",
      "Epoch 94/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.9148WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.4598 - acc: 0.8281\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 41s 168ms/step - loss: 0.2412 - acc: 0.9150 - val_loss: 0.4598 - val_acc: 0.8281\n",
      "Epoch 95/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9140WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.4968 - acc: 0.8350\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 30s 123ms/step - loss: 0.2307 - acc: 0.9138 - val_loss: 0.4968 - val_acc: 0.8350\n",
      "Epoch 96/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9120WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3832 - acc: 0.8525\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 32s 133ms/step - loss: 0.2325 - acc: 0.9122 - val_loss: 0.3832 - val_acc: 0.8525\n",
      "Epoch 97/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9127WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3974 - acc: 0.8475\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 36s 150ms/step - loss: 0.2389 - acc: 0.9125 - val_loss: 0.3974 - val_acc: 0.8475\n",
      "Epoch 98/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9171WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2796 - acc: 0.8919\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 33s 136ms/step - loss: 0.2290 - acc: 0.9171 - val_loss: 0.2796 - val_acc: 0.8919\n",
      "Epoch 99/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9178WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3279 - acc: 0.8634\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 39s 160ms/step - loss: 0.2270 - acc: 0.9176 - val_loss: 0.3279 - val_acc: 0.8634\n",
      "Epoch 100/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9166WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.4684 - acc: 0.8144\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 29s 118ms/step - loss: 0.2221 - acc: 0.9163 - val_loss: 0.4684 - val_acc: 0.8144\n",
      "Epoch 101/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9175WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.3795 - acc: 0.8556\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 31s 126ms/step - loss: 0.2315 - acc: 0.9175 - val_loss: 0.3795 - val_acc: 0.8556\n",
      "Epoch 102/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9156WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.2712 - acc: 0.8981\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 33s 135ms/step - loss: 0.2253 - acc: 0.9157 - val_loss: 0.2712 - val_acc: 0.8981\n",
      "Epoch 103/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9183WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.4276 - acc: 0.8313\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 25s 102ms/step - loss: 0.2258 - acc: 0.9182 - val_loss: 0.4276 - val_acc: 0.8313\n",
      "Epoch 104/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9213WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2792 - acc: 0.8838\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.25149\n",
      "242/242 [==============================] - 38s 155ms/step - loss: 0.2109 - acc: 0.9214 - val_loss: 0.2792 - val_acc: 0.8838\n",
      "Epoch 105/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9231WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2443 - acc: 0.9050\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.25149 to 0.24425, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/105-0.2443.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 37s 155ms/step - loss: 0.2163 - acc: 0.9233 - val_loss: 0.2443 - val_acc: 0.9050\n",
      "Epoch 106/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9222WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4534 - acc: 0.8206\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.24425\n",
      "242/242 [==============================] - 26s 106ms/step - loss: 0.2124 - acc: 0.9223 - val_loss: 0.4534 - val_acc: 0.8206\n",
      "Epoch 107/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9224WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2572 - acc: 0.9150\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.24425\n",
      "242/242 [==============================] - 30s 122ms/step - loss: 0.2071 - acc: 0.9223 - val_loss: 0.2572 - val_acc: 0.9150\n",
      "Epoch 108/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9247WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3866 - acc: 0.8547\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.24425\n",
      "242/242 [==============================] - 26s 108ms/step - loss: 0.2056 - acc: 0.9246 - val_loss: 0.3866 - val_acc: 0.8547\n",
      "Epoch 109/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9246WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3871 - acc: 0.8531\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.24425\n",
      "242/242 [==============================] - 33s 138ms/step - loss: 0.2089 - acc: 0.9247 - val_loss: 0.3871 - val_acc: 0.8531\n",
      "Epoch 110/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9254WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3371 - acc: 0.8931\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.24425\n",
      "242/242 [==============================] - 29s 119ms/step - loss: 0.1998 - acc: 0.9255 - val_loss: 0.3371 - val_acc: 0.8931\n",
      "Epoch 111/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9251WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3427 - acc: 0.8578\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.24425\n",
      "242/242 [==============================] - 35s 144ms/step - loss: 0.2047 - acc: 0.9249 - val_loss: 0.3427 - val_acc: 0.8578\n",
      "Epoch 112/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9268WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.5526 - acc: 0.7900\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.24425\n",
      "242/242 [==============================] - 31s 127ms/step - loss: 0.2045 - acc: 0.9264 - val_loss: 0.5526 - val_acc: 0.7900\n",
      "Epoch 113/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9178WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.4123 - acc: 0.8450\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.24425\n",
      "242/242 [==============================] - 33s 135ms/step - loss: 0.2193 - acc: 0.9180 - val_loss: 0.4123 - val_acc: 0.8450\n",
      "Epoch 114/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9264WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3068 - acc: 0.8737\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.24425\n",
      "242/242 [==============================] - 31s 128ms/step - loss: 0.2014 - acc: 0.9265 - val_loss: 0.3068 - val_acc: 0.8737\n",
      "Epoch 115/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9292WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3096 - acc: 0.8859\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.24425\n",
      "242/242 [==============================] - 30s 124ms/step - loss: 0.1901 - acc: 0.9295 - val_loss: 0.3096 - val_acc: 0.8859\n",
      "Epoch 116/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9243WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2993 - acc: 0.8925\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.24425\n",
      "242/242 [==============================] - 40s 166ms/step - loss: 0.1951 - acc: 0.9243 - val_loss: 0.2993 - val_acc: 0.8925\n",
      "Epoch 117/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9228WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.2025 - acc: 0.9300\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.24425 to 0.20251, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/117-0.2025.hdf5\n",
      "242/242 [==============================] - 27s 113ms/step - loss: 0.2081 - acc: 0.9226 - val_loss: 0.2025 - val_acc: 0.9300\n",
      "Epoch 118/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9266WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4527 - acc: 0.8409\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 37s 151ms/step - loss: 0.1990 - acc: 0.9263 - val_loss: 0.4527 - val_acc: 0.8409\n",
      "Epoch 119/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9302WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2702 - acc: 0.8825\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 33s 137ms/step - loss: 0.1924 - acc: 0.9304 - val_loss: 0.2702 - val_acc: 0.8825\n",
      "Epoch 120/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9297WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3047 - acc: 0.8950\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 38s 155ms/step - loss: 0.1881 - acc: 0.9292 - val_loss: 0.3047 - val_acc: 0.8950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9341- ETA: 2s -WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.4049 - acc: 0.8600\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 30s 125ms/step - loss: 0.1843 - acc: 0.9340 - val_loss: 0.4049 - val_acc: 0.8600\n",
      "Epoch 122/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9329WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.3209 - acc: 0.8841\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 35s 146ms/step - loss: 0.1826 - acc: 0.9331 - val_loss: 0.3209 - val_acc: 0.8841\n",
      "Epoch 123/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9289WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3392 - acc: 0.8725\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 37s 152ms/step - loss: 0.1954 - acc: 0.9291 - val_loss: 0.3392 - val_acc: 0.8725\n",
      "Epoch 124/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9347WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3640 - acc: 0.8753\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 30s 123ms/step - loss: 0.1793 - acc: 0.9343 - val_loss: 0.3640 - val_acc: 0.8753\n",
      "Epoch 125/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9289WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3379 - acc: 0.8675\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 25s 103ms/step - loss: 0.1933 - acc: 0.9292 - val_loss: 0.3379 - val_acc: 0.8675\n",
      "Epoch 126/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9341WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.4260 - acc: 0.8375\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 30s 125ms/step - loss: 0.1806 - acc: 0.9341 - val_loss: 0.4260 - val_acc: 0.8375\n",
      "Epoch 127/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9359WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2359 - acc: 0.9200\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 29s 119ms/step - loss: 0.1828 - acc: 0.9358 - val_loss: 0.2359 - val_acc: 0.9200\n",
      "Epoch 128/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9317WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.2405 - acc: 0.9175\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 43s 178ms/step - loss: 0.1876 - acc: 0.9317 - val_loss: 0.2405 - val_acc: 0.9175\n",
      "Epoch 129/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9331WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3277 - acc: 0.8850\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 32s 132ms/step - loss: 0.1826 - acc: 0.9331 - val_loss: 0.3277 - val_acc: 0.8850\n",
      "Epoch 130/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9381WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.2537 - acc: 0.9125\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 33s 138ms/step - loss: 0.1714 - acc: 0.9383 - val_loss: 0.2537 - val_acc: 0.9125\n",
      "Epoch 131/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9374WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2390 - acc: 0.9044\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 28s 114ms/step - loss: 0.1756 - acc: 0.9376 - val_loss: 0.2390 - val_acc: 0.9044\n",
      "Epoch 132/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9380WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2712 - acc: 0.8906\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 35s 145ms/step - loss: 0.1755 - acc: 0.9381 - val_loss: 0.2712 - val_acc: 0.8906\n",
      "Epoch 133/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9350WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 5ms/sample - loss: 0.3105 - acc: 0.8878\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 37s 154ms/step - loss: 0.1791 - acc: 0.9351 - val_loss: 0.3105 - val_acc: 0.8878\n",
      "Epoch 134/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9385WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2596 - acc: 0.9050\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 34s 140ms/step - loss: 0.1674 - acc: 0.9384 - val_loss: 0.2596 - val_acc: 0.9050\n",
      "Epoch 135/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9407WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3879 - acc: 0.8700\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 27s 113ms/step - loss: 0.1693 - acc: 0.9405 - val_loss: 0.3879 - val_acc: 0.8700\n",
      "Epoch 136/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9379WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3085 - acc: 0.8900\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 41s 170ms/step - loss: 0.1779 - acc: 0.9380 - val_loss: 0.3085 - val_acc: 0.8900\n",
      "Epoch 137/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9429WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2925 - acc: 0.8950\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 32s 132ms/step - loss: 0.1653 - acc: 0.9430 - val_loss: 0.2925 - val_acc: 0.8950\n",
      "Epoch 138/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9366WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.3588 - acc: 0.8697\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 46s 192ms/step - loss: 0.1747 - acc: 0.9366 - val_loss: 0.3588 - val_acc: 0.8697\n",
      "Epoch 139/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9395WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3130 - acc: 0.8956\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 29s 120ms/step - loss: 0.1669 - acc: 0.9396 - val_loss: 0.3130 - val_acc: 0.8956\n",
      "Epoch 140/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9391WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2377 - acc: 0.9166\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 33s 138ms/step - loss: 0.1743 - acc: 0.9392 - val_loss: 0.2377 - val_acc: 0.9166\n",
      "Epoch 141/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9350WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2714 - acc: 0.9050\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 49s 201ms/step - loss: 0.1756 - acc: 0.9349 - val_loss: 0.2714 - val_acc: 0.9050\n",
      "Epoch 142/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9415WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.3230 - acc: 0.8800\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 42s 172ms/step - loss: 0.1592 - acc: 0.9412 - val_loss: 0.3230 - val_acc: 0.8800\n",
      "Epoch 143/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9376WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2188 - acc: 0.9231\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 32s 132ms/step - loss: 0.1720 - acc: 0.9376 - val_loss: 0.2188 - val_acc: 0.9231\n",
      "Epoch 144/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9450WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2810 - acc: 0.8875\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 27s 111ms/step - loss: 0.1635 - acc: 0.9450 - val_loss: 0.2810 - val_acc: 0.8875\n",
      "Epoch 145/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9448WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3029 - acc: 0.8913\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 47s 193ms/step - loss: 0.1647 - acc: 0.9448 - val_loss: 0.3029 - val_acc: 0.8913\n",
      "Epoch 146/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9437WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2093 - acc: 0.9300\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 37s 154ms/step - loss: 0.1593 - acc: 0.9434 - val_loss: 0.2093 - val_acc: 0.9300\n",
      "Epoch 147/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9417WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3084 - acc: 0.8831\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 36s 149ms/step - loss: 0.1660 - acc: 0.9419 - val_loss: 0.3084 - val_acc: 0.8831\n",
      "Epoch 148/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9436WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3030 - acc: 0.8850\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 45s 186ms/step - loss: 0.1553 - acc: 0.9433 - val_loss: 0.3030 - val_acc: 0.8850\n",
      "Epoch 149/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9419WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2375 - acc: 0.9025\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.20251\n",
      "242/242 [==============================] - 42s 172ms/step - loss: 0.1615 - acc: 0.9412 - val_loss: 0.2375 - val_acc: 0.9025\n",
      "Epoch 150/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9377WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.1939 - acc: 0.9400\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.20251 to 0.19393, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/150-0.1939.hdf5\n",
      "242/242 [==============================] - 39s 163ms/step - loss: 0.1683 - acc: 0.9379 - val_loss: 0.1939 - val_acc: 0.9400\n",
      "Epoch 151/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9375WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2943 - acc: 0.8825\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.19393\n",
      "242/242 [==============================] - 44s 184ms/step - loss: 0.1713 - acc: 0.9375 - val_loss: 0.2943 - val_acc: 0.8825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9418WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2043 - acc: 0.9337\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.19393\n",
      "242/242 [==============================] - 37s 152ms/step - loss: 0.1608 - acc: 0.9415 - val_loss: 0.2043 - val_acc: 0.9337\n",
      "Epoch 153/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9481WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3836 - acc: 0.8750\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.19393\n",
      "242/242 [==============================] - 25s 105ms/step - loss: 0.1473 - acc: 0.9479 - val_loss: 0.3836 - val_acc: 0.8750\n",
      "Epoch 154/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9410WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2748 - acc: 0.8959\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.19393\n",
      "242/242 [==============================] - 41s 169ms/step - loss: 0.1544 - acc: 0.9410 - val_loss: 0.2748 - val_acc: 0.8959\n",
      "Epoch 155/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9403WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3214 - acc: 0.8728\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.19393\n",
      "242/242 [==============================] - 29s 122ms/step - loss: 0.1604 - acc: 0.9404 - val_loss: 0.3214 - val_acc: 0.8728\n",
      "Epoch 156/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9478WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.3175 - acc: 0.8778\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.19393\n",
      "242/242 [==============================] - 50s 205ms/step - loss: 0.1479 - acc: 0.9480 - val_loss: 0.3175 - val_acc: 0.8778\n",
      "Epoch 157/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9396WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.2987 - acc: 0.8753\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.19393\n",
      "242/242 [==============================] - 29s 122ms/step - loss: 0.1564 - acc: 0.9396 - val_loss: 0.2987 - val_acc: 0.8753\n",
      "Epoch 158/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9437WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1816 - acc: 0.9322\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.19393 to 0.18162, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/158-0.1816.hdf5\n",
      "242/242 [==============================] - 43s 178ms/step - loss: 0.1565 - acc: 0.9439 - val_loss: 0.1816 - val_acc: 0.9322\n",
      "Epoch 159/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9469WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2530 - acc: 0.8975\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 37s 151ms/step - loss: 0.1501 - acc: 0.9471 - val_loss: 0.2530 - val_acc: 0.8975\n",
      "Epoch 160/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9459WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.1831 - acc: 0.9300\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 38s 155ms/step - loss: 0.1520 - acc: 0.9460 - val_loss: 0.1831 - val_acc: 0.9300\n",
      "Epoch 161/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9468WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3656 - acc: 0.8750\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 25s 104ms/step - loss: 0.1495 - acc: 0.9468 - val_loss: 0.3656 - val_acc: 0.8750\n",
      "Epoch 162/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9486WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.2893 - acc: 0.8831\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 35s 144ms/step - loss: 0.1433 - acc: 0.9485 - val_loss: 0.2893 - val_acc: 0.8831\n",
      "Epoch 163/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9490WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3135 - acc: 0.9000\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 31s 128ms/step - loss: 0.1424 - acc: 0.9488 - val_loss: 0.3135 - val_acc: 0.9000\n",
      "Epoch 164/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9463WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3916 - acc: 0.8575\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 35s 144ms/step - loss: 0.1467 - acc: 0.9464 - val_loss: 0.3916 - val_acc: 0.8575\n",
      "Epoch 165/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9467WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2632 - acc: 0.91251s - loss: 0.2715 - acc:\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 37s 152ms/step - loss: 0.1525 - acc: 0.9465 - val_loss: 0.2632 - val_acc: 0.9125\n",
      "Epoch 166/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9449WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2914 - acc: 0.8959\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 38s 158ms/step - loss: 0.1498 - acc: 0.9450 - val_loss: 0.2914 - val_acc: 0.8959\n",
      "Epoch 167/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/242 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9495WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.2061 - acc: 0.9275\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 30s 125ms/step - loss: 0.1427 - acc: 0.9497 - val_loss: 0.2061 - val_acc: 0.9275\n",
      "Epoch 168/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9477WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.2629 - acc: 0.8938\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 30s 122ms/step - loss: 0.1470 - acc: 0.9475 - val_loss: 0.2629 - val_acc: 0.8938\n",
      "Epoch 169/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9487WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.3284 - acc: 0.8775\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 37s 152ms/step - loss: 0.1484 - acc: 0.9486 - val_loss: 0.3284 - val_acc: 0.8775\n",
      "Epoch 170/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9511WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2990 - acc: 0.9000\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 38s 158ms/step - loss: 0.1366 - acc: 0.9510 - val_loss: 0.2990 - val_acc: 0.9000\n",
      "Epoch 171/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9506WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.2975 - acc: 0.8928\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 36s 147ms/step - loss: 0.1368 - acc: 0.9507 - val_loss: 0.2975 - val_acc: 0.8928\n",
      "Epoch 172/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9459WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3805 - acc: 0.8600\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 40s 165ms/step - loss: 0.1494 - acc: 0.9462 - val_loss: 0.3805 - val_acc: 0.8600\n",
      "Epoch 173/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9516WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.2545 - acc: 0.9075\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 32s 133ms/step - loss: 0.1424 - acc: 0.9516 - val_loss: 0.2545 - val_acc: 0.9075\n",
      "Epoch 174/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.9497WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.2569 - acc: 0.9137\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 44s 181ms/step - loss: 0.1379 - acc: 0.9498 - val_loss: 0.2569 - val_acc: 0.9137\n",
      "Epoch 175/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9487WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 28s 9ms/sample - loss: 0.2726 - acc: 0.9025\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 55s 225ms/step - loss: 0.1469 - acc: 0.9486 - val_loss: 0.2726 - val_acc: 0.9025\n",
      "Epoch 176/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9532WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3124 - acc: 0.8750\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 32s 134ms/step - loss: 0.1377 - acc: 0.9529 - val_loss: 0.3124 - val_acc: 0.8750\n",
      "Epoch 177/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9514WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2245 - acc: 0.9150\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 34s 142ms/step - loss: 0.1353 - acc: 0.9513 - val_loss: 0.2245 - val_acc: 0.9150\n",
      "Epoch 178/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9509WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.3773 - acc: 0.8637\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 44s 181ms/step - loss: 0.1377 - acc: 0.9508 - val_loss: 0.3773 - val_acc: 0.8637\n",
      "Epoch 179/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9540WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2737 - acc: 0.8875\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 50s 205ms/step - loss: 0.1320 - acc: 0.9541 - val_loss: 0.2737 - val_acc: 0.8875\n",
      "Epoch 180/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9515WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.3053 - acc: 0.8881\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 28s 118ms/step - loss: 0.1366 - acc: 0.9515 - val_loss: 0.3053 - val_acc: 0.8881\n",
      "Epoch 181/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9541WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.2566 - acc: 0.9066\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 39s 162ms/step - loss: 0.1347 - acc: 0.9539 - val_loss: 0.2566 - val_acc: 0.9066\n",
      "Epoch 182/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9527WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.3213 - acc: 0.8950\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 37s 151ms/step - loss: 0.1329 - acc: 0.9529 - val_loss: 0.3213 - val_acc: 0.8950\n",
      "Epoch 183/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9538WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1990 - acc: 0.9256\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 46s 188ms/step - loss: 0.1258 - acc: 0.9541 - val_loss: 0.1990 - val_acc: 0.9256\n",
      "Epoch 184/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9512WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2094 - acc: 0.9237\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 34s 141ms/step - loss: 0.1369 - acc: 0.9514 - val_loss: 0.2094 - val_acc: 0.9237\n",
      "Epoch 185/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9499WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2758 - acc: 0.9075\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 24s 98ms/step - loss: 0.1383 - acc: 0.9498 - val_loss: 0.2758 - val_acc: 0.9075\n",
      "Epoch 186/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9509WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3522 - acc: 0.8662\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 41s 171ms/step - loss: 0.1388 - acc: 0.9508 - val_loss: 0.3522 - val_acc: 0.8662\n",
      "Epoch 187/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9547WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2178 - acc: 0.9175\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 51s 209ms/step - loss: 0.1264 - acc: 0.9548 - val_loss: 0.2178 - val_acc: 0.9175\n",
      "Epoch 188/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9551WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.3230 - acc: 0.8850\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.18162\n",
      "242/242 [==============================] - 48s 197ms/step - loss: 0.1271 - acc: 0.9551 - val_loss: 0.3230 - val_acc: 0.8850\n",
      "Epoch 189/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9576WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.1758 - acc: 0.9322\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.18162 to 0.17580, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv_checkpoint/189-0.1758.hdf5\n",
      "242/242 [==============================] - 60s 249ms/step - loss: 0.1262 - acc: 0.9578 - val_loss: 0.1758 - val_acc: 0.9322\n",
      "Epoch 190/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9523WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.3437 - acc: 0.8825\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 33s 137ms/step - loss: 0.1307 - acc: 0.9524 - val_loss: 0.3437 - val_acc: 0.8825\n",
      "Epoch 191/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9556WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 27s 9ms/sample - loss: 0.2693 - acc: 0.9075\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 57s 235ms/step - loss: 0.1281 - acc: 0.9555 - val_loss: 0.2693 - val_acc: 0.9075\n",
      "Epoch 192/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9611WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1954 - acc: 0.9425\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 48s 197ms/step - loss: 0.1245 - acc: 0.9611 - val_loss: 0.1954 - val_acc: 0.9425\n",
      "Epoch 193/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9594WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2013 - acc: 0.9262\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 46s 189ms/step - loss: 0.1154 - acc: 0.9593 - val_loss: 0.2013 - val_acc: 0.9262\n",
      "Epoch 194/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9541WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.2866 - acc: 0.8809\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 38s 155ms/step - loss: 0.1258 - acc: 0.9544 - val_loss: 0.2866 - val_acc: 0.8809\n",
      "Epoch 195/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9558WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4052 - acc: 0.8644\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 37s 152ms/step - loss: 0.1252 - acc: 0.9557 - val_loss: 0.4052 - val_acc: 0.8644\n",
      "Epoch 196/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9555WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2247 - acc: 0.9150\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 55s 225ms/step - loss: 0.1239 - acc: 0.9556 - val_loss: 0.2247 - val_acc: 0.9150\n",
      "Epoch 197/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9526WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2477 - acc: 0.9031\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 39s 163ms/step - loss: 0.1325 - acc: 0.9525 - val_loss: 0.2477 - val_acc: 0.9031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9567WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.2496 - acc: 0.8950\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 52s 216ms/step - loss: 0.1155 - acc: 0.9567 - val_loss: 0.2496 - val_acc: 0.8950\n",
      "Epoch 199/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9532WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 29s 9ms/sample - loss: 0.2386 - acc: 0.8997\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 57s 238ms/step - loss: 0.1286 - acc: 0.9530 - val_loss: 0.2386 - val_acc: 0.8997\n",
      "Epoch 200/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9574WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.1831 - acc: 0.9350\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 60s 246ms/step - loss: 0.1199 - acc: 0.9576 - val_loss: 0.1831 - val_acc: 0.9350\n",
      "Epoch 201/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9580WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2470 - acc: 0.9200\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 49s 203ms/step - loss: 0.1172 - acc: 0.9578 - val_loss: 0.2470 - val_acc: 0.9200\n",
      "Epoch 202/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9573WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.4283 - acc: 0.8350\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 42s 173ms/step - loss: 0.1174 - acc: 0.9573 - val_loss: 0.4283 - val_acc: 0.8350\n",
      "Epoch 203/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9610WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3086 - acc: 0.8900\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 47s 193ms/step - loss: 0.1106 - acc: 0.9610 - val_loss: 0.3086 - val_acc: 0.8900\n",
      "Epoch 204/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9568WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.1824 - acc: 0.93946s - loss: 0\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 46s 188ms/step - loss: 0.1173 - acc: 0.9570 - val_loss: 0.1824 - val_acc: 0.9394\n",
      "Epoch 205/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9546WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.2130 - acc: 0.9219\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 46s 188ms/step - loss: 0.1177 - acc: 0.9549 - val_loss: 0.2130 - val_acc: 0.9219\n",
      "Epoch 206/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9550WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3747 - acc: 0.8525\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 34s 142ms/step - loss: 0.1238 - acc: 0.9550 - val_loss: 0.3747 - val_acc: 0.8525\n",
      "Epoch 207/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9617WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.3364 - acc: 0.8825\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 56s 232ms/step - loss: 0.1166 - acc: 0.9617 - val_loss: 0.3364 - val_acc: 0.8825\n",
      "Epoch 208/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9581WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.2884 - acc: 0.9025\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 45s 186ms/step - loss: 0.1185 - acc: 0.9582 - val_loss: 0.2884 - val_acc: 0.9025\n",
      "Epoch 209/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9590WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2358 - acc: 0.9200\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 47s 195ms/step - loss: 0.1135 - acc: 0.9588 - val_loss: 0.2358 - val_acc: 0.9200\n",
      "Epoch 210/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9550WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.3588 - acc: 0.8703\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 48s 197ms/step - loss: 0.1269 - acc: 0.9547 - val_loss: 0.3588 - val_acc: 0.8703\n",
      "Epoch 211/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9569WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.3260 - acc: 0.8894\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 53s 218ms/step - loss: 0.1191 - acc: 0.9571 - val_loss: 0.3260 - val_acc: 0.8894\n",
      "Epoch 212/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9595WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2819 - acc: 0.9144\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 49s 203ms/step - loss: 0.1117 - acc: 0.9594 - val_loss: 0.2819 - val_acc: 0.9144\n",
      "Epoch 213/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9596WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.1863 - acc: 0.9375\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 67s 278ms/step - loss: 0.1178 - acc: 0.9594 - val_loss: 0.1863 - val_acc: 0.9375\n",
      "Epoch 214/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9595WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3423 - acc: 0.8763\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 34s 140ms/step - loss: 0.1138 - acc: 0.9595 - val_loss: 0.3423 - val_acc: 0.8763\n",
      "Epoch 215/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9578WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.3761 - acc: 0.8863\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 48s 200ms/step - loss: 0.1132 - acc: 0.9579 - val_loss: 0.3761 - val_acc: 0.8863\n",
      "Epoch 216/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9609WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3656 - acc: 0.8694\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 50s 205ms/step - loss: 0.1131 - acc: 0.9609 - val_loss: 0.3656 - val_acc: 0.8694\n",
      "Epoch 217/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9591WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3816 - acc: 0.8491\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 45s 185ms/step - loss: 0.1137 - acc: 0.9591 - val_loss: 0.3816 - val_acc: 0.8491\n",
      "Epoch 218/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9617WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.4818 - acc: 0.8313\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 44s 182ms/step - loss: 0.1035 - acc: 0.9618 - val_loss: 0.4818 - val_acc: 0.8313\n",
      "Epoch 219/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9552WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3201 - acc: 0.8850\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 41s 169ms/step - loss: 0.1233 - acc: 0.9553 - val_loss: 0.3201 - val_acc: 0.8850\n",
      "Epoch 220/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9590WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3175 - acc: 0.8891\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 41s 169ms/step - loss: 0.1128 - acc: 0.9590 - val_loss: 0.3175 - val_acc: 0.8891\n",
      "Epoch 221/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9604WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.3021 - acc: 0.8984\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 47s 196ms/step - loss: 0.1140 - acc: 0.9603 - val_loss: 0.3021 - val_acc: 0.8984\n",
      "Epoch 222/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9603WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2641 - acc: 0.9112\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 35s 144ms/step - loss: 0.1038 - acc: 0.9603 - val_loss: 0.2641 - val_acc: 0.9112\n",
      "Epoch 223/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9578WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2783 - acc: 0.9050\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 25s 105ms/step - loss: 0.1180 - acc: 0.9579 - val_loss: 0.2783 - val_acc: 0.9050\n",
      "Epoch 224/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9601WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.2526 - acc: 0.9041\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 25s 105ms/step - loss: 0.1131 - acc: 0.9602 - val_loss: 0.2526 - val_acc: 0.9041\n",
      "Epoch 225/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9572WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.3414 - acc: 0.8691\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 31s 128ms/step - loss: 0.1160 - acc: 0.9573 - val_loss: 0.3414 - val_acc: 0.8691\n",
      "Epoch 226/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9534WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3166 - acc: 0.8863\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 23s 96ms/step - loss: 0.1237 - acc: 0.9537 - val_loss: 0.3166 - val_acc: 0.8863\n",
      "Epoch 227/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9624WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2316 - acc: 0.9087\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 31s 127ms/step - loss: 0.1091 - acc: 0.9623 - val_loss: 0.2316 - val_acc: 0.9087\n",
      "Epoch 228/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9636WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.2580 - acc: 0.9056\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 32s 133ms/step - loss: 0.1015 - acc: 0.9636 - val_loss: 0.2580 - val_acc: 0.9056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9616WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2625 - acc: 0.9125\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 41s 168ms/step - loss: 0.1070 - acc: 0.9617 - val_loss: 0.2625 - val_acc: 0.9125\n",
      "Epoch 230/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9611WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.2405 - acc: 0.9262\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 27s 112ms/step - loss: 0.1046 - acc: 0.9611 - val_loss: 0.2405 - val_acc: 0.9262\n",
      "Epoch 231/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9629WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.3843 - acc: 0.8900\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 40s 164ms/step - loss: 0.0989 - acc: 0.9630 - val_loss: 0.3843 - val_acc: 0.8900\n",
      "Epoch 232/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9599WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.2290 - acc: 0.9181\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 36s 147ms/step - loss: 0.1082 - acc: 0.9602 - val_loss: 0.2290 - val_acc: 0.9181\n",
      "Epoch 233/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9625WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.2784 - acc: 0.9150\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 31s 128ms/step - loss: 0.1062 - acc: 0.9624 - val_loss: 0.2784 - val_acc: 0.9150\n",
      "Epoch 234/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9626WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.4272 - acc: 0.8662\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 46s 190ms/step - loss: 0.1067 - acc: 0.9627 - val_loss: 0.4272 - val_acc: 0.8662\n",
      "Epoch 235/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9592WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2316 - acc: 0.9281\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 46s 189ms/step - loss: 0.1108 - acc: 0.9589 - val_loss: 0.2316 - val_acc: 0.9281\n",
      "Epoch 236/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9606WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.2544 - acc: 0.9200\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 50s 207ms/step - loss: 0.1096 - acc: 0.9607 - val_loss: 0.2544 - val_acc: 0.9200\n",
      "Epoch 237/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9640WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.1777 - acc: 0.9459\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 51s 211ms/step - loss: 0.1038 - acc: 0.9639 - val_loss: 0.1777 - val_acc: 0.9459\n",
      "Epoch 238/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9639WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2653 - acc: 0.9175\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 46s 188ms/step - loss: 0.1037 - acc: 0.9638 - val_loss: 0.2653 - val_acc: 0.9175\n",
      "Epoch 239/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9628WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.3809 - acc: 0.8666\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.17580\n",
      "242/242 [==============================] - 57s 236ms/step - loss: 0.1033 - acc: 0.9627 - val_loss: 0.3809 - val_acc: 0.8666\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3183 [==============================] - 3s 1ms/sample - loss: 0.2597 - acc: 0.9045\n",
      "Loss: 0.259689773843418 Accuracy: 0.9044926\n",
      "\n",
      "Epoch 1/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 1.5192 - acc: 0.3471WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 1.5979 - acc: 0.3091\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59791, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/001-1.5979.hdf5\n",
      "242/242 [==============================] - 28s 116ms/step - loss: 1.5178 - acc: 0.3479 - val_loss: 1.5979 - val_acc: 0.3091\n",
      "Epoch 2/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.2811 - acc: 0.4787WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 1.4420 - acc: 0.3875\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.59791 to 1.44195, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/002-1.4420.hdf5\n",
      "242/242 [==============================] - 57s 236ms/step - loss: 1.2807 - acc: 0.4786 - val_loss: 1.4420 - val_acc: 0.3875\n",
      "Epoch 3/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.1557 - acc: 0.5431WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 1.2492 - acc: 0.5038\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.44195 to 1.24916, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/003-1.2492.hdf5\n",
      "242/242 [==============================] - 37s 154ms/step - loss: 1.1561 - acc: 0.5427 - val_loss: 1.2492 - val_acc: 0.5038\n",
      "Epoch 4/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.0455 - acc: 0.5990WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 1.3295 - acc: 0.4575\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.24916\n",
      "242/242 [==============================] - 25s 103ms/step - loss: 1.0452 - acc: 0.5992 - val_loss: 1.3295 - val_acc: 0.4575\n",
      "Epoch 5/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.9620 - acc: 0.6333WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.9448 - acc: 0.6972\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.24916 to 0.94481, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/005-0.9448.hdf5\n",
      "242/242 [==============================] - 47s 196ms/step - loss: 0.9620 - acc: 0.6330 - val_loss: 0.9448 - val_acc: 0.6972\n",
      "Epoch 6/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.8821 - acc: 0.6589WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 1.0262 - acc: 0.5941\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.94481\n",
      "242/242 [==============================] - 28s 116ms/step - loss: 0.8825 - acc: 0.6584 - val_loss: 1.0262 - val_acc: 0.5941\n",
      "Epoch 7/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8379 - acc: 0.6782WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 1.5463 - acc: 0.3675\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.94481\n",
      "242/242 [==============================] - 37s 153ms/step - loss: 0.8377 - acc: 0.6784 - val_loss: 1.5463 - val_acc: 0.3675\n",
      "Epoch 8/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8007 - acc: 0.6906WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 1.0225 - acc: 0.5950\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.94481\n",
      "242/242 [==============================] - 27s 112ms/step - loss: 0.8006 - acc: 0.6908 - val_loss: 1.0225 - val_acc: 0.5950\n",
      "Epoch 9/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7584 - acc: 0.7104WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.7516 - acc: 0.7250\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.94481 to 0.75158, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/009-0.7516.hdf5\n",
      "242/242 [==============================] - 37s 155ms/step - loss: 0.7590 - acc: 0.7105 - val_loss: 0.7516 - val_acc: 0.7250\n",
      "Epoch 10/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7267 - acc: 0.7189WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.9009 - acc: 0.6572\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.75158\n",
      "242/242 [==============================] - 26s 109ms/step - loss: 0.7265 - acc: 0.7192 - val_loss: 0.9009 - val_acc: 0.6572\n",
      "Epoch 11/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.7156 - acc: 0.7205WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.7061 - acc: 0.7481\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.75158 to 0.70606, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/011-0.7061.hdf5\n",
      "242/242 [==============================] - 40s 166ms/step - loss: 0.7144 - acc: 0.7211 - val_loss: 0.7061 - val_acc: 0.7481\n",
      "Epoch 12/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.7301WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.7816 - acc: 0.7156\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.70606\n",
      "242/242 [==============================] - 29s 118ms/step - loss: 0.6928 - acc: 0.7300 - val_loss: 0.7816 - val_acc: 0.7156\n",
      "Epoch 13/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6625 - acc: 0.7446WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.9575 - acc: 0.6472\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.70606\n",
      "242/242 [==============================] - 50s 209ms/step - loss: 0.6627 - acc: 0.7448 - val_loss: 0.9575 - val_acc: 0.6472\n",
      "Epoch 14/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6456 - acc: 0.7487WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5057 - acc: 0.8234\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.70606 to 0.50569, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/014-0.5057.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 27s 112ms/step - loss: 0.6451 - acc: 0.7490 - val_loss: 0.5057 - val_acc: 0.8234\n",
      "Epoch 15/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6257 - acc: 0.7591WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.6472 - acc: 0.7681\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.50569\n",
      "242/242 [==============================] - 66s 273ms/step - loss: 0.6250 - acc: 0.7594 - val_loss: 0.6472 - val_acc: 0.7681\n",
      "Epoch 16/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5929 - acc: 0.7688WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.8179 - acc: 0.7078\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.50569\n",
      "242/242 [==============================] - 31s 130ms/step - loss: 0.5927 - acc: 0.7685 - val_loss: 0.8179 - val_acc: 0.7078\n",
      "Epoch 17/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5855 - acc: 0.7713WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.6359 - acc: 0.7797\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.50569\n",
      "242/242 [==============================] - 76s 314ms/step - loss: 0.5857 - acc: 0.7712 - val_loss: 0.6359 - val_acc: 0.7797\n",
      "Epoch 18/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.7789WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.7511 - acc: 0.7297\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.50569\n",
      "242/242 [==============================] - 26s 106ms/step - loss: 0.5739 - acc: 0.7788 - val_loss: 0.7511 - val_acc: 0.7297\n",
      "Epoch 19/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7914WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.7484 - acc: 0.7275\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.50569\n",
      "242/242 [==============================] - 28s 117ms/step - loss: 0.5498 - acc: 0.7914 - val_loss: 0.7484 - val_acc: 0.7275\n",
      "Epoch 20/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.7809WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.6998 - acc: 0.7375\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.50569\n",
      "242/242 [==============================] - 47s 193ms/step - loss: 0.5671 - acc: 0.7811 - val_loss: 0.6998 - val_acc: 0.7375\n",
      "Epoch 21/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.7885WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5246 - acc: 0.8025\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.50569\n",
      "242/242 [==============================] - 33s 135ms/step - loss: 0.5538 - acc: 0.7886 - val_loss: 0.5246 - val_acc: 0.8025\n",
      "Epoch 22/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8050WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5023 - acc: 0.8056\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.50569 to 0.50227, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/022-0.5023.hdf5\n",
      "242/242 [==============================] - 30s 124ms/step - loss: 0.5168 - acc: 0.8051 - val_loss: 0.5023 - val_acc: 0.8056\n",
      "Epoch 23/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.8096WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.7344 - acc: 0.7425\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.50227\n",
      "242/242 [==============================] - 29s 120ms/step - loss: 0.5015 - acc: 0.8105 - val_loss: 0.7344 - val_acc: 0.7425\n",
      "Epoch 24/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.5070 - acc: 0.8143WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5833 - acc: 0.7875\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.50227\n",
      "242/242 [==============================] - 32s 131ms/step - loss: 0.5066 - acc: 0.8146 - val_loss: 0.5833 - val_acc: 0.7875\n",
      "Epoch 25/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.8128WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 1.1022 - acc: 0.5925\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.50227\n",
      "242/242 [==============================] - 33s 135ms/step - loss: 0.4748 - acc: 0.8127 - val_loss: 1.1022 - val_acc: 0.5925\n",
      "Epoch 26/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.4787 - acc: 0.8163WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.6495 - acc: 0.7675\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.50227\n",
      "242/242 [==============================] - 27s 113ms/step - loss: 0.4779 - acc: 0.8166 - val_loss: 0.6495 - val_acc: 0.7675\n",
      "Epoch 27/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.8169WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5438 - acc: 0.7969\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.50227\n",
      "242/242 [==============================] - 38s 157ms/step - loss: 0.4780 - acc: 0.8172 - val_loss: 0.5438 - val_acc: 0.7969\n",
      "Epoch 28/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4624 - acc: 0.8247WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.5450 - acc: 0.7925\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.50227\n",
      "242/242 [==============================] - 32s 134ms/step - loss: 0.4636 - acc: 0.8238 - val_loss: 0.5450 - val_acc: 0.7925\n",
      "Epoch 29/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.8314WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.5088 - acc: 0.8175s may duplicate y\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.50227\n",
      "242/242 [==============================] - 54s 225ms/step - loss: 0.4431 - acc: 0.8315 - val_loss: 0.5088 - val_acc: 0.8175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4293 - acc: 0.8376WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5972 - acc: 0.7675\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.50227\n",
      "242/242 [==============================] - 63s 259ms/step - loss: 0.4295 - acc: 0.8376 - val_loss: 0.5972 - val_acc: 0.7675\n",
      "Epoch 31/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8387WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.6592 - acc: 0.7487\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.50227\n",
      "242/242 [==============================] - 41s 168ms/step - loss: 0.4126 - acc: 0.8387 - val_loss: 0.6592 - val_acc: 0.7487\n",
      "Epoch 32/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3989 - acc: 0.8436WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4243 - acc: 0.8506\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.50227 to 0.42425, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/032-0.4243.hdf5\n",
      "242/242 [==============================] - 55s 229ms/step - loss: 0.3994 - acc: 0.8432 - val_loss: 0.4243 - val_acc: 0.8506\n",
      "Epoch 33/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8434WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.4256 - acc: 0.8494\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.42425\n",
      "242/242 [==============================] - 42s 174ms/step - loss: 0.4051 - acc: 0.8435 - val_loss: 0.4256 - val_acc: 0.8494\n",
      "Epoch 34/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3941 - acc: 0.8473WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.5272 - acc: 0.8153\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.42425\n",
      "242/242 [==============================] - 28s 116ms/step - loss: 0.3941 - acc: 0.8471 - val_loss: 0.5272 - val_acc: 0.8153\n",
      "Epoch 35/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3890 - acc: 0.8497WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3590 - acc: 0.8788\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.42425 to 0.35904, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/035-0.3590.hdf5\n",
      "242/242 [==============================] - 40s 163ms/step - loss: 0.3891 - acc: 0.8494 - val_loss: 0.3590 - val_acc: 0.8788\n",
      "Epoch 36/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8465WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.6189 - acc: 0.7775\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.35904\n",
      "242/242 [==============================] - 34s 141ms/step - loss: 0.3912 - acc: 0.8471 - val_loss: 0.6189 - val_acc: 0.7775\n",
      "Epoch 37/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8621WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.5471 - acc: 0.8125\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.35904\n",
      "242/242 [==============================] - 38s 158ms/step - loss: 0.3682 - acc: 0.8622 - val_loss: 0.5471 - val_acc: 0.8125\n",
      "Epoch 38/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8622WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.4761 - acc: 0.82004s - loss: 0.4672 \n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.35904\n",
      "242/242 [==============================] - 38s 159ms/step - loss: 0.3557 - acc: 0.8622 - val_loss: 0.4761 - val_acc: 0.8200\n",
      "Epoch 39/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8606WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.6109 - acc: 0.7775\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.35904\n",
      "242/242 [==============================] - 26s 109ms/step - loss: 0.3563 - acc: 0.8606 - val_loss: 0.6109 - val_acc: 0.7775\n",
      "Epoch 40/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8675WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.6520 - acc: 0.7625\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.35904\n",
      "242/242 [==============================] - 61s 253ms/step - loss: 0.3390 - acc: 0.8677 - val_loss: 0.6520 - val_acc: 0.7625\n",
      "Epoch 41/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8688WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3249 - acc: 0.8850\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.35904 to 0.32486, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/041-0.3249.hdf5\n",
      "242/242 [==============================] - 49s 201ms/step - loss: 0.3382 - acc: 0.8689 - val_loss: 0.3249 - val_acc: 0.8850\n",
      "Epoch 42/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.8671WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.6157 - acc: 0.7875\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.32486\n",
      "242/242 [==============================] - 26s 109ms/step - loss: 0.3424 - acc: 0.8674 - val_loss: 0.6157 - val_acc: 0.7875\n",
      "Epoch 43/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8779WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2368 - acc: 0.9109\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.32486 to 0.23675, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/043-0.2368.hdf5\n",
      "242/242 [==============================] - 43s 177ms/step - loss: 0.3175 - acc: 0.8782 - val_loss: 0.2368 - val_acc: 0.9109\n",
      "Epoch 44/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3210 - acc: 0.8774WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.5603 - acc: 0.8125\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 37s 154ms/step - loss: 0.3204 - acc: 0.8773 - val_loss: 0.5603 - val_acc: 0.8125\n",
      "Epoch 45/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.8723WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.5304 - acc: 0.8200\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 37s 152ms/step - loss: 0.3237 - acc: 0.8725 - val_loss: 0.5304 - val_acc: 0.8200\n",
      "Epoch 46/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.8781WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.3664 - acc: 0.8675\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 34s 139ms/step - loss: 0.3088 - acc: 0.8780 - val_loss: 0.3664 - val_acc: 0.8675\n",
      "Epoch 47/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.8827WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.4289 - acc: 0.8525\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 29s 121ms/step - loss: 0.3049 - acc: 0.8825 - val_loss: 0.4289 - val_acc: 0.8525\n",
      "Epoch 48/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.8934WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3012 - acc: 0.8884\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 44s 181ms/step - loss: 0.2842 - acc: 0.8932 - val_loss: 0.3012 - val_acc: 0.8884\n",
      "Epoch 49/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.8833WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3369 - acc: 0.8750\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 26s 106ms/step - loss: 0.3011 - acc: 0.8835 - val_loss: 0.3369 - val_acc: 0.8750\n",
      "Epoch 50/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.8874WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3896 - acc: 0.8550\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 65s 268ms/step - loss: 0.2933 - acc: 0.8875 - val_loss: 0.3896 - val_acc: 0.8550\n",
      "Epoch 51/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.8890WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.7567 - acc: 0.7284\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 39s 161ms/step - loss: 0.2872 - acc: 0.8887 - val_loss: 0.7567 - val_acc: 0.7284\n",
      "Epoch 52/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.8971WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.5587 - acc: 0.8078\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 32s 134ms/step - loss: 0.2741 - acc: 0.8972 - val_loss: 0.5587 - val_acc: 0.8078\n",
      "Epoch 53/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.8847WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.4452 - acc: 0.8350\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 39s 162ms/step - loss: 0.2942 - acc: 0.8842 - val_loss: 0.4452 - val_acc: 0.8350\n",
      "Epoch 54/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9017WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.3778 - acc: 0.8494\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 55s 227ms/step - loss: 0.2600 - acc: 0.9020 - val_loss: 0.3778 - val_acc: 0.8494\n",
      "Epoch 55/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9007WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4084 - acc: 0.8525\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 52s 214ms/step - loss: 0.2718 - acc: 0.9007 - val_loss: 0.4084 - val_acc: 0.8525\n",
      "Epoch 56/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9012WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2938 - acc: 0.9000\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 47s 195ms/step - loss: 0.2655 - acc: 0.9013 - val_loss: 0.2938 - val_acc: 0.9000\n",
      "Epoch 57/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.8992WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3580 - acc: 0.8591\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 33s 135ms/step - loss: 0.2526 - acc: 0.8995 - val_loss: 0.3580 - val_acc: 0.8591\n",
      "Epoch 58/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9064WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3354 - acc: 0.8700\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 55s 228ms/step - loss: 0.2521 - acc: 0.9060 - val_loss: 0.3354 - val_acc: 0.8700\n",
      "Epoch 59/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.9084WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.6321 - acc: 0.7581\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 31s 127ms/step - loss: 0.2491 - acc: 0.9088 - val_loss: 0.6321 - val_acc: 0.7581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9033WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.3773 - acc: 0.8575\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 48s 200ms/step - loss: 0.2482 - acc: 0.9036 - val_loss: 0.3773 - val_acc: 0.8575\n",
      "Epoch 61/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9126WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.5190 - acc: 0.8075\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 34s 140ms/step - loss: 0.2332 - acc: 0.9127 - val_loss: 0.5190 - val_acc: 0.8075\n",
      "Epoch 62/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9060WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.4540 - acc: 0.8388\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 41s 170ms/step - loss: 0.2509 - acc: 0.9061 - val_loss: 0.4540 - val_acc: 0.8388\n",
      "Epoch 63/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9097WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.2534 - acc: 0.9081\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 46s 188ms/step - loss: 0.2408 - acc: 0.9097 - val_loss: 0.2534 - val_acc: 0.9081\n",
      "Epoch 64/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9126WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.4244 - acc: 0.8438\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 34s 139ms/step - loss: 0.2300 - acc: 0.9129 - val_loss: 0.4244 - val_acc: 0.8438\n",
      "Epoch 65/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9135WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3654 - acc: 0.8700\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 34s 142ms/step - loss: 0.2254 - acc: 0.9135 - val_loss: 0.3654 - val_acc: 0.8700\n",
      "Epoch 66/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9171WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3339 - acc: 0.8859\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 53s 219ms/step - loss: 0.2176 - acc: 0.9172 - val_loss: 0.3339 - val_acc: 0.8859\n",
      "Epoch 67/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9174WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.3073 - acc: 0.8975\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.23675\n",
      "242/242 [==============================] - 37s 154ms/step - loss: 0.2203 - acc: 0.9173 - val_loss: 0.3073 - val_acc: 0.8975\n",
      "Epoch 68/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9205WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2246 - acc: 0.9162 8s \n",
      "\n",
      "Epoch 00068: val_loss improved from 0.23675 to 0.22462, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/068-0.2246.hdf5\n",
      "242/242 [==============================] - 48s 196ms/step - loss: 0.2124 - acc: 0.9200 - val_loss: 0.2246 - val_acc: 0.9162\n",
      "Epoch 69/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9191WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.5330 - acc: 0.7975\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.22462\n",
      "242/242 [==============================] - 40s 164ms/step - loss: 0.2188 - acc: 0.9189 - val_loss: 0.5330 - val_acc: 0.7975\n",
      "Epoch 70/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9248WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3432 - acc: 0.8550\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.22462\n",
      "242/242 [==============================] - 57s 234ms/step - loss: 0.2028 - acc: 0.9249 - val_loss: 0.3432 - val_acc: 0.8550\n",
      "Epoch 71/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9220WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.6106 - acc: 0.7900\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.22462\n",
      "242/242 [==============================] - 27s 114ms/step - loss: 0.2104 - acc: 0.9217 - val_loss: 0.6106 - val_acc: 0.7900\n",
      "Epoch 72/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9198WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.3638 - acc: 0.8675\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.22462\n",
      "242/242 [==============================] - 31s 128ms/step - loss: 0.2096 - acc: 0.9200 - val_loss: 0.3638 - val_acc: 0.8675\n",
      "Epoch 73/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9254WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3145 - acc: 0.8900\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.22462\n",
      "242/242 [==============================] - 45s 188ms/step - loss: 0.1967 - acc: 0.9254 - val_loss: 0.3145 - val_acc: 0.8900\n",
      "Epoch 74/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9259WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.2570 - acc: 0.89001s - loss: 0.2580 - acc: 0\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.22462\n",
      "242/242 [==============================] - 33s 137ms/step - loss: 0.1979 - acc: 0.9256 - val_loss: 0.2570 - val_acc: 0.8900\n",
      "Epoch 75/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/242 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9328WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 7s 2ms/sample - loss: 0.2723 - acc: 0.8875\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.22462\n",
      "242/242 [==============================] - 25s 102ms/step - loss: 0.1854 - acc: 0.9325 - val_loss: 0.2723 - val_acc: 0.8875\n",
      "Epoch 76/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9305WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2108 - acc: 0.9194\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.22462 to 0.21082, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/076-0.2108.hdf5\n",
      "242/242 [==============================] - 27s 114ms/step - loss: 0.1855 - acc: 0.9304 - val_loss: 0.2108 - val_acc: 0.9194\n",
      "Epoch 77/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9274WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3068 - acc: 0.8850\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.21082\n",
      "242/242 [==============================] - 30s 126ms/step - loss: 0.1979 - acc: 0.9277 - val_loss: 0.3068 - val_acc: 0.8850\n",
      "Epoch 78/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9364WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4870 - acc: 0.8275\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.21082\n",
      "242/242 [==============================] - 37s 151ms/step - loss: 0.1788 - acc: 0.9365 - val_loss: 0.4870 - val_acc: 0.8275\n",
      "Epoch 79/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9326WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.4937 - acc: 0.8175\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.21082\n",
      "242/242 [==============================] - 56s 231ms/step - loss: 0.1877 - acc: 0.9322 - val_loss: 0.4937 - val_acc: 0.8175\n",
      "Epoch 80/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9373WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3768 - acc: 0.8600\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.21082\n",
      "242/242 [==============================] - 38s 156ms/step - loss: 0.1747 - acc: 0.9372 - val_loss: 0.3768 - val_acc: 0.8600\n",
      "Epoch 81/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9349WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2959 - acc: 0.9050\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.21082\n",
      "242/242 [==============================] - 31s 130ms/step - loss: 0.1791 - acc: 0.9349 - val_loss: 0.2959 - val_acc: 0.9050\n",
      "Epoch 82/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9371WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.2543 - acc: 0.9013\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.21082\n",
      "242/242 [==============================] - 58s 239ms/step - loss: 0.1757 - acc: 0.9371 - val_loss: 0.2543 - val_acc: 0.9013\n",
      "Epoch 83/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9358WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.1687 - acc: 0.9375\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.21082 to 0.16872, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/083-0.1687.hdf5\n",
      "242/242 [==============================] - 36s 150ms/step - loss: 0.1719 - acc: 0.9360 - val_loss: 0.1687 - val_acc: 0.9375\n",
      "Epoch 84/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9370WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.3613 - acc: 0.8537\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 41s 168ms/step - loss: 0.1724 - acc: 0.9369 - val_loss: 0.3613 - val_acc: 0.8537\n",
      "Epoch 85/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9367WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3601 - acc: 0.8788\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 22s 89ms/step - loss: 0.1753 - acc: 0.9366 - val_loss: 0.3601 - val_acc: 0.8788\n",
      "Epoch 86/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9415WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.4176 - acc: 0.8562\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 43s 176ms/step - loss: 0.1618 - acc: 0.9414 - val_loss: 0.4176 - val_acc: 0.8562\n",
      "Epoch 87/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9379WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2347 - acc: 0.9225\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 48s 196ms/step - loss: 0.1662 - acc: 0.9378 - val_loss: 0.2347 - val_acc: 0.9225\n",
      "Epoch 88/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9448WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.5628 - acc: 0.8041\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 38s 157ms/step - loss: 0.1577 - acc: 0.9449 - val_loss: 0.5628 - val_acc: 0.8041\n",
      "Epoch 89/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9401WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.1914 - acc: 0.9237\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 61s 254ms/step - loss: 0.1629 - acc: 0.9399 - val_loss: 0.1914 - val_acc: 0.9237\n",
      "Epoch 90/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/242 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9435WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3255 - acc: 0.8894\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 39s 160ms/step - loss: 0.1590 - acc: 0.9433 - val_loss: 0.3255 - val_acc: 0.8894\n",
      "Epoch 91/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9388WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2622 - acc: 0.9025\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 50s 206ms/step - loss: 0.1673 - acc: 0.9385 - val_loss: 0.2622 - val_acc: 0.9025\n",
      "Epoch 92/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9399WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2755 - acc: 0.9050\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 39s 162ms/step - loss: 0.1602 - acc: 0.9400 - val_loss: 0.2755 - val_acc: 0.9050\n",
      "Epoch 93/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9477WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.4823 - acc: 0.8050\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 40s 167ms/step - loss: 0.1467 - acc: 0.9478 - val_loss: 0.4823 - val_acc: 0.8050\n",
      "Epoch 94/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9480WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2938 - acc: 0.8750\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 39s 161ms/step - loss: 0.1509 - acc: 0.9479 - val_loss: 0.2938 - val_acc: 0.8750\n",
      "Epoch 95/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9430WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3882 - acc: 0.8284\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 37s 155ms/step - loss: 0.1612 - acc: 0.9429 - val_loss: 0.3882 - val_acc: 0.8284\n",
      "Epoch 96/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9486WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2758 - acc: 0.8950\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 46s 190ms/step - loss: 0.1430 - acc: 0.9486 - val_loss: 0.2758 - val_acc: 0.8950\n",
      "Epoch 97/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9494WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.4460 - acc: 0.8250\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 51s 213ms/step - loss: 0.1439 - acc: 0.9494 - val_loss: 0.4460 - val_acc: 0.8250\n",
      "Epoch 98/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9453WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 25s 8ms/sample - loss: 0.2451 - acc: 0.9025\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 52s 215ms/step - loss: 0.1476 - acc: 0.9453 - val_loss: 0.2451 - val_acc: 0.9025\n",
      "Epoch 99/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9496WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2965 - acc: 0.8669\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 39s 160ms/step - loss: 0.1473 - acc: 0.9495 - val_loss: 0.2965 - val_acc: 0.8669\n",
      "Epoch 100/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9455WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2782 - acc: 0.8975\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 38s 156ms/step - loss: 0.1454 - acc: 0.9454 - val_loss: 0.2782 - val_acc: 0.8975\n",
      "Epoch 101/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9493WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.4018 - acc: 0.8475\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 66s 273ms/step - loss: 0.1403 - acc: 0.9493 - val_loss: 0.4018 - val_acc: 0.8475\n",
      "Epoch 102/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9522WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.1993 - acc: 0.9250\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 38s 159ms/step - loss: 0.1373 - acc: 0.9522 - val_loss: 0.1993 - val_acc: 0.9250\n",
      "Epoch 103/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9533WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3198/3183 [==============================] - 10s 3ms/sample - loss: 0.2590 - acc: 0.9090\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 46s 192ms/step - loss: 0.1301 - acc: 0.9529 - val_loss: 0.2590 - val_acc: 0.9090\n",
      "Epoch 104/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9511WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.4046 - acc: 0.8550\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 48s 199ms/step - loss: 0.1397 - acc: 0.9509 - val_loss: 0.4046 - val_acc: 0.8550\n",
      "Epoch 105/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9510WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.1964 - acc: 0.9275\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 43s 180ms/step - loss: 0.1376 - acc: 0.9511 - val_loss: 0.1964 - val_acc: 0.9275\n",
      "Epoch 106/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9512WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.4042 - acc: 0.8384\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 42s 172ms/step - loss: 0.1400 - acc: 0.9512 - val_loss: 0.4042 - val_acc: 0.8384\n",
      "Epoch 107/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9501WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2800 - acc: 0.8944\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 39s 160ms/step - loss: 0.1441 - acc: 0.9502 - val_loss: 0.2800 - val_acc: 0.8944\n",
      "Epoch 108/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9518WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.2125 - acc: 0.9150\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 52s 216ms/step - loss: 0.1353 - acc: 0.9514 - val_loss: 0.2125 - val_acc: 0.9150\n",
      "Epoch 109/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9517WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2076 - acc: 0.9159\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 36s 147ms/step - loss: 0.1311 - acc: 0.9515 - val_loss: 0.2076 - val_acc: 0.9159\n",
      "Epoch 110/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9453WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.2656 - acc: 0.8941\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 54s 225ms/step - loss: 0.1478 - acc: 0.9453 - val_loss: 0.2656 - val_acc: 0.8941\n",
      "Epoch 111/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9576WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.3469 - acc: 0.8750\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 47s 195ms/step - loss: 0.1186 - acc: 0.9576 - val_loss: 0.3469 - val_acc: 0.8750\n",
      "Epoch 112/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9533WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2456 - acc: 0.8975\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 44s 182ms/step - loss: 0.1303 - acc: 0.9531 - val_loss: 0.2456 - val_acc: 0.8975\n",
      "Epoch 113/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9550WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.4945 - acc: 0.8200\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 42s 175ms/step - loss: 0.1291 - acc: 0.9550 - val_loss: 0.4945 - val_acc: 0.8200\n",
      "Epoch 114/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9520WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2420 - acc: 0.9144\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 39s 161ms/step - loss: 0.1292 - acc: 0.9520 - val_loss: 0.2420 - val_acc: 0.9144\n",
      "Epoch 115/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9534WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2448 - acc: 0.9050\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 37s 153ms/step - loss: 0.1283 - acc: 0.9533 - val_loss: 0.2448 - val_acc: 0.9050\n",
      "Epoch 116/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9566- ETA: 2s - loss: 0.12WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.2730 - acc: 0.9066\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 46s 191ms/step - loss: 0.1225 - acc: 0.9565 - val_loss: 0.2730 - val_acc: 0.9066\n",
      "Epoch 117/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9540WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.2792 - acc: 0.8875\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 45s 186ms/step - loss: 0.1309 - acc: 0.9543 - val_loss: 0.2792 - val_acc: 0.8875\n",
      "Epoch 118/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9545WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 18s 6ms/sample - loss: 0.4846 - acc: 0.8428\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 45s 187ms/step - loss: 0.1263 - acc: 0.9543 - val_loss: 0.4846 - val_acc: 0.8428\n",
      "Epoch 119/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9597WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3842 - acc: 0.8587\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 43s 177ms/step - loss: 0.1114 - acc: 0.9597 - val_loss: 0.3842 - val_acc: 0.8587\n",
      "Epoch 120/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9621WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2669 - acc: 0.8950\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 59s 245ms/step - loss: 0.1069 - acc: 0.9622 - val_loss: 0.2669 - val_acc: 0.8950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9540WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3383 - acc: 0.8750\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 40s 166ms/step - loss: 0.1219 - acc: 0.9540 - val_loss: 0.3383 - val_acc: 0.8750\n",
      "Epoch 122/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9572WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.2701 - acc: 0.9050\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 50s 209ms/step - loss: 0.1186 - acc: 0.9571 - val_loss: 0.2701 - val_acc: 0.9050\n",
      "Epoch 123/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9584WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.1824 - acc: 0.9362\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 41s 169ms/step - loss: 0.1108 - acc: 0.9584 - val_loss: 0.1824 - val_acc: 0.9362\n",
      "Epoch 124/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9550WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.2490 - acc: 0.9225\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 31s 126ms/step - loss: 0.1223 - acc: 0.9553 - val_loss: 0.2490 - val_acc: 0.9225\n",
      "Epoch 125/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9575- ETWARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.1699 - acc: 0.9450\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 0.1160 - acc: 0.9577 - val_loss: 0.1699 - val_acc: 0.9450\n",
      "Epoch 126/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9603WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2605 - acc: 0.9025\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.16872\n",
      "242/242 [==============================] - 39s 160ms/step - loss: 0.1140 - acc: 0.9601 - val_loss: 0.2605 - val_acc: 0.9025\n",
      "Epoch 127/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9623WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.1644 - acc: 0.9463\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.16872 to 0.16438, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/127-0.1644.hdf5\n",
      "242/242 [==============================] - 32s 131ms/step - loss: 0.1027 - acc: 0.9627 - val_loss: 0.1644 - val_acc: 0.9463\n",
      "Epoch 128/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9622WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.3882 - acc: 0.8625\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.16438\n",
      "242/242 [==============================] - 43s 179ms/step - loss: 0.1074 - acc: 0.9619 - val_loss: 0.3882 - val_acc: 0.8625\n",
      "Epoch 129/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9624WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.1834 - acc: 0.9250\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.16438\n",
      "242/242 [==============================] - 59s 243ms/step - loss: 0.1094 - acc: 0.9624 - val_loss: 0.1834 - val_acc: 0.9250\n",
      "Epoch 130/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9576WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2293 - acc: 0.9222\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.16438\n",
      "242/242 [==============================] - 38s 157ms/step - loss: 0.1156 - acc: 0.9574 - val_loss: 0.2293 - val_acc: 0.9222\n",
      "Epoch 131/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9584WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.2751 - acc: 0.9041\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.16438\n",
      "242/242 [==============================] - 32s 133ms/step - loss: 0.1177 - acc: 0.9582 - val_loss: 0.2751 - val_acc: 0.9041\n",
      "Epoch 132/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9615WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2050 - acc: 0.9225\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.16438\n",
      "242/242 [==============================] - 64s 264ms/step - loss: 0.1065 - acc: 0.9615 - val_loss: 0.2050 - val_acc: 0.9225\n",
      "Epoch 133/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9622WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2490 - acc: 0.9300\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.16438\n",
      "242/242 [==============================] - 39s 162ms/step - loss: 0.1056 - acc: 0.9622 - val_loss: 0.2490 - val_acc: 0.9300\n",
      "Epoch 134/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9604- ETA: 0s - loss: 0.1093 - acc:WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3155 - acc: 0.8916\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.16438\n",
      "242/242 [==============================] - 32s 132ms/step - loss: 0.1106 - acc: 0.9603 - val_loss: 0.3155 - val_acc: 0.8916\n",
      "Epoch 135/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9603WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4637 - acc: 0.8275\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.16438\n",
      "242/242 [==============================] - 38s 157ms/step - loss: 0.1097 - acc: 0.9604 - val_loss: 0.4637 - val_acc: 0.8275\n",
      "Epoch 136/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/242 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9592WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.2164 - acc: 0.9250\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.16438\n",
      "242/242 [==============================] - 33s 137ms/step - loss: 0.1137 - acc: 0.9594 - val_loss: 0.2164 - val_acc: 0.9250\n",
      "Epoch 137/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9640WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2601 - acc: 0.9144\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.16438\n",
      "242/242 [==============================] - 27s 111ms/step - loss: 0.1084 - acc: 0.9640 - val_loss: 0.2601 - val_acc: 0.9144\n",
      "Epoch 138/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9622WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.1500 - acc: 0.9525\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.16438 to 0.15004, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/138-0.1500.hdf5\n",
      "242/242 [==============================] - 39s 159ms/step - loss: 0.1019 - acc: 0.9623 - val_loss: 0.1500 - val_acc: 0.9525\n",
      "Epoch 139/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9616WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3603 - acc: 0.8650\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 63s 259ms/step - loss: 0.1036 - acc: 0.9616 - val_loss: 0.3603 - val_acc: 0.8650\n",
      "Epoch 140/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9603WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3051 - acc: 0.9050\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 49s 203ms/step - loss: 0.1074 - acc: 0.9603 - val_loss: 0.3051 - val_acc: 0.9050\n",
      "Epoch 141/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9620WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 8ms/sample - loss: 0.1979 - acc: 0.9300\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 52s 213ms/step - loss: 0.1012 - acc: 0.9621 - val_loss: 0.1979 - val_acc: 0.9300\n",
      "Epoch 142/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9651WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2914 - acc: 0.8925\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 28s 115ms/step - loss: 0.1052 - acc: 0.9650 - val_loss: 0.2914 - val_acc: 0.8925\n",
      "Epoch 143/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9646WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.4661 - acc: 0.8300\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 27s 113ms/step - loss: 0.0980 - acc: 0.9645 - val_loss: 0.4661 - val_acc: 0.8300\n",
      "Epoch 144/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9624WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.3192 - acc: 0.8600\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 46s 192ms/step - loss: 0.1019 - acc: 0.9628 - val_loss: 0.3192 - val_acc: 0.8600\n",
      "Epoch 145/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9602WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2597 - acc: 0.9100\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 37s 154ms/step - loss: 0.1074 - acc: 0.9602 - val_loss: 0.2597 - val_acc: 0.9100\n",
      "Epoch 146/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9628WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.1958 - acc: 0.9275\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 30s 125ms/step - loss: 0.1023 - acc: 0.9629 - val_loss: 0.1958 - val_acc: 0.9275\n",
      "Epoch 147/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9610WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.2635 - acc: 0.8975\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 39s 161ms/step - loss: 0.1084 - acc: 0.9612 - val_loss: 0.2635 - val_acc: 0.8975\n",
      "Epoch 148/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9655WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3349 - acc: 0.8900\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 31s 127ms/step - loss: 0.0955 - acc: 0.9653 - val_loss: 0.3349 - val_acc: 0.8900\n",
      "Epoch 149/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9651WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.3116 - acc: 0.8775\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 50s 208ms/step - loss: 0.0997 - acc: 0.9651 - val_loss: 0.3116 - val_acc: 0.8775\n",
      "Epoch 150/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9640WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3883 - acc: 0.8625s may du\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 31s 129ms/step - loss: 0.0987 - acc: 0.9642 - val_loss: 0.3883 - val_acc: 0.8625\n",
      "Epoch 151/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9628WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2298 - acc: 0.9125\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 39s 161ms/step - loss: 0.1004 - acc: 0.9629 - val_loss: 0.2298 - val_acc: 0.9125\n",
      "Epoch 152/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9649WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.3285 - acc: 0.8700\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 33s 138ms/step - loss: 0.0969 - acc: 0.9651 - val_loss: 0.3285 - val_acc: 0.8700\n",
      "Epoch 153/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9621WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2419 - acc: 0.9200\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 62s 258ms/step - loss: 0.0978 - acc: 0.9621 - val_loss: 0.2419 - val_acc: 0.9200\n",
      "Epoch 154/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9681WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3551 - acc: 0.8775\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 24s 100ms/step - loss: 0.0921 - acc: 0.9682 - val_loss: 0.3551 - val_acc: 0.8775\n",
      "Epoch 155/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9644WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2784 - acc: 0.9013\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 39s 162ms/step - loss: 0.1013 - acc: 0.9642 - val_loss: 0.2784 - val_acc: 0.9013\n",
      "Epoch 156/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9644WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.2989 - acc: 0.9000\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 41s 170ms/step - loss: 0.1028 - acc: 0.9645 - val_loss: 0.2989 - val_acc: 0.9000\n",
      "Epoch 157/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9661WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.1938 - acc: 0.9325\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 63s 260ms/step - loss: 0.0934 - acc: 0.9663 - val_loss: 0.1938 - val_acc: 0.9325\n",
      "Epoch 158/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9660WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.4116 - acc: 0.8650\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 25s 102ms/step - loss: 0.0995 - acc: 0.9662 - val_loss: 0.4116 - val_acc: 0.8650\n",
      "Epoch 159/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9629WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 6ms/sample - loss: 0.2421 - acc: 0.9075\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 45s 186ms/step - loss: 0.1038 - acc: 0.9628 - val_loss: 0.2421 - val_acc: 0.9075\n",
      "Epoch 160/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9650WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2493 - acc: 0.9150\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 53s 218ms/step - loss: 0.1005 - acc: 0.9651 - val_loss: 0.2493 - val_acc: 0.9150\n",
      "Epoch 161/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9651WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.2816 - acc: 0.8975\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 30s 125ms/step - loss: 0.0919 - acc: 0.9652 - val_loss: 0.2816 - val_acc: 0.8975\n",
      "Epoch 162/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9687WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.3504 - acc: 0.8750\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 33s 138ms/step - loss: 0.0922 - acc: 0.9683 - val_loss: 0.3504 - val_acc: 0.8750\n",
      "Epoch 163/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9638WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2319 - acc: 0.9225\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 39s 162ms/step - loss: 0.0999 - acc: 0.9638 - val_loss: 0.2319 - val_acc: 0.9225\n",
      "Epoch 164/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9695WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.2931 - acc: 0.9050\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 26s 106ms/step - loss: 0.0946 - acc: 0.9695 - val_loss: 0.2931 - val_acc: 0.9050\n",
      "Epoch 165/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9637WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2402 - acc: 0.9175\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 25s 105ms/step - loss: 0.1004 - acc: 0.9636 - val_loss: 0.2402 - val_acc: 0.9175\n",
      "Epoch 166/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9656WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2676 - acc: 0.9150\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 38s 157ms/step - loss: 0.0914 - acc: 0.9654 - val_loss: 0.2676 - val_acc: 0.9150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9693WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.2314 - acc: 0.9250\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 29s 119ms/step - loss: 0.0862 - acc: 0.9693 - val_loss: 0.2314 - val_acc: 0.9250\n",
      "Epoch 168/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9678WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.3763 - acc: 0.8700\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 30s 124ms/step - loss: 0.0898 - acc: 0.9678 - val_loss: 0.3763 - val_acc: 0.8700\n",
      "Epoch 169/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9674WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.2082 - acc: 0.9400\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 43s 179ms/step - loss: 0.0866 - acc: 0.9671 - val_loss: 0.2082 - val_acc: 0.9400\n",
      "Epoch 170/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9680WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3150 - acc: 0.8856\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 30s 125ms/step - loss: 0.0878 - acc: 0.9678 - val_loss: 0.3150 - val_acc: 0.8856\n",
      "Epoch 171/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9646WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3355 - acc: 0.8725\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.15004\n",
      "242/242 [==============================] - 30s 122ms/step - loss: 0.0951 - acc: 0.9646 - val_loss: 0.3355 - val_acc: 0.8725\n",
      "Epoch 172/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9671WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.1431 - acc: 0.9450\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.15004 to 0.14306, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv_checkpoint/172-0.1431.hdf5\n",
      "242/242 [==============================] - 38s 158ms/step - loss: 0.0878 - acc: 0.9672 - val_loss: 0.1431 - val_acc: 0.9450\n",
      "Epoch 173/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9696WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2667 - acc: 0.9050\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 60s 249ms/step - loss: 0.0886 - acc: 0.9697 - val_loss: 0.2667 - val_acc: 0.9050\n",
      "Epoch 174/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9702WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.3430 - acc: 0.8725\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 52s 213ms/step - loss: 0.0860 - acc: 0.9701 - val_loss: 0.3430 - val_acc: 0.8725\n",
      "Epoch 175/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9665WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2779 - acc: 0.9006\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 60s 247ms/step - loss: 0.0905 - acc: 0.9667 - val_loss: 0.2779 - val_acc: 0.9006\n",
      "Epoch 176/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9700WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2787 - acc: 0.9025\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 38s 157ms/step - loss: 0.0877 - acc: 0.9700 - val_loss: 0.2787 - val_acc: 0.9025\n",
      "Epoch 177/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9664WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.3705 - acc: 0.8625\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 38s 159ms/step - loss: 0.0922 - acc: 0.9665 - val_loss: 0.3705 - val_acc: 0.8625\n",
      "Epoch 178/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9669WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.2637 - acc: 0.9175\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 54s 222ms/step - loss: 0.0929 - acc: 0.9670 - val_loss: 0.2637 - val_acc: 0.9175\n",
      "Epoch 179/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9716WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.2481 - acc: 0.9150\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 29s 120ms/step - loss: 0.0837 - acc: 0.9717 - val_loss: 0.2481 - val_acc: 0.9150\n",
      "Epoch 180/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9734WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2816 - acc: 0.9100\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 47s 195ms/step - loss: 0.0779 - acc: 0.9733 - val_loss: 0.2816 - val_acc: 0.9100\n",
      "Epoch 181/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9691WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3728 - acc: 0.8788\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 60s 248ms/step - loss: 0.0890 - acc: 0.9693 - val_loss: 0.3728 - val_acc: 0.8788\n",
      "Epoch 182/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9683WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 23s 7ms/sample - loss: 0.3058 - acc: 0.8800\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 51s 209ms/step - loss: 0.0885 - acc: 0.9682 - val_loss: 0.3058 - val_acc: 0.8800\n",
      "Epoch 183/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9672WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 22s 7ms/sample - loss: 0.3296 - acc: 0.8875\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 40s 167ms/step - loss: 0.0892 - acc: 0.9673 - val_loss: 0.3296 - val_acc: 0.8875\n",
      "Epoch 184/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9707WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.3434 - acc: 0.8925\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 42s 172ms/step - loss: 0.0783 - acc: 0.9707 - val_loss: 0.3434 - val_acc: 0.8925\n",
      "Epoch 185/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9702WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.1897 - acc: 0.9391\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 32s 133ms/step - loss: 0.0830 - acc: 0.9701 - val_loss: 0.1897 - val_acc: 0.9391\n",
      "Epoch 186/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9717WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.1751 - acc: 0.9325\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 70s 288ms/step - loss: 0.0830 - acc: 0.9718 - val_loss: 0.1751 - val_acc: 0.9325\n",
      "Epoch 187/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9716WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.1938 - acc: 0.9300\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 37s 155ms/step - loss: 0.0817 - acc: 0.9714 - val_loss: 0.1938 - val_acc: 0.9300\n",
      "Epoch 188/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9697WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3518 - acc: 0.8662\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 39s 160ms/step - loss: 0.0877 - acc: 0.9697 - val_loss: 0.3518 - val_acc: 0.8662\n",
      "Epoch 189/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9672WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3580 - acc: 0.8866\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 27s 110ms/step - loss: 0.0887 - acc: 0.9673 - val_loss: 0.3580 - val_acc: 0.8866\n",
      "Epoch 190/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9720WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.2081 - acc: 0.9250\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 36s 147ms/step - loss: 0.0718 - acc: 0.9723 - val_loss: 0.2081 - val_acc: 0.9250\n",
      "Epoch 191/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9660WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2361 - acc: 0.9100\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 31s 126ms/step - loss: 0.0904 - acc: 0.9662 - val_loss: 0.2361 - val_acc: 0.9100\n",
      "Epoch 192/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9699WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.2847 - acc: 0.9006\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 38s 157ms/step - loss: 0.0846 - acc: 0.9698 - val_loss: 0.2847 - val_acc: 0.9006\n",
      "Epoch 193/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9736WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.2139 - acc: 0.9225\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 30s 123ms/step - loss: 0.0781 - acc: 0.9736 - val_loss: 0.2139 - val_acc: 0.9225\n",
      "Epoch 194/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9721WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 13s 4ms/sample - loss: 0.3051 - acc: 0.9031\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 30s 126ms/step - loss: 0.0758 - acc: 0.9722 - val_loss: 0.3051 - val_acc: 0.9031\n",
      "Epoch 195/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9689WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 4ms/sample - loss: 0.3852 - acc: 0.8700\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 30s 122ms/step - loss: 0.0855 - acc: 0.9689 - val_loss: 0.3852 - val_acc: 0.8700\n",
      "Epoch 196/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9692WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.2379 - acc: 0.9150\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 36s 149ms/step - loss: 0.0846 - acc: 0.9692 - val_loss: 0.2379 - val_acc: 0.9150\n",
      "Epoch 197/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9708WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 19s 6ms/sample - loss: 0.3339 - acc: 0.8850\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 73s 302ms/step - loss: 0.0807 - acc: 0.9709 - val_loss: 0.3339 - val_acc: 0.8850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9744WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.3447 - acc: 0.8975\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 29s 118ms/step - loss: 0.0739 - acc: 0.9744 - val_loss: 0.3447 - val_acc: 0.8975\n",
      "Epoch 199/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9697WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2636 - acc: 0.9162\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 29s 121ms/step - loss: 0.0815 - acc: 0.9698 - val_loss: 0.2636 - val_acc: 0.9162\n",
      "Epoch 200/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9753WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 14s 5ms/sample - loss: 0.5743 - acc: 0.8100\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 31s 127ms/step - loss: 0.0715 - acc: 0.9753 - val_loss: 0.5743 - val_acc: 0.8100\n",
      "Epoch 201/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9707WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.3276 - acc: 0.8925\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 49s 201ms/step - loss: 0.0866 - acc: 0.9707 - val_loss: 0.3276 - val_acc: 0.8925\n",
      "Epoch 202/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9700WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.2537 - acc: 0.9075\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 42s 172ms/step - loss: 0.0844 - acc: 0.9700 - val_loss: 0.2537 - val_acc: 0.9075\n",
      "Epoch 203/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9708WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3166 - acc: 0.9025\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 39s 163ms/step - loss: 0.0844 - acc: 0.9709 - val_loss: 0.3166 - val_acc: 0.9025\n",
      "Epoch 204/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9726WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 17s 5ms/sample - loss: 0.3595 - acc: 0.8788\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 47s 193ms/step - loss: 0.0747 - acc: 0.9727 - val_loss: 0.3595 - val_acc: 0.8788\n",
      "Epoch 205/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9697WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.2856 - acc: 0.9075\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 53s 220ms/step - loss: 0.0796 - acc: 0.9698 - val_loss: 0.2856 - val_acc: 0.9075\n",
      "Epoch 206/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9713WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 8s 3ms/sample - loss: 0.3477 - acc: 0.8834\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 38s 158ms/step - loss: 0.0766 - acc: 0.9711 - val_loss: 0.3477 - val_acc: 0.8834\n",
      "Epoch 207/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9711WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 15s 5ms/sample - loss: 0.2583 - acc: 0.9225\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 30s 125ms/step - loss: 0.0793 - acc: 0.9712 - val_loss: 0.2583 - val_acc: 0.9225\n",
      "Epoch 208/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9695WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 16s 5ms/sample - loss: 0.2939 - acc: 0.9025\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 43s 179ms/step - loss: 0.0815 - acc: 0.9696 - val_loss: 0.2939 - val_acc: 0.9025\n",
      "Epoch 209/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9706WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 21s 7ms/sample - loss: 0.4183 - acc: 0.8631\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 39s 162ms/step - loss: 0.0822 - acc: 0.9705 - val_loss: 0.4183 - val_acc: 0.8631\n",
      "Epoch 210/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9704WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 20s 6ms/sample - loss: 0.2213 - acc: 0.9200\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 73s 303ms/step - loss: 0.0780 - acc: 0.9704 - val_loss: 0.2213 - val_acc: 0.9200\n",
      "Epoch 211/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9742WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.3253 - acc: 0.8791\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 30s 126ms/step - loss: 0.0713 - acc: 0.9740 - val_loss: 0.3253 - val_acc: 0.8791\n",
      "Epoch 212/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9749WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2865 - acc: 0.9009\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 37s 155ms/step - loss: 0.0718 - acc: 0.9750 - val_loss: 0.2865 - val_acc: 0.9009\n",
      "Epoch 213/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9691WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.1834 - acc: 0.9237\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 39s 160ms/step - loss: 0.0831 - acc: 0.9693 - val_loss: 0.1834 - val_acc: 0.9237\n",
      "Epoch 214/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9724WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 4ms/sample - loss: 0.2058 - acc: 0.9187\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 39s 160ms/step - loss: 0.0758 - acc: 0.9722 - val_loss: 0.2058 - val_acc: 0.9187\n",
      "Epoch 215/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9701WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 10s 3ms/sample - loss: 0.1531 - acc: 0.93505s - loss: 0.1\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 65s 269ms/step - loss: 0.0819 - acc: 0.9702 - val_loss: 0.1531 - val_acc: 0.9350\n",
      "Epoch 216/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9731WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 9s 3ms/sample - loss: 0.1763 - acc: 0.9269\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 39s 161ms/step - loss: 0.0766 - acc: 0.9731 - val_loss: 0.1763 - val_acc: 0.9269\n",
      "Epoch 217/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9700WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.3155 - acc: 0.8900\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 52s 214ms/step - loss: 0.0788 - acc: 0.9699 - val_loss: 0.3155 - val_acc: 0.8900\n",
      "Epoch 218/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9727WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2521 - acc: 0.9075\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 39s 160ms/step - loss: 0.0737 - acc: 0.9727 - val_loss: 0.2521 - val_acc: 0.9075\n",
      "Epoch 219/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9713WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 24s 7ms/sample - loss: 0.1632 - acc: 0.9425\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 61s 253ms/step - loss: 0.0793 - acc: 0.9713 - val_loss: 0.1632 - val_acc: 0.9425\n",
      "Epoch 220/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9743WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.3198 - acc: 0.8884\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 40s 164ms/step - loss: 0.0709 - acc: 0.9743 - val_loss: 0.3198 - val_acc: 0.8884\n",
      "Epoch 221/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9739WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 11s 3ms/sample - loss: 0.1824 - acc: 0.9250\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 29s 119ms/step - loss: 0.0724 - acc: 0.9739 - val_loss: 0.1824 - val_acc: 0.9250\n",
      "Epoch 222/10000\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9751WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 12s 4ms/sample - loss: 0.2694 - acc: 0.9150\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.14306\n",
      "242/242 [==============================] - 39s 161ms/step - loss: 0.0706 - acc: 0.9752 - val_loss: 0.2694 - val_acc: 0.9150\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3183 [==============================] - 3s 988us/sample - loss: 0.3208 - acc: 0.8882\n",
      "Loss: 0.3208058054927307 Accuracy: 0.8881558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 8):\n",
    "    base = 'vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    \n",
    "#     with tf.device('/cpu:0'):\n",
    "    model = build_2d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit_generator(data_generator,\n",
    "            steps_per_epoch=len(x_train)//batch_size,\n",
    "            epochs=10000,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks = [checkpointer, early_stopping],\n",
    "            workers=8, \n",
    "            use_multiprocessing=True\n",
    "    )\n",
    "    \n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_3_conv Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 185862    \n",
      "=================================================================\n",
      "Total params: 265,190\n",
      "Trainable params: 265,190\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 4s 1ms/sample - loss: 0.6285 - acc: 0.7339\n",
      "Loss: 0.6284815692841689 Accuracy: 0.7338988\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 46470     \n",
      "=================================================================\n",
      "Total params: 228,262\n",
      "Trainable params: 228,262\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 3s 815us/sample - loss: 0.4642 - acc: 0.8131\n",
      "Loss: 0.4641880625604347 Accuracy: 0.8130694\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 11, 11, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 27654     \n",
      "=================================================================\n",
      "Total params: 414,374\n",
      "Trainable params: 414,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3183 [==============================] - 2s 762us/sample - loss: 0.4856 - acc: 0.8014\n",
      "Loss: 0.48557202282975837 Accuracy: 0.8014452\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 11, 11, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 6, 6, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 6918      \n",
      "=================================================================\n",
      "Total params: 803,366\n",
      "Trainable params: 803,366\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 3s 792us/sample - loss: 0.2597 - acc: 0.9045\n",
      "Loss: 0.259689773843418 Accuracy: 0.9044926\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 11, 11, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 6, 6, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 3, 3, 256)         819456    \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 1,622,054\n",
      "Trainable params: 1,622,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 3s 816us/sample - loss: 0.3208 - acc: 0.8882s - loss: 0.\n",
      "Loss: 0.3208058054927307 Accuracy: 0.8881558\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = 'vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 8):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 185862    \n",
      "=================================================================\n",
      "Total params: 265,190\n",
      "Trainable params: 265,190\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 3s 843us/sample - loss: 0.3761 - acc: 0.8577\n",
      "Loss: 0.37613690830123453 Accuracy: 0.85768145\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 46470     \n",
      "=================================================================\n",
      "Total params: 228,262\n",
      "Trainable params: 228,262\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 3s 797us/sample - loss: 0.3482 - acc: 0.8724\n",
      "Loss: 0.3481792215476742 Accuracy: 0.8724474\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 11, 11, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 27654     \n",
      "=================================================================\n",
      "Total params: 414,374\n",
      "Trainable params: 414,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 3s 810us/sample - loss: 0.2771 - acc: 0.8998\n",
      "Loss: 0.2770636764536704 Accuracy: 0.8997801\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 11, 11, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 6, 6, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 6918      \n",
      "=================================================================\n",
      "Total params: 803,366\n",
      "Trainable params: 803,366\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3183 [==============================] - 3s 839us/sample - loss: 0.2244 - acc: 0.9211\n",
      "Loss: 0.22444919844374805 Accuracy: 0.9211436\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 11, 11, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 6, 6, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 3, 3, 256)         819456    \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 1,622,054\n",
      "Trainable params: 1,622,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 3s 928us/sample - loss: 0.2718 - acc: 0.9230\n",
      "Loss: 0.271799382457859 Accuracy: 0.9230286\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 8):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_050_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 172, 172, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 172, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 86, 86, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 86, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 22, 22, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 11, 11, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 6, 6, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 6918      \n",
      "=================================================================\n",
      "Total params: 803,366\n",
      "Trainable params: 803,366\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3183/3183 [==============================] - 3s 886us/sample - loss: 0.2244 - acc: 0.9211\n",
      "Loss: 0.22444919844374805 Accuracy: 0.9211436\n"
     ]
    }
   ],
   "source": [
    "i = 6\n",
    "model_name = base+'_{}_conv'.format(i)\n",
    "print()\n",
    "print(model_name, 'Model')\n",
    "model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "# model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "\n",
    "model = load_model(model_filename)\n",
    "model.summary()\n",
    "\n",
    "[loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[  28    0    1    1    0    0]\n",
      " [   5  932   12   85    2   14]\n",
      " [   0    3   62   13    0    0]\n",
      " [   4   29   75 1831    0    1]\n",
      " [   0    0    0    1   32    1]\n",
      " [   0    1    0    1    2   47]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bed       0.76      0.93      0.84        30\n",
      "        bird       0.97      0.89      0.93      1050\n",
      "         cat       0.41      0.79      0.54        78\n",
      "         dog       0.95      0.94      0.95      1940\n",
      "       house       0.89      0.94      0.91        34\n",
      "        tree       0.75      0.92      0.82        51\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      3183\n",
      "   macro avg       0.79      0.90      0.83      3183\n",
      "weighted avg       0.93      0.92      0.93      3183\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f82284b7780>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAFdCAYAAAAJ/HYjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGkFJREFUeJzt3XuUXVWB5/HvjyRkwKQKdIGPIRFkRURloFsxijqymtYBdXz06KitS+IMjK1LRB4yQ8sYFBFEjPHV4wiOyAyOrY6A0j5a1CiNiCKIdJMmIIY3iiBVeYfAnj/OKb19rUoq4da9VTvfz1p3VZ2zz2Pve0/9zr77nLo3pRQkSfXZZdAVkCRNDQNekiplwEtSpQx4SaqUAS9JlTLgJalSBrwkVcqAl6RKGfCSVCkDXpIqZcBLUqVmD7oC/ZYkwJOANYOuiyRth/nA3WU7PkBspwt4mnC/c9CVkKQdsA9w12QX3hkDfg3Ar1avZv7Q0KDr8qjMmTVr0FWQ1Aejo6MsWLAAtnPkYWcMeADmDw0xZMBLqpgXWSWpUga8JFXKgJekShnwklQpA16SKmXAS1KlDHhJqpQBL0mVMuAlqVIGvCRVyoCXpEoZ8JJUKQNekiplwEtSpQx4SaqUAS9JlTLgJalSfQ34JCuSLO/xNpckebCX25SkGtiDl6RKGfCSVKlBBPzsJJ9MMpLkt0nOSBKAJHOTnJvkriTrklyd5PDOldshmduTrE9yMfC4AbRBkqa9QQT80cAW4DnA8cCJwDFt2SeB5wGvB/4N8GXgW0kWASRZDHy2Xe4Q4PvAaVvbWXvSGBp7APN73iJJmoZSSunfzpIVwN7AM0q74yRnA68AjgRuBRaWUu7uWOdy4CellL9O8gVguJTyso7yLwJHllL2mGCfpwNLu+f/9oEHGBoa6lXTBmLOrFmDroKkPhgdHWV4eBia/Bud7HqD6MH/uPzLs8pVwCLgIGAWsCrJ2rEH8CJg/3bZA4Gru7Z31Tb2dxYw3PHY51HWX5JmhNmDrkCHecDDwLPan53W7uhGSymbgE1j0+1wvyRVbxABv7hr+rnAzcB1ND34vUspV0yw7soJ1pckdRnEEM3CJMuSHJDkDcBxwMdKKauAi4ALk/xFkv2SPCfJqUnGxtw/DhyZ5OQki5K8g2bsXpLUZRABfyGwG/AT4FPAx4DPtGVvacs/AtwEXAIcCtwOUEr5MXAszd031wMvAT7Qx7pL0ozR17topoP2VskR76KRNFPMpLtoJEl9YMBLUqUMeEmqlAEvSZUy4CWpUga8JFXKgJekShnwklQpA16SKmXAS1KlDHhJqpQBL0mVMuAlqVIGvCRVyoCXpEoZ8JJUKQNekiplwEtSpWYPugKDMmfWrBn/lXfv/G/LBl2Fnll+1gmDrkJPjG7YMOgq9MQeu+8+6Cr0zCMVfC3pjrbBHrwkVcqAl6RKGfCSVCkDXpIqZcBLUqUMeEmqlAEvSZUy4CWpUga8JFXKgJekShnwklQpA16SKmXAS1KlDHhJqpQBL0mVMuAlqVIGvCRVyoCXpEoZ8JJUqb4FfJIVSZZvpXx1knftwHYPT1KS7PHoaihJdZlOX7p9KLBu0JWQpFpMm4Avpdy3tfIkc0opD/WrPpI00/V7DH52kk8mGUny2yRnJAn88RBNO+zytiRfS7IOeE87/6VJViXZkOT7wL59boMkzQj9DvijgS3Ac4DjgROBY7ay/OnAxcBBwP9KsgD4KvB14BDgfODsKayvJM1Y/R6iuQM4oZRSgJuSHAScAJw3wfJfKKV8bmwiyQeBX5ZSTmpnjW3jv060wyRzgbkds+Y/mgZI0kzR7x78j9twH3MVsCjJrAmWv6Zr+kDg6q55V21jn6cCIx2POydZV0ma0ab7ffC9uKvmLGC447FPD7YpSdNev4doFndNPxe4uZTycHutdVtWAq8YZxsTKqVsAjaNTU9yP5I04/W7B78wybIkByR5A3Ac8LHtWP/TNEM6H2638ZfAkqmoqCTNdP0O+AuB3YCfAJ+iCffPTHblUsrtwH8AXgVcD/wV8Ne9r6YkzXx9G6IppRzeMfm2ccr37ZoedyyllHIZcFnX7M+Nt6wk7cym+0VWSdIOMuAlqVIGvCRVyoCXpEoZ8JJUKQNekiplwEtSpQx4SaqUAS9JlTLgJalSBrwkVcqAl6RKGfCSVCkDXpIqZcBLUqUMeEmqlAEvSZUy4CWpUga8JFUqpZRB16GvkgwBIyMjIwwNDQ26Omqt27Rx0FXoid12nTvoKvTELhn3K5E1IKOjowwPDwMMl1JGJ7uePXhJqpQBL0mVMuAlqVIGvCRVyoCXpEoZ8JJUKQNekiplwEtSpQx4SaqUAS9JlTLgJalSBrwkVcqAl6RKGfCSVCkDXpIqZcBLUqUMeEmqlAEvSZUy4CWpUga8JFVqRgd8ktOT/HzQ9ZCk6WhGB7wkaWIDD/gkuyQ5JcktSTYluT3Je9qyDyVZlWR9kluTnJFkTlu2BFgKHJyktI8lg2uJJE0vswddAeAs4FjgBOAfgCcCT2vL1gBLgLuBg4Dz2nnnAH8LPBM4EvjzdvmR7o0nmQvM7Zg1v9cNkKTpaKABn2Q+cDzwjlLK59vZv6QJekopH+hYfHWSc4HXA+eUUjYkWQtsKaXcu5XdnErT05ekncqge/AH0vSuvzteYZLXAe8E9gfm0dR3dDv3cRawrGN6PnDndtdUkmaYQY/Bb5ioIMnzgIuAbwAvB/4EOBPYdXt2UErZVEoZHXvQDPFIUvUGHfA304T8EeOUHQbcVko5s5RyTSnlZuDJXctsBmZNcR0laUYa6BBNKWVjkg8B5yTZDFwJ7AU8gyb8FyZ5PfBT4GXAq7s2sRrYL8khNMMua0opm/pVf0mazgbdgwc4A/gI8H5gJc3dMXuXUr4GfBT4JPBzmh79GV3r/j/gW8D3gfuAN/SpzpI07aWUMug69FWSIWBkZGSEoaGhQVdHrXWbNg66Cj2x265zt73QDLBLMugqqMPo6CjDw8MAw+21xEmZDj14SdIUMOAlqVIGvCRVyoCXpEoZ8JJUKQNekiplwEtSpQx4SaqUAS9JlTLgJalSBrwkVcqAl6RKGfCSVCkDXpIqZcBLUqUMeEmqlAEvSZUy4CWpUgP90m09Opu3bBl0FXrm4Ufq+OrIWbvU0Wfa2b7Ks1Z1HI2SpD9iwEtSpQx4SaqUAS9JlTLgJalSBrwkVcqAl6RKGfCSVCkDXpIqZcBLUqUMeEmqlAEvSZUy4CWpUga8JFXKgJekShnwklQpA16SKmXAS1KlDHhJqtTAAz7JiiTLB10PSarNwANekjQ1DHhJqlRfAz7JY5JcmGRtknuSnNRVvmdb/rsk65N8M8mirmWOTXJHW35xkhOTPNjPdkjSTNDvHvyHgRcBrwReAhwO/GlH+QXAs4FXAM8DAnwjyRyAJM8HPg18DDgE+A7wnq3tMMncJENjD2B+D9sjSdPW7H7tKMk84D8DbyqlfLeddzRwZ/v7Ippgf34p5UftvDcCdwCvAr4MHAd8s5RybrvZVUkOA16+lV2fCiztfYskaXrrZw9+f2BX4OqxGaWUB4Cb2skDgS1d5fe35Qe2sw4AftK13e7pbmcBwx2PfXas+pI0s/StBz8opZRNwKax6SQDrI0k9U8/e/C/BB4CFo/NSLIn8NR2ciXNCaez/HE0vfYb21k3AYd2bbd7WpJEH3vwpZS1ST4LfDjJ/cBvgDOBR9rym5NcCpyX5K3AGuBs4C7g0nYznwB+mORE4OvAnwFHAaVf7ZCkmaLfd9G8G7iCJpwvB/4B+FlH+Vva6cuAq2juonlpKeUhgFLKlcBfAScC1wNHAh8FNvap/pI0Y6SUmd35TXIe8LRSygsnufwQMDIyMsLQ0NDUVm6Kbd6yZdBV6JmNDz006Cr0xPDuuw+6Cj0x03OhNqOjowwPDwMMl1JGJ7vejLvImuRkmvvf19EMzxwNvH2glZKkaWjGBTzwHOAUmn9YuhV4Zynl/MFWSZKmnxkX8KWU/zjoOkjSTOCHjUlSpQx4SaqUAS9JlTLgJalSBrwkVcqAl6RKGfCSVCkDXpIqZcBLUqUMeEmqlAEvSZUy4CWpUga8JFXKgJekShnwklSpGfd58PqDXWfX8/LV0pZavuqupq+DrOXY2hH24CWpUga8JFXKgJekShnwklQpA16SKmXAS1KlDHhJqpQBL0mVMuAlqVIGvCRVyoCXpEoZ8JJUKQNekiplwEtSpQx4SaqUAS9JlTLgJalSBrwkVcqAl6RKGfCSVKntCvgkK5Isn6rKSJJ6xx68JFXKgJekSu1IwO+S5JwkDyS5N8npYwVJFia5NMnaJKNJvpTk8R3lFyS5pHNjSZYnWdEx/ZokNyTZkOT+JJcneUxH+TFJVibZmOSfk7x9B9ogSdWbvQPrHA0sAxYDzwMuSHIl8F3gUmAt8KJ2258C/hY4fDIbTvJE4P8CpwAXA/OBFwJpy98IvB94B3Ad8CfAeUnWlVI+P8E25wJzO2bNn3xTJWnm2pGA/0Up5X3t7zcneQdwRDt9ELBfKeUOgCRvBv4pyaGllJ9OYttPbOv01VLKbe28GzrK3wecVEr5ajv9qyRPB94KjBvwwKnA0sk0TJJqsiNDNL/omr4H2Bs4ELhjLNwBSik3Ag+2ZZNxPc07gRuSfDnJsUn2BGiHafYHPtsOAa1NshY4rZ0/kbOA4Y7HPpOsiyTNaDvSg3+oa7ow+RPFI7TDLR3m/H5DpTyc5MXAYcBLgOOAM5MsBta3ix0LXN21jYcn2mEpZROwaWw66d69JNWpl3fRrAQWJFkwNqMdPtkDuLGddR/NMEynQzonSuPKUspSmjH2zcCrSym/Bu4GnlJKuaXr8asetkOSqrAjPfiJXE4zXn5Rkne12/4b4AellGvaZb4HvLsdm78KeBPwTJoLprQ99SOAvwd+Q3Mhdy+akwc0Y+kfTzICfIvm4umzgT1LKct62BZJmvF61oMvpRTglcDvgB/SBP6twOs6lvk2cAZwDvBTmjtaLuzYzCjwb4FvAKuAD9BcVP1mu/75wDHAW2hOJj8AlgD24CWpS5pc3nkkGQJGRkZGGBoaGnR1pGlp85Ytg65Cz+w6u5cDFYMxOjrK8PAwwHApZXSy6/mfrJJUKQNekiplwEtSpQx4SaqUAS9JlTLgJalSBrwkVcqAl6RKGfCSVCkDXpIqZcBLUqUMeEmqlAEvSZUy4CWpUga8JFXKgJekShnwklQpA16SKjXzv8tKUs/V8DV3YzZs3jzoKjxqO9oGe/CSVCkDXpIqZcBLUqUMeEmqlAEvSZUy4CWpUga8JFXKgJekShnwklQpA16SKmXAS1KlDHhJqpQBL0mVMuAlqVIGvCRVyoCXpEoZ8JJUKQNekiplwEtSpQx4SapUXwI+yYoky/uxL0lSY1r04NOo52vcJWkamPKAT3IB8CLg+CSlfSxpfx6V5GfAJuAF7fKvTHJtko1Jbk2ytDP8k+yR5Pwk9yUZTfK9JAdPdTskaabpR6/5eOCpwD8C723nPaP9eTZwMnAr8LskLwQuBN4JXAHsD3ymXfZ97c8vAxuAo4AR4K3Ad5M8tZTyQPfOk8wF5nbMmt+bZknS9DblPfhSygiwGVhfSrm3lHIv8HBb/N5SyndKKb9sw3kpcHYp5fOllFtLKd8B/jtNiJPkBcBzgNeWUq4ppdxcSjkZeBB4zQRVOJXmRDD2uHOKmipJ08qgx72v6Zo+GHh+kvd0zJsF/Ksku7fl84D7k3SutxtNb388ZwHLOqbnY8hL2gkMOuDXdU3Po+nFf3WcZTe25fcAh49T/uB4OyilbKIZ4weg68QgSdXqV8BvpumJb8u1wAGllFvGK0xyLfAEYEspZXXvqidJ9elXwK8GFifZF1jLxGP/7wcuS3I78BXgEZphmWeWUk4DLgeuAi5JcgqwCngS8DLg4lJK95CPJO20+nUf/Lk0F1ZvBO4DFo63UCnl28DLgZcAPwV+DJwA3NaWF+ClwA+Bz9EE/BeBJwO/ntIWSNIMkyYzdx5JhoCRkZERhoaGBl0dSVNsw+bNg67CozY6OsoT9toLYLiUMjrZ9abFf7JKknrPgJekShnwklQpA16SKmXAS1KlDHhJqpQBL0mVMuAlqVIGvCRVyoCXpEoZ8JJUKQNekiplwEtSpQx4SaqUAS9JlTLgJalSBrwkVapf38k67YyOTvpLUSTNYDV8o9OaNWt2aL2d8Sv7/jVw56DrIUk7YJ9Syl2TXXhnDPgATwJ27JQ4efNpTiT79GFfU8l2TD+1tMV2bP9+7i7bEdo73RBN++RM+gy4o5rzCABrtudLcqcb2zH91NIW27HdtnvbXmSVpEoZ8JJUKQN+6mwC3tf+nMlsx/RTS1tsxxTb6S6yStLOwh68JFXKgJekShnwklQpA347JFmRZHmPt7kkyYO93OY29rfVNiRZneRdO7Ddw5OUJHs8uhpqzFQcb4/WdKyTJmbAq9uhwGcGXYl+SnJ6kp8Puh6aHmo6ie10/8mqrSul3Le18iRzSikP9as+0nTTftzJrFLKlkHXZVvswW+/2Uk+mWQkyW+TnNG+4CSZm+TcJHclWZfk6iSHd67cDsncnmR9kouBx02zNvyLIZp22OVtSb6WZB3wnnb+S5OsSrIhyfeBfQfQjt9LskuSU5LckmRT+xyP1fVDbV3XJ7m1be+ctmwJsBQ4uG1raef1u/6PSXJhkrVJ7klyUlf5nm3579p2fDPJoq5ljk1yx9ixleTEKRr+2yXJOUkeSHJvktM76rAwyaVtO0aTfCnJ4zvKL0hySVe9lydZ0TH9miQ3tMfW/UkuT/KYjvJjkqxMsjHJPyd5e68aluQC4EXA8Z3HQ/vzqCQ/o7nf/QXt8q9Mcm1bl1uTLE0yu2N7eyQ5P8l97fPxvSQH96q+21RK8THJB7CC5sOElgMHAG8E1gHHtuXnAVcCLwT2B04GNgKL2vLFwMPAKcBTgXcCvwMenEZtWA28q2P5AvwaeAvwFGAhsKBt10c6tnFvu+weA3ptPgQ8ABzdPvcvAI5py04DDqM5Cf37tq6ntGW7AecC/wg8oX3sNoD6/w1wG3AEcBDwdZrPHlnell8K3NgeWwcD3wJuBua05c9vj62T22Pr7cD9vT622uNnhOakuAh4M/AI8GKaDuN1wBXAs9rj/RpgRcf6FwCXdG1z+dgywBOBh4AT2tfroLYt89ryNwJ3A38B7Nf+vB84ukftGwZ+RDNMOXY8HNEe29e37dwfeGz7Woy0x9xT2rJfAUs7tvcd4GvAs9vn61zgt8Bj+3Jc9ftAnsmP9uC+kfYfxNp5Z7fzFgJbgCd1rXM58MH29y8Af9dV/sVe/xHuaBva31fzxwH/0a5tfBD4p655ZzOggKf5lL2NtIE+ieVPBq7pmD4d+PkAj6t5NL3C13bMeyywvg2/Re1ze1hH+ePa8td2HEeXdW33//T62GqPnyu65v2kff1f3P4NLOgoe3pb90Pb6QvYesD/abv8kyfY/y3AG7rmnQb8qMdtXN4xfXhbp1d2LXc5cGrXvDfRfOIjNJ2MEWDuOG34L/04thyi2X4/Lu2r1LqK5g/wIGAWsKp9e7o2yVqat3v7t8seCFzdtb2rprrC4xi3DUlmTbD8NV3T06UdYw4E5gLfHa8wyeuSXNkOJ6wFPkBzQp4u9gd2peM5LaU8ANzUTh5IE5yd5fe35Qe2sw6gCdpO3dO98ouu6XuAvdu63FFKuaOjnjcCD3bUc1uup3kdb0jy5XbYaU9ohrFonqvPdv2NncYf/samUvffwcHAe7vqch7wxCS7t+XzgPu7ltmvT/X1ImsPzaN5i/ys9mentf2vTk+tG3QFtmHDRAVJngdcRDOk8G2aHtXrgZMmWkfb1H2RvTD563mPAOmaN+f3Gyrl4SQvphlSewlwHHBmksU071gAjuWPOxjdf3NTofvvYB7NcfXVcZbd2JbfQ/MOoFtfbo024Lff4q7p59KMhV5H04Pfu5RyxQTrrpxg/X4btw3tH9dk1l8JvGKcbQzKzTQhfwRwflfZYcBtpZQzx2YkeXLXMptpXrtB+SVNaC4GbofmoirNWPoPaJ7v2W35j9ryx9H02m9st3ETzS2unbqnp9pKYEGSBWO9+CRPB/bgD/W8D3hm13qH0HHSaN9dXglcmeT9NNcmXl1KWZbkbuAppZSLprAdkz0ergUOKKXcMl5hkmtpxvC3lFJW9656k2fAb7+FSZYB/5NmvPA44KRSyqokFwEXtndAXAfsRRM6vyil/B3wcZqD9mSai2b/DjhyurRhO9b/NHBSkg/TBOqzgCW9ruRklVI2JvkQcE6SzTThsBfwDJrwX5jk9cBPgZcBr+7axGpgvySH0Hwzz5pSSt8+GbCUsjbJZ4EPJ7kf+A1wJk1vl1LKzUkuBc5L8laai+Rn03xxzaXtZj4B/DDJiTQXaP8MOIqmd90vlwM3ABeluRNrNs3F4x+UUsaGN74HvDvJm2mG9d5EE/jXAbQ99SOAv6d5HhbTvJYr2/WXAh9PMkJzoXkuzQXMPUspy3rUjtXA4iT70rz7nujdyfuBy5LcDnyF5vU6GHhmKeU0mufjKuCSJKcAq2i+Te5lwMUdz8nU6cdAfy0PmosvnwL+B81b/Qdo/hDHPpVzDs3Hhv6KphdwN83bt4M6tvGfgDto3m5+jSZY+32RdWttWM0fX2R91TjbeTlNeG4Efkhzl80g76LZheYWztXtc38b7QUw4ByaOxfW0FyMfFfnc04TEl+huaOpAEsGUP95wP+mGQa4F3g3HRf7gD2BC2ne2q+nCbdFXds4luYEtR64uH0+7pmC42d517xLgAva3xfSnHTW0twF9CXg8V3Lv69t44PAMpqT04q27MC2bb9pj62bgHd0rf+XNCeETe3x+wOaHn6v2vhUmmBeP3Y8THRs03TSrmyXHaEZOjq2o3w+Tcfurva4vJ3m4veCXtV3aw8/LliqVJLzgKeVUl446LpoMByikSrRDv19h+ZdwFE092f37J+ANPPYg5cqkeRLNHdszAduBT5RSvn0QCulgTLgJalS/qOTJFXKgJekShnwklQpA16SKmXAS1KlDHhJqpQBL0mVMuAlqVIGvCRV6v8DVlhy6COdJzwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_real = np.argmax(y_test, axis=1)\n",
    "confusion_mat = confusion_matrix(y_real, y_pred)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_mat)\n",
    "print()\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_real, y_pred, target_names=y_list))\n",
    "print()\n",
    "\n",
    "# labels = y_table.T[0]\n",
    "plt.figure(figsize=(4,4), dpi=100)\n",
    "plt.xticks(np.arange(len(y_list)), y_list)\n",
    "plt.yticks(np.arange(len(y_list)), y_list)\n",
    "plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.bone_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
