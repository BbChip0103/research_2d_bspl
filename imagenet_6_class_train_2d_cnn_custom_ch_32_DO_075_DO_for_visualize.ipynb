{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical, multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(path.join(data_dir, 'imagenet_6_class_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'imagenet_6_class_val_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'imagenet_6_class_val_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4680, 341, 341, 3),\n",
       " (4680,),\n",
       " (1560, 341, 341, 3),\n",
       " (1560,),\n",
       " (1560, 341, 341, 3),\n",
       " (1560,),\n",
       " (6, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_data']\n",
    "y_train = train_data['y_data']\n",
    "x_val = val_data['x_data']\n",
    "y_val = val_data['y_data']\n",
    "x_test = test_data['x_data']\n",
    "y_test = test_data['y_data']\n",
    "y_table_array = test_data['y_table_array']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4680, 341, 341, 3),\n",
       " (4680, 6),\n",
       " (1560, 341, 341, 3),\n",
       " (1560, 6),\n",
       " (1560, 341, 341, 3),\n",
       " (1560, 6))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bed', 'bird', 'cat', 'dog', 'house', 'tree']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list = [text for _, text in y_table_array]\n",
    "y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train[0].shape\n",
    "output_size = len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    for i in range(conv_num):\n",
    "        x = Conv2D(kernel_size=5, filters=32*(2**(i//2)), strides=(1,1), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling2D(pool_size=3, strides=(3,3), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "  \n",
    "    x = Dropout(0.75)(x)\n",
    "    output_layer = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 415872)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 415872)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 2495238   \n",
      "=================================================================\n",
      "Total params: 2,497,670\n",
      "Trainable params: 2,497,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 46208)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 46208)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 277254    \n",
      "=================================================================\n",
      "Total params: 305,318\n",
      "Trainable params: 305,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 38, 38, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 64902     \n",
      "=================================================================\n",
      "Total params: 144,230\n",
      "Trainable params: 144,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 38, 38, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 13, 13, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 9606      \n",
      "=================================================================\n",
      "Total params: 191,398\n",
      "Trainable params: 191,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 38, 38, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 13, 13, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 5, 5, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 389,798\n",
      "Trainable params: 389,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model = build_2d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4680 samples, validate on 1560 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.5915 - acc: 0.3497\n",
      "Epoch 00001: val_loss improved from inf to 1.31431, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/001-1.3143.hdf5\n",
      "4680/4680 [==============================] - 19s 4ms/sample - loss: 1.5909 - acc: 0.3496 - val_loss: 1.3143 - val_acc: 0.5160\n",
      "Epoch 2/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.2506 - acc: 0.5235\n",
      "Epoch 00002: val_loss improved from 1.31431 to 1.14875, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/002-1.1487.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.2501 - acc: 0.5237 - val_loss: 1.1487 - val_acc: 0.5641\n",
      "Epoch 3/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.0933 - acc: 0.6019\n",
      "Epoch 00003: val_loss improved from 1.14875 to 1.04478, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/003-1.0448.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.0925 - acc: 0.6024 - val_loss: 1.0448 - val_acc: 0.6019\n",
      "Epoch 4/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.0135 - acc: 0.6355\n",
      "Epoch 00004: val_loss improved from 1.04478 to 1.00011, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/004-1.0001.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.0135 - acc: 0.6353 - val_loss: 1.0001 - val_acc: 0.6186\n",
      "Epoch 5/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9492 - acc: 0.6650\n",
      "Epoch 00005: val_loss improved from 1.00011 to 0.93625, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/005-0.9362.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.9495 - acc: 0.6647 - val_loss: 0.9362 - val_acc: 0.6558\n",
      "Epoch 6/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8900 - acc: 0.6903\n",
      "Epoch 00006: val_loss improved from 0.93625 to 0.91303, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/006-0.9130.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8907 - acc: 0.6902 - val_loss: 0.9130 - val_acc: 0.6679\n",
      "Epoch 7/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8398 - acc: 0.7145\n",
      "Epoch 00007: val_loss improved from 0.91303 to 0.90733, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/007-0.9073.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8393 - acc: 0.7150 - val_loss: 0.9073 - val_acc: 0.6699\n",
      "Epoch 8/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8087 - acc: 0.7202\n",
      "Epoch 00008: val_loss did not improve from 0.90733\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8094 - acc: 0.7203 - val_loss: 0.9151 - val_acc: 0.6686\n",
      "Epoch 9/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7700 - acc: 0.7348\n",
      "Epoch 00009: val_loss improved from 0.90733 to 0.84409, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/009-0.8441.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7701 - acc: 0.7348 - val_loss: 0.8441 - val_acc: 0.6929\n",
      "Epoch 10/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7369 - acc: 0.7451\n",
      "Epoch 00010: val_loss did not improve from 0.84409\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7362 - acc: 0.7455 - val_loss: 0.8813 - val_acc: 0.6718\n",
      "Epoch 11/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7068 - acc: 0.7583\n",
      "Epoch 00011: val_loss improved from 0.84409 to 0.83109, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/011-0.8311.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7061 - acc: 0.7585 - val_loss: 0.8311 - val_acc: 0.7013\n",
      "Epoch 12/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6790 - acc: 0.7708\n",
      "Epoch 00012: val_loss improved from 0.83109 to 0.83095, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/012-0.8309.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6785 - acc: 0.7709 - val_loss: 0.8309 - val_acc: 0.7154\n",
      "Epoch 13/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6659 - acc: 0.7744\n",
      "Epoch 00013: val_loss improved from 0.83095 to 0.79848, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/013-0.7985.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6652 - acc: 0.7748 - val_loss: 0.7985 - val_acc: 0.7077\n",
      "Epoch 14/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6440 - acc: 0.7795\n",
      "Epoch 00014: val_loss did not improve from 0.79848\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6443 - acc: 0.7795 - val_loss: 0.8305 - val_acc: 0.6968\n",
      "Epoch 15/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6142 - acc: 0.7902\n",
      "Epoch 00015: val_loss did not improve from 0.79848\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6143 - acc: 0.7904 - val_loss: 0.8080 - val_acc: 0.7250\n",
      "Epoch 16/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5899 - acc: 0.8014\n",
      "Epoch 00016: val_loss improved from 0.79848 to 0.79765, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/016-0.7976.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5895 - acc: 0.8015 - val_loss: 0.7976 - val_acc: 0.7218\n",
      "Epoch 17/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5642 - acc: 0.8110\n",
      "Epoch 00017: val_loss improved from 0.79765 to 0.78941, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/017-0.7894.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5644 - acc: 0.8109 - val_loss: 0.7894 - val_acc: 0.7179\n",
      "Epoch 18/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5583 - acc: 0.8119\n",
      "Epoch 00018: val_loss improved from 0.78941 to 0.77098, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/018-0.7710.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5582 - acc: 0.8120 - val_loss: 0.7710 - val_acc: 0.7353\n",
      "Epoch 19/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.8185\n",
      "Epoch 00019: val_loss did not improve from 0.77098\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5400 - acc: 0.8186 - val_loss: 0.7746 - val_acc: 0.7308\n",
      "Epoch 20/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.8283\n",
      "Epoch 00020: val_loss did not improve from 0.77098\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5194 - acc: 0.8286 - val_loss: 0.7804 - val_acc: 0.7346\n",
      "Epoch 21/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4943 - acc: 0.8343\n",
      "Epoch 00021: val_loss did not improve from 0.77098\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4945 - acc: 0.8344 - val_loss: 0.7857 - val_acc: 0.7237\n",
      "Epoch 22/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4815 - acc: 0.8388\n",
      "Epoch 00022: val_loss did not improve from 0.77098\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4831 - acc: 0.8387 - val_loss: 0.7781 - val_acc: 0.7308\n",
      "Epoch 23/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4754 - acc: 0.8412\n",
      "Epoch 00023: val_loss improved from 0.77098 to 0.75410, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/023-0.7541.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4762 - acc: 0.8406 - val_loss: 0.7541 - val_acc: 0.7346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4477 - acc: 0.8491\n",
      "Epoch 00024: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4479 - acc: 0.8489 - val_loss: 0.7647 - val_acc: 0.7372\n",
      "Epoch 25/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4465 - acc: 0.8497\n",
      "Epoch 00025: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4464 - acc: 0.8498 - val_loss: 0.7601 - val_acc: 0.7353\n",
      "Epoch 26/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8615\n",
      "Epoch 00026: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4185 - acc: 0.8611 - val_loss: 0.7859 - val_acc: 0.7314\n",
      "Epoch 27/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.8525\n",
      "Epoch 00027: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4310 - acc: 0.8526 - val_loss: 0.7838 - val_acc: 0.7321\n",
      "Epoch 28/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8602\n",
      "Epoch 00028: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4185 - acc: 0.8603 - val_loss: 0.7850 - val_acc: 0.7263\n",
      "Epoch 29/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8660\n",
      "Epoch 00029: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4013 - acc: 0.8660 - val_loss: 0.8222 - val_acc: 0.7090\n",
      "Epoch 30/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8731\n",
      "Epoch 00030: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3885 - acc: 0.8729 - val_loss: 0.7567 - val_acc: 0.7365\n",
      "Epoch 31/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3605 - acc: 0.8833\n",
      "Epoch 00031: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3607 - acc: 0.8831 - val_loss: 0.8138 - val_acc: 0.7077\n",
      "Epoch 32/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8729\n",
      "Epoch 00032: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3568 - acc: 0.8726 - val_loss: 0.8296 - val_acc: 0.7173\n",
      "Epoch 33/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8765\n",
      "Epoch 00033: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3684 - acc: 0.8765 - val_loss: 0.7763 - val_acc: 0.7436\n",
      "Epoch 34/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8861\n",
      "Epoch 00034: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3391 - acc: 0.8861 - val_loss: 0.8180 - val_acc: 0.7276\n",
      "Epoch 35/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8898\n",
      "Epoch 00035: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3307 - acc: 0.8900 - val_loss: 0.8038 - val_acc: 0.7301\n",
      "Epoch 36/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3295 - acc: 0.8917\n",
      "Epoch 00036: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3292 - acc: 0.8919 - val_loss: 0.7876 - val_acc: 0.7327\n",
      "Epoch 37/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8921\n",
      "Epoch 00037: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3242 - acc: 0.8919 - val_loss: 0.8168 - val_acc: 0.7365\n",
      "Epoch 38/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9022\n",
      "Epoch 00038: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3071 - acc: 0.9024 - val_loss: 0.7994 - val_acc: 0.7340\n",
      "Epoch 39/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.8953\n",
      "Epoch 00039: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2957 - acc: 0.8955 - val_loss: 0.8252 - val_acc: 0.7397\n",
      "Epoch 40/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9000\n",
      "Epoch 00040: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2936 - acc: 0.9000 - val_loss: 0.7950 - val_acc: 0.7333\n",
      "Epoch 41/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9167\n",
      "Epoch 00041: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2681 - acc: 0.9167 - val_loss: 0.8133 - val_acc: 0.7378\n",
      "Epoch 42/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9088\n",
      "Epoch 00042: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2741 - acc: 0.9090 - val_loss: 0.8052 - val_acc: 0.7385\n",
      "Epoch 43/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.9075\n",
      "Epoch 00043: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2706 - acc: 0.9073 - val_loss: 0.8486 - val_acc: 0.7154\n",
      "Epoch 44/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9090\n",
      "Epoch 00044: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2724 - acc: 0.9085 - val_loss: 0.7865 - val_acc: 0.7378\n",
      "Epoch 45/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9199\n",
      "Epoch 00045: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2535 - acc: 0.9201 - val_loss: 0.8017 - val_acc: 0.7372\n",
      "Epoch 46/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.9170\n",
      "Epoch 00046: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2468 - acc: 0.9171 - val_loss: 0.8378 - val_acc: 0.7397\n",
      "Epoch 47/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9140\n",
      "Epoch 00047: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2468 - acc: 0.9141 - val_loss: 0.8325 - val_acc: 0.7397\n",
      "Epoch 48/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9165\n",
      "Epoch 00048: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2451 - acc: 0.9167 - val_loss: 0.8328 - val_acc: 0.7378\n",
      "Epoch 49/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9274\n",
      "Epoch 00049: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2289 - acc: 0.9271 - val_loss: 0.8736 - val_acc: 0.7269\n",
      "Epoch 50/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9170\n",
      "Epoch 00050: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2436 - acc: 0.9171 - val_loss: 0.8070 - val_acc: 0.7417\n",
      "Epoch 51/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9262\n",
      "Epoch 00051: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2231 - acc: 0.9259 - val_loss: 0.8202 - val_acc: 0.7449\n",
      "Epoch 52/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9240\n",
      "Epoch 00052: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2190 - acc: 0.9241 - val_loss: 0.8469 - val_acc: 0.7423\n",
      "Epoch 53/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9285\n",
      "Epoch 00053: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2099 - acc: 0.9286 - val_loss: 0.8508 - val_acc: 0.7436\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9300\n",
      "Epoch 00054: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2067 - acc: 0.9299 - val_loss: 0.8784 - val_acc: 0.7372\n",
      "Epoch 55/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9279\n",
      "Epoch 00055: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2158 - acc: 0.9280 - val_loss: 0.8537 - val_acc: 0.7449\n",
      "Epoch 56/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9416\n",
      "Epoch 00056: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1877 - acc: 0.9417 - val_loss: 0.8547 - val_acc: 0.7365\n",
      "Epoch 57/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9317\n",
      "Epoch 00057: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2038 - acc: 0.9316 - val_loss: 0.8781 - val_acc: 0.7288\n",
      "Epoch 58/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9347\n",
      "Epoch 00058: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1979 - acc: 0.9346 - val_loss: 0.8593 - val_acc: 0.7397\n",
      "Epoch 59/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9439\n",
      "Epoch 00059: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1770 - acc: 0.9440 - val_loss: 0.8754 - val_acc: 0.7474\n",
      "Epoch 60/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9347\n",
      "Epoch 00060: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1855 - acc: 0.9348 - val_loss: 0.8546 - val_acc: 0.7423\n",
      "Epoch 61/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9429\n",
      "Epoch 00061: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1778 - acc: 0.9429 - val_loss: 0.9119 - val_acc: 0.7449\n",
      "Epoch 62/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9390\n",
      "Epoch 00062: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1911 - acc: 0.9389 - val_loss: 0.8402 - val_acc: 0.7449\n",
      "Epoch 63/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9411\n",
      "Epoch 00063: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 15s 3ms/sample - loss: 0.1783 - acc: 0.9410 - val_loss: 0.8909 - val_acc: 0.7423\n",
      "Epoch 64/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9407\n",
      "Epoch 00064: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1711 - acc: 0.9404 - val_loss: 0.8613 - val_acc: 0.7474\n",
      "Epoch 65/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9403\n",
      "Epoch 00065: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1734 - acc: 0.9404 - val_loss: 0.8868 - val_acc: 0.7442\n",
      "Epoch 66/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9454\n",
      "Epoch 00066: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1642 - acc: 0.9455 - val_loss: 0.9213 - val_acc: 0.7417\n",
      "Epoch 67/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9488\n",
      "Epoch 00067: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1611 - acc: 0.9487 - val_loss: 0.9694 - val_acc: 0.7147\n",
      "Epoch 68/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9482\n",
      "Epoch 00068: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1634 - acc: 0.9481 - val_loss: 0.9648 - val_acc: 0.7385\n",
      "Epoch 69/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9473\n",
      "Epoch 00069: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1576 - acc: 0.9474 - val_loss: 0.9238 - val_acc: 0.7404\n",
      "Epoch 70/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9527\n",
      "Epoch 00070: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1518 - acc: 0.9528 - val_loss: 0.9404 - val_acc: 0.7404\n",
      "Epoch 71/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9583\n",
      "Epoch 00071: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1359 - acc: 0.9583 - val_loss: 0.9258 - val_acc: 0.7487\n",
      "Epoch 72/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9521\n",
      "Epoch 00072: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1509 - acc: 0.9521 - val_loss: 0.9332 - val_acc: 0.7429\n",
      "Epoch 73/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9495\n",
      "Epoch 00073: val_loss did not improve from 0.75410\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1535 - acc: 0.9496 - val_loss: 0.9402 - val_acc: 0.7353\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd4VFX++PH3mckkIT0kgUAoAWkBUoCACFIsYEERRUDRVSyou6wu61dWLOuq67rW1WV/NkTsgigiYkNRIKCgtCSAgKEkkISSQHrPzOf3xyEFSAMyTMp5Pc99kpk5994zk8n9nH6ViGAYhmEYABZXZ8AwDMNoOkxQMAzDMCqZoGAYhmFUMkHBMAzDqGSCgmEYhlHJBAXDMAyjkgkKhmEYRiUTFAzDMIxKJigYhmEYldxcnYHTFRwcLOHh4a7OhmEYRrOyadOmTBEJqS9dswsK4eHhbNy40dXZMAzDaFaUUikNSWeajwzDMIxKJigYhmEYlUxQMAzDMCo5rU9BKTUfuAo4IiL9a0kzGngZsAGZIjLqTM5VVlZGamoqxcXFZ5rdVs/T05NOnTphs9lcnRXDMFzImR3N7wD/D3ivpheVUgHAq8DlIrJfKdXuTE+UmpqKr68v4eHhKKXO9DCtlohw9OhRUlNT6datm6uzYxiGCzmt+UhE4oBjdSSZCnwmIvuPpz9ypucqLi4mKCjIBIQzpJQiKCjI1LQMw3Bpn0IvIFAptUoptUkpdcvZHMwEhLNjPj/DMMC18xTcgEHAJUAbYJ1Sar2I/H5yQqXUXcBdAF26dDmjk9nthZSXZ2GztcdiaXbTMwzDMM4JV9YUUoHlIlIgIplAHBBdU0IRmSsisSISGxJS74S8GjkcJZSWHkSk5MxzXIvs7GxeffXVM9r3yiuvJDs7u8HpH3/8cV544YUzOpdhGEZ9XBkUlgIXKqXclFJewPnADmedTCk9qkakvNGPXVdQKC+v+3xff/01AQEBjZ4nwzCMM+G0oKCUWgCsA3orpVKVUncope5RSt0DICI7gG+BROBXYJ6IbHNWfiwWHRQcjrJGP/bs2bPZs2cPMTExzJo1i1WrVjFixAjGjx9P3759AZgwYQKDBg2iX79+zJ07t3Lf8PBwMjMzSU5OJiIigunTp9OvXz/Gjh1LUVFRneeNj49n6NChREVFce2115KVlQXAnDlz6Nu3L1FRUdxwww0ArF69mpiYGGJiYhgwYAB5eXmN/jkYhtH8Oa1xXURubECa54HnG/O8SUkzyc+Pr+ls2O35WCweKOV+Wsf08YmhZ8+Xa339mWeeYdu2bcTH6/OuWrWKzZs3s23btsohnvPnz6dt27YUFRUxePBgJk6cSFBQ0El5T2LBggW8+eabTJ48mcWLF3PzzTfXet5bbrmF//3vf4waNYrHHnuMJ554gpdffplnnnmGffv24eHhUdk09cILL/DKK68wfPhw8vPz8fT0PK3PwDCM1qEVzWhWgEJEzsnZhgwZcsKY/zlz5hAdHc3QoUM5cOAASUlJp+zTrVs3YmJiABg0aBDJycm1Hj8nJ4fs7GxGjdLz/W699Vbi4uIAiIqK4qabbuKDDz7AzU3H/eHDh3P//fczZ84csrOzK583DMOorsVdGeoq0efnb8Vq9aZNm+5Oz4e3t3fl76tWrWLFihWsW7cOLy8vRo8eXeOcAA8Pj8rfrVZrvc1Htfnqq6+Ii4tj2bJl/Otf/2Lr1q3Mnj2bcePG8fXXXzN8+HCWL19Onz59zuj4hmG0XK2opqA7m0Uav0/B19e3zjb6nJwcAgMD8fLyYufOnaxfv/6sz+nv709gYCBr1qwB4P3332fUqFE4HA4OHDjARRddxLPPPktOTg75+fns2bOHyMhIHnzwQQYPHszOnTvPOg+GYbQ8La6mUBeLxQ2Ho/Fn7QYFBTF8+HD69+/PFVdcwbhx4054/fLLL+f1118nIiKC3r17M3To0EY577vvvss999xDYWEh3bt35+2338Zut3PzzTeTk5ODiHDfffcREBDA3//+d1auXInFYqFfv35cccUVjZIHwzBaFnWu2tgbS2xsrJx8k50dO3YQERFR777FxSmUlWXh6xvjrOw1aw39HA3DaH6UUptEJLa+dK2u+QjKEXG4OiuGYRhNUisMCs6ZwGYYhtEStLKgoLtQnNHZbBiG0RK0sqBgagqGYRh1aVVBwZlLXRiGYbQErSoomOYjwzCMurWyoGAFLE0iKPj4+JzW84ZhGOdCqwoK4LxZzYZhGC1BKw0KjdvRPHv2bF555ZXKxxU3wsnPz+eSSy5h4MCBREZGsnTp0gYfU0SYNWsW/fv3JzIyko8//hiAgwcPMnLkSGJiYujfvz9r1qzBbrczbdq0yrQvvfRSo74/wzBaj5a3zMXMmRBf09LZmqejCMQBVu9a05wiJgZern2hvSlTpjBz5kxmzJgBwKJFi1i+fDmenp4sWbIEPz8/MjMzGTp0KOPHj2/Q/ZA/++wz4uPjSUhIIDMzk8GDBzNy5Eg++ugjLrvsMh555BHsdjuFhYXEx8eTlpbGtm36dhSncyc3wzCM6lpeUKiHQuGgcZf2GDBgAEeOHCE9PZ2MjAwCAwPp3LkzZWVlPPzww8TFxWGxWEhLS+Pw4cOEhobWe8y1a9dy4403YrVaad++PaNGjWLDhg0MHjyY22+/nbKyMiZMmEBMTAzdu3dn79693HvvvYwbN46xY8c26vszDKP1aHlBoY4SPUBZSTqlpen4+AxEqcZrPZs0aRKffvophw4dYsqUKQB8+OGHZGRksGnTJmw2G+Hh4TUumX06Ro4cSVxcHF999RXTpk3j/vvv55ZbbiEhIYHly5fz+uuvs2jRIubPn98Yb8swjFamVfYpQONPYJsyZQoLFy7k008/ZdKkSYBeMrtdu3bYbDZWrlxJSkpKg483YsQIPv74Y+x2OxkZGcTFxTFkyBBSUlJo374906dP584772Tz5s1kZmbicDiYOHEiTz31FJs3b27U92YYRuvR8moK9agKCmXA6d2Wsy79+vUjLy+PsLAwOnToAMBNN93E1VdfTWRkJLGxsad1U5trr72WdevWER0djVKK5557jtDQUN59912ef/55bDYbPj4+vPfee6SlpXHbbbfhcOiF/v7973832vsyDKN1cdrS2Uqp+cBVwBER6V9HusHAOuAGEfm0vuOezdLZAHZ7PoWFO/H07IHNFtCgfVoLs3S2YbRcTWHp7HeAy+tKoPRssmeB75yYj5POWb2mYBiGYVTntKAgInHAsXqS3QssBo44Kx8nM4viGYZh1M5lHc1KqTDgWuC1BqS9Sym1USm1MSMj4yzPawGspqZgGIZRA1eOPnoZeFAacBs0EZkrIrEiEhsSEnLWJ1bKzQQFwzCMGrhy9FEssPD47N5g4EqlVLmIfO7sE1ssjb/UhWEYRkvgsqAgIt0qfldKvQN8eS4Cgj6fDbu96FycyjAMo1lxWvORUmoBeqhpb6VUqlLqDqXUPUqpe5x1zobnrXFXSs3OzubVV189o32vvPJKs1aRYRhNhtNqCiJy42mkneasfNRE32zHjoijUZa6qAgKf/rTn055rby8HDe32j/mr7/++qzPbxiG0Vha3TIX0PhzFWbPns2ePXuIiYlh1qxZrFq1ihEjRjB+/Hj69u0LwIQJExg0aBD9+vVj7ty5lfuGh4eTmZlJcnIyERERTJ8+nX79+jF27FiKik5t4lq2bBnnn38+AwYM4NJLL+Xw4cMA5Ofnc9tttxEZGUlUVBSLFy8G4Ntvv2XgwIFER0dzySWXNMr7NQyj5Wpxy1zUs3I2ACKBOByeWCxuNGAV6/pWzuaZZ55h27ZtxB8/8apVq9i8eTPbtm2jWzfddTJ//nzatm1LUVERgwcPZuLEiQQFBZ1wnKSkJBYsWMCbb77J5MmTWbx4MTfffPMJaS688ELWr1+PUop58+bx3HPP8eKLL/LPf/4Tf39/tm7dCkBWVhYZGRlMnz6duLg4unXrxrFj9U0bMQyjtWtxQaFhKiKBc5b4ABgyZEhlQACYM2cOS5YsAeDAgQMkJSWdEhS6detGTEwMAIMGDSI5OfmU46ampjJlyhQOHjxIaWlp5TlWrFjBwoULK9MFBgaybNkyRo4cWZmmbdu2jfoeDcNoeVpcUKhn5WwAHI5yCgp24eHRFXf3s5/3UBNv76qb+KxatYoVK1awbt06vLy8GD16dI1LaHt4eFT+brVaa2w+uvfee7n//vsZP348q1at4vHHH3dK/g3DaJ1aV5+CCIg0+lIXvr6+5OXl1fp6Tk4OgYGBeHl5sXPnTtavX3/G58rJySEsLAyAd999t/L5MWPGnHBL0KysLIYOHUpcXBz79u0DMM1HhmHUq/UEhaws2LIFSksbfamLoKAghg8fTv/+/Zk1a9Ypr19++eWUl5cTERHB7NmzGTp06Bmf6/HHH2fSpEkMGjSI4ODgyucfffRRsrKy6N+/P9HR0axcuZKQkBDmzp3LddddR3R0dOXNfwzDMGrjtKWzneWMl87Oz4edO6FHDwgIID9/G1ZrG9q0Oc+JuW1ezNLZhtFyNYWls5uWNm30z8JCACwWs/6RYRjGyVpPULBawdOzMigoZcPhMOsfGYZhVNd6ggLo2kK1oGBqCoZhGCdqXUHBywtKS6G8/PgIJL3UhWEYhqG1vqAAUFR0fP0jc1tOwzCM6lpnUCgsNPdqNgzDqEHrCgo2m94KC7FYdFBwVWezj4+PS85rGIZRl9YVFEB3NhcVVasplLo4Q4ZhGE1H6wsKXl46KOAGWHE4zv4ObLNnzz5hiYnHH3+cF154gfz8fC655BIGDhxIZGQkS5curfdYtS2xXdMS2LUtl20YhnGmWtyCeDO/nUn8oTrWzi4vh6IiSPDGQTEiYLV61XnMmNAYXr689pX2pkyZwsyZM5kxYwYAixYtYvny5Xh6erJkyRL8/PzIzMxk6NChjB8/HlXHet01LbHtcDhqXAK7puWyDcMwzkaLCwr1shyvHNntekIbZ998NGDAAI4cOUJ6ejoZGRkEBgbSuXNnysrKePjhh4mLi8NisZCWlsbhw4cJDQ2t9Vg1LbGdkZFR4xLYNS2XbRiGcTacFhSUUvOBq4AjItK/htdvAh5E39wgD/ijiCSc7XnrKtEDeqXULVsgOJiyDj4UF+/Fy6tvvbWF+kyaNIlPP/2UQ4cOVS489+GHH5KRkcGmTZuw2WyEh4fXuGR2hYYusW0YhuEszuxTeAe4vI7X9wGjRCQS+Ccwt460jUepys5mi0UHAru94KwPO2XKFBYuXMinn37KpEmTAL3Mdbt27bDZbKxcuZKUlJQ6j1HbEtu1LYFd03LZhmEYZ8NpQUFE4oBaF/AXkZ9FpOIqth7o5Ky8nMLL6/iwVA90Z3PhWR+yX79+5OXlERYWRocOHQC46aab2LhxI5GRkbz33nv06dOnzmPUtsR2bUtg17RctmEYxtlw6tLZSqlw4Muamo9OSvcA0EdE7qzl9buAuwC6dOky6OQS92kv+ZyRASkpEBlJoT0ZEQfe3mbJaLN0tmG0XM1m6Wyl1EXAHej+hRqJyFwRiRWR2JCQRrh9ZrWZzRaLFw5HIc3tvhKGYRjO4NKgoJSKAuYB14jI0XN2Yk9P/bOo6HgHszTKfAXDMIzmzmVBQSnVBfgM+IOI/H62xzutkn61eytUdTaffb9Cc2ZqSoZhgHOHpC4ARgPBSqlU4B+ADUBEXgceA4KAV49P5ipvSHtXTTw9PTl69ChBQUF1Tgw7gZcX5OdjsXgClkbpbG6uRISjR4/iWVGDMgyj1XJaUBCRG+t5/U6gxo7l09WpUydSU1PJyMho+E45OZCdDVYrpeU5QA7u7mc/NLW58vT0pFOnczcAzDCMpqlFzGi22WyVs30b7Pvv4Yor4McfSeq0lIMH32TEiFyUsjonk4ZhGM2Ay0cfucyAAfrn2rX4+g7E4SiksHCXa/NkGIbhYq03KAQHw+DB8NVX+PoOAiAvb7OLM2UYhuFarTcoAFx9Nfz6K21yA7BY2pCfv8nVOTIMw3ApExREsHyzHB+fGFNTMAyj1WvdQSE6Gjp1gi+/xMdnIPn5WxBxuDpXhmEYLtO6g4JScNVV8N13+NqisNvzKCra7epcGYZhuEzrDgqgg0JBAQHx5QDk5Zl+BcMwWi8TFC6+GNq0wXPFNpTyIC9vo6tzZBiG4TImKLRpA2PGoL78igD/UWRkfIKI3dW5MgzDcAkTFEA3Ie3fT6essZSUHODYse9cnSPDMAyXMEEBYNw4AAJ/LsRmC+HgwTddnCHDMAzXMEEBoGNHGDQIy5dfExo6jaNHl1FScsjVuTIMwzjnTFCocPXV8MsvdLBei0g5hw694+ocGYZhnHMmKFQ4PrvZa9Uu/P1HcfDgPDORzTCMVscEhQoDBkCXLvDyy3QMvo3i4j1kZ69yda4MwzDOKRMUKigFc+ZAQgIhr2zHzS3QdDgbhtHqmKBQ3TXXwB13YHn+RboeuJiMjM8oLc10da4MwzDOGacFBaXUfKXUEaXUtlpeV0qpOUqp3UqpRKXUQGfl5bS89BKEhxP28Hos+aUcPvy+q3NkGIZxzjizpvAOcHkdr18B9Dy+3QW85sS8NJyvL7z3Hpb9B4mYG0J6+uumw9kwjFbDaUFBROKAY3UkuQZ4T7T1QIBSqoOz8nNahg+H2bMJ/iID7+9/JyNjsatzZBiGcU64sk8hDDhQ7XHq8eeahn/8Axk4kF7/tbL/9ycREVfnyDAMw+maRUezUuoupdRGpdTGjIyMc3NSd3fUCy/gnmnH75NtHD365bk5r2EYhgu5MiikAZ2rPe50/LlTiMhcEYkVkdiQkJBzkjkARo9Ghg+j68dW9u82tQXDMFo+VwaFL4Bbjo9CGgrkiMhBF+bnVEqh/v4YHofteC/eSFbW967OkWEYTVFpqatz0GicOSR1AbAO6K2USlVK3aGUukcpdc/xJF8De4HdwJvAn5yVl7MydiwSO4iuH1lJ2fNPV+fGMIymZuFC8PCAdu3gwgvhttvguecgO9vVOTsjqrk1icTGxsrGjef47mhffAHXXMOO2dDhwdUEBIw8t+c3DKNpOnIE+vaFsDAYMgSSkuD33+HgQRg7Fr7+GqzWWnfPzIStWyExEQ4fhv79YdAg6NkTLI1cZFdKbRKR2PrSuTXuaVuoq69GoiPp+tFvJE16koCBK1ydI8MwXEwEsv/4MGk5ncl8YQnW88Kx2dDbF4txf/IRbA+8hu2vf8Zmg9RU2Latatu6VceOChYLOI5PifL1heho6NQJQkKqtiFDYKCTp/mamkJDffIJTJ7M9r9D2P1xBASMOPd5MIwW7LffwM0NevWqP21eHnz3HcTFQWQkXHmlvi1KdenpsHIlZGVBeDh0765/enlBeblu3Tl2DHJy9AXZZgN3d/2zeildRF+8f/utatu9G9JT7RSX1l4LqI2nJ0T0Efr3dRA1wEpUFERFQVCQPvamTXpLTIRDhyAjQ+cR4KG/FPL0y16nfU4wNYXGN3EiEtGHbh/s5vfL/kb0sJ9RSrk6V4bRrNntsGwZvPwyrF6tn+vbFyZOhOuu0xfLrCx9cTx4EHbuhC+/1Bf70lJ9AS8r0/sNHKiDQ1YW/Pgj7NhR8zm9vaGg4Mzy6+UFERFw/sBSwg6/RcegPMJeuJ/gUDdEdJ7Kyo5vecWUPvIEZXnFlD38D9r3DqB/n3K6//Q+1iceg8+PQbu7YMr9EKoHYkZH6+3220S/2Z9/hp9/pmTtBjJ/P4qHZQbw8JllvoFMTeF0fP45ct11FHYRyj74fwRcOMM1+TAMJ0pP19ejCy6ANm1qTnP4MJSUgJ+fbuqwWvWFMDlZN6knJUFamr74FhRAYaFO7+sLAQF6s1rhww9h7169av299+pS9OLFugbgcOg0dvuJ5+7ZE8aP17dAGTZMX/y/+koHi/Xr9TFGjoRLLoGLL9Y1iORkfZ59+3Q7fmAgtG2rf/r763NVXMxLS3XtoLqgIOjXD7p2PV6LuPtumDdPn3Dw4No/zB07IDZWbzNnwiOP6OeGDNFvZOFCvULzzTfDjTfq6sGaNfDTT3D0qD5G27b6jQ4bBldcATExZ/R3bWhNwQSF0+T4fjnlN4zDWuDA8v/eQN1xp/6jGkYTcPCgvqAGBkKHDhAaqi9o9XVaHjqkL8Yffwxr1+qLore3Lnlfdx1ceikkJMC33+pt20nLXPr4QFHRiRdwT0/9vJeXPpa7O+Tn62ab7Gyddtgw+OtfYcIE3XRU4cgRWLpUX8hDQ/XWoYMOHuHhtb+P3Fx9Xnf30/7oGu7HH3XEeeABeP75+tN/+KG+6AP07g1PPw3XXquvGykp8MILOsAUF+s0PXrAiBF6JNOFF+rg0QjXGBMUnChj62u4TfsTgZuBqVPhzjt1na9t26pEhw7p+vDq1bokcdttLsuv0fQVFurbebz7rr5ujB4No0bp5pM6Bq8A+mK8dKne97vvqjorK7i5Qfv2+qJasZWX67bqim3vXh0I+veHyZP11/mbb2DJEl0rqODurq9Xl12mv+65uXrLydEX/549q7bg4NqvZSK65uDpeVYfW/2OHdPF//btG+d4O3boC3VwMGzZot90Q7zyiq523XLLidGvwpEjsHGjvtlXB+csAdeoQUEp9RfgbSAPmAcMAGaLyHdnm9HT1RSCgoiDjb8MIHT+ATq9lYOq+C/s1En/VyUn6/p3BS8vPfQgMNAl+TWarvJymD8fnnhCN9uMGKF/7tmjX/f316VjP7+qTamqJpmCAn2dysnRX78//EEXQouLda2hoi2++u+HDunrUvVRLX36wPXX6/b86ux2WLdOl22io3Ww8vE55x/TmXE44PzzdbVk5876o2t90tJ0m1ppqW7r7969cfJ5jjR2UEgQkWil1GXA3cDfgfdF5JzfA6EpBAWAo0e/ZuvWcfQOfJYOh6J1W2BCgq5Xh4Xp/57Ro/UXcdAgPZll1ixXZ9twMocDfvkFFi2Czz/X5YGhQ/W1ZOhQXS5ISqrali7VbfDDhsGzz+pCKOgyxOrVuinn0KETS+RK6eNWNMt06qQrrBdddPbXvRZlwQL9wYD+Y1xzzZkfKytLd1SkpOg/zIABjZPHc6ixg0KiiEQppf4LrBKRJUqpLSJyzj+ZphIURIT4+FEUFSVx/vl7sFrrqEZefLEew7Z3b81VR6PJEtEdkykpupRd0R6ena2HRVaXl6c7PA8c0M0sl12mg8T69VV9htW5u+vS96OP6k5T0zXViEpK9DAhX1/9xzrvPN0XcLLyct3mdsklelZyTYqL9US09et1h8rFFzs3707S2ENSNymlvgO6AQ8ppXyBVn3nGaUU3bv/my1bLmT//mfo1u3J2hPPnKlLKUuWwKRJp75eVqbH1hlO8fvvsH+/bpqp7f++QlaWnoS6bJmeXJScrJtpauLhcWIHrtWqK4f/+pceHePvr58X0WWCdet0c0+PHrrNvXNnU7J3mtdf10ONvv1W1+AffFDX5qOiTkz3wgvw0EP6or9kyal9BHl5cMMNusq2YEGzDQinRUTq3dBrJA0EAo4/bgtENWTfxt4GDRokTcn27VNl1Sp3KSj4vfZEdrvIeeeJDBt26mvz5ol4e4v8+KPzMtkK/fabyJNPikRFiejLsoifn8gtt4h8+aVISYlIVpbIli0in38u8vzzIhdfLOLmptO2by8yYYLIzJkiL7+s0/z6q0hSkkhGhkhpqavfoZPZ7Y17vMREkWnTRHbubPg+H3wg8uCDIseO1fy63S6yebNIcfGJz2dniwQFiVxyiYjDIXL0qEibNiJ33HFiun379PP9+olYLCIjRojk5FS9vmePfs1qFXnjjYbnu4kCNkpDrvcNSgTDAe/jv98M/Afo2pB9G3trakGhuDhd4uL8JD7+MnE4HLUnnDNHf9y//FL13MqVVVehXr1O/XIbDVJSIrJhg/6Ip04V6dZNf6RKiVx4ob6oL10qctttIgEB+jWrtSpYVGwRESKzZ4usW9f418Rm5cUXRcLCRNLTG+d4P/6oIzKI+PrqCFuX/HyRW2+t+sOEhIi8/XbVH8XhEPnsM5H+/fXr/frpL0CFhx/Wz2/cWPXc3XeLeHrqiF5xjKuu0gWy/ftFPv5Y/y/GxopkZuo8t20rEhgo8v33jfM5uFhjB4VEQAHRwBZgBrC6Ifs29tbUgoKIyIEDL8vKlciRI5/Wnig3V/9j3HijfpyUpL90ERH6CwkiTzxxbjLcRDkcuuT+/fciBQU1pyksFFm9WuS//9UX+QEDRNzdq64fHTqIXHedyP/+J5KWdur+JSUiX30l8re/6drBJ5/o68mRI859b83GypW61Ay6lH62Fi7Uf6CICJG1a/VFF0T+/veaI+/27SJ9++qI/thj+sJ+wQV6n+HDRd59t+oYvXqJPPOMSMeOOso//LDI3r269D916qnHBZF//Us/XrJEP37++ao0y5aJeHjoUoWbm85zUtLZfwZNRGMHhc3Hfz4G3FH9uXO9NcWgYLeXya+/RsvPP3eSsrK82hPef7/+sm3bJtKnj67i7t6tX5syRX8hf6+jGaqFOnBA/2/361d1cffwELn0Uv0/++WXIo8+qkv91QNAu3YiY8aIPPCAjqv79+vAYpyhgwd1u1nv3roU7eenm2LO1Isv6j/UiBFVTUBFRTqag8iVV4rMny/y6qsi//mPyCOPiHh56ZrBd99VHcduF3nrLZHgYL1f1656v7Iy/XpWlm6aAl0bcHfXweFkY8fqAHLsmEjnzrqmcXI74IoVIj4++v1Xb0pqARo7KKwGHgKSgNDjfQxbG7JvY29NMSiIiGRn/yQrVyK7d/+t9kR79+pSmK+viM2mi7wV0tP1P+Gll7a4K5vDIbJjhy69T5qk3+KIESJDhuj/S6X0N/GCC/T14auvRP761xODhNUqMniwDgBffKGvX63S++/rD6CxlZWJjB6tS9lbt4ps2qQ/+Geeadj+v/4q8vjj+uI8erRIeLje//rrdSCozuHQf2ibreoPXLFdfHHNVTwR3Tfw7be1N7N++aVIly66BFGTr77S54iJ0T/Xrq05XUFBi/sfFGl4UGgFYSilAAAgAElEQVTokNRQYCqwQUTWKKW6AKNF5L0z7eA+U01lSGpNdu68ncOH3yc2Nh5v7341J5o4ET77TE9rv+OOE1975RX485/1tPiK8dXNgIieFbtnj57fU33Y5oEDeiRgaqpOGx6uJ2x6eFRtAwboVQB69Dj12Glpejz/oEF6dGGr9sMPMGaMnsGWnKwXEGqIjAw9/rViOFRNHnlEL7/wzjtw6636ubFj9Yid5OS6px6//TbcdZee6daxo/4jh4fr9X3+/Ofa19g4elSve+HpqTcPj7Of4lxxPatpfK/DoYep/v67/t+bN+/sztXMNPoyF0qp9kDFyk+/isiRs8jfGWvKQaG0NINff43AavUmJuZH2rQ579REGRl6OvsVV5z6mt2uZzmlpOh/NDc3/eW2WPRVsbaLgIieBhsW1rhvqJbTJCToLT4edu3SwSA//9T0FoueLTtihL6WXXppE5oEWl7evOaMZGbqSQ1ubnp87WOP6WnQ9Vm6VBdE7HY9y61fP721basvkg6HnhH30kunXigr1vh54w190T+Zw6Hz8a9/6XSffNL0Z+2//74OfmvX6kWhWpGGBoWGNh9NBlKAd4H3gH3A9Q3Zt7G3ptp8VCE3d5OsWRMkP/3UQfLzfzv9A2zeXHO1unt3kdTUU9Pb7VVttDNnNvoIprIy3cz6xz/q5tjqWerWTWTcOJH77tMdv19+qUceHjggkpd3DmvgpzNUKD5ejzX18Ki9+cBZfvrpzEayOBwi11yj28q3bNE96X5+tQ/VrLBhg26jj43VzUA336x75j08Tv1+jRihe/FPPm9srEiPHiLl5Se+VlQkcsMNet877mgFY3SbPxq5TyEBaFftcQiQ0ID9Lgd2oe/DPLuG17sAK9EjmhKBK+s7ZlMPCiIieXlbZe3a9rJ2bYjk5cWf/gFSUvQFZO1akTVrRD79VPdD9OkjcvhwVTqHQw+1A5GRI/XP6Gg9SP8MlZfrJuW33tLNw23b6sN6eYlMnKiHfcbFnV3/Y6Pas0d3Gv7733Wn27pVt2+DiL+/7rSMiTn1Ylfhm2/0ONaGBhyHQ2TVKh0Na3ptzhzdMaKU/nBPx2uv6Xz/5z/6cUKCfvzYY7Xvk5wsEhqqO2VP7oApL9fDsMrK9PurK3p/+qk+16JF+nFZmcjixSKDBlX1ObTA9veWqLGDwtaTHtfb0QxYgT1Ad8D9eGDpe1KaucAfj//eF0iuLy/NISiIiBQU7JKff+4ka9YESk7Or2d/wLg43QkYHa073BwOXUQHPbje4dBD6oKDdbq5c2u9oKWmirz+uu7MveceffGfMkVk1Cg98KKi8BgYqAuXS5bUPkTUpbKz9fDFiplpWVk1p5s7V1+MfX31UMhjx0QWLND71TQp6aefqiYy9Oql05zcWVqd3a4/SNAX4nnzqoJNaWlV4L76apHLLtO/v/baiccoLxd5803dm37LLSLvvKOHU23frkfUXHbZiX/PiRNrry1kZ+sefH9/PdLtbJSXi/TsqQPo00/rAAy6Q/fTOoZgG01OYweF54HlwLTj2zfAs/XscwGwvNrjh4CHTkrzBvBgtfQ/15eX5hIUREQKC/fJunXdZO3aYCkqSjn7A373nW5CGDJE5C9/0X++v/71xJJaeroe3gP6H/ivfxXHz+skfotDHn9cZODAqou+t7ce/de5s772nX++yIwZIu+9J7JrVxOfwFVWJnL55XqI73/+IyeMQa/u2DEd3UaN0pOSKjgcuskkOPjEC2tmpv5AunfXH0TFB9a+vb4onlwTKC3VY+JB5K67qsbUR0XpCVajR1cFbrtdB5errtLP/fe/+hjr1lWVvCMjq4ZeVgyxDAk5tbRfUVv4+99PfL6oSI/TdXNrvElXb75ZlZ9LLtGlhNpqWEaT1ahBQR+PieiZzP8Brm1A+uuBedUe/wH4fyel6QBsBVKBLGBQfcdtTkFBRKSgYKfExfnKxo2xUl5eR2mzob74omoW9IwZNVfd7XaRjz6SfZfcIf+yPip92SYgorDLsN6Z8sw/CmT79mZe668IihUl/Suu0BfPk6s0s2bpWkJ8Dc14W7boIcJ/+Yt+7HDo0rzNVjVD1uEQ+eGHqhJ+SIgOQkVFerv6av18RfOVw6GbWiqmVbu76+BSXUmJ7heAqqDRsaPIhx/q/e123Tnz8st6DG9tS6Bcf72uLRw9qt/3yy9XdfzMm3dmn2tNSkv157x9e+Md0zjnGj0onO7WwKBwP/B/UlVT+A2w1HCsu4CNwMYuXbo47UNzloyMz2XlSmTHjtvqXgqjob76Srfl1lCUP3JEDwEfPryqcHdhr0PyetQrcsSrq37CYtHNFM89d3Yd03l5dTernImyMt2nkptbe9R6/XWp7FivEBenn5szp+q5lBTdqXrLLbWf7+67dVPR9u1Vk60qSvAnW7dOl5RBLwMxZIj+/ZVXTk1bXKwvpNWXX6iutFTPbrfZ9Mzhmvoi6pOYWFV6b9dO/z5qlA5ihnGSRgkK6Jvq5Naw5QG59ezbkOaj7UDnao/3Vu/QrmlrbjWFCnv3PiorVyKpqa826nFLS3VTz3vv6cJyRVN4RIRuTdm3r1rikhLdcf3441XNHD166GFD9Z0kMVGfZNYsPRO1a9eqkvCIEXrC0Hff6XVrztTevVXr2YCuEbVrp5tyOnTQpeKKWtIVV1TNaK1w4YW66aekRD++9VYdFFLqaLrLyNALIkVF6WNPmFB/FerHH/Xn5+amJ5OdKYdDB7+zMWmS/jzGjtWB0TBq0dCg4LTbcSql3IDfgUuANGADMFVEtldL8w3wsYi8o5SKAH4AwqSOTDXleQp1EbGzdevVZGWtICZmFf7+wxq8b2Ghnm+TkqLnESUn67kBu3bpWzSUl+t0Xbroe3/feKNeIbje9fm/+w7uu08f6Kqr9ML++fl6HPyBA3rp4cREfeOg0lK9j7u7vk1XxXj37Gx905HNm/VY+KAgPfnusstO7wNas0bfDLi8XI99t9v1rRSPHdN5atNG31HG21vfCvHOO0+dzfbNN/qmwm+/rWfEDRig76P73HN1n/t//9OfQ9eu+haLDRlrL6KXVfbzO7332djy8/XfKiLCtfkwmrxGnadwphtwJTow7AEeOf7ck8D447/3BX5Cj0yKB8bWd8zmWlMQESktPSbr1p0nP/0UKoWFNazNUo3DoVsepk/XHcLVh5R7e+sC9cSJIg89pAeqbNhwhh3DJSW6Gan6sKPqq8uNHatXj/voI93EcnLpvEJurh7GGRmp2/Aff/zUDJWX6+UQdu06cVz7vHm6GaVXL/3amXI49CiZ3r11H0BgYP1j+UX0e3rkEd15axgtFK6uKThLc60pVCgo2M6WLSOw2YIYMGAt7u4n3lA8L08XtOfOrbov+A036AnQFasHBAU54S5d6em6xN+hg777S6dO9d+RpiaFhfDHP8J778Hll+sZpLt2wcKFesZrxV3grVY9vbl9ez27dMwY+Pjjs58Ru2gRTJmif3/+eV1TMAyj8Ze5aCqae1AAyMn5mYSES/Hy6kNMzCrc3PzYuRNefVUvPZOXp1c0uPtuvQRSXUvWNEki8OabcO+9uhnIbtdr2lx1lW4iKi3V7WG//65vSTZmjF56oDGWnbDb9d3ni4t1MDrbtXQM4xwQEQrKCvBx93HaOUxQaOKOHv2G9etvZvPmB1i16m/8+KMVd3d9t84ZM/RN3pv9PXs3boT58/Xd6K+++tytaLd/v16XJzy8QclFhG92f0N0+2jC/Jy7flRzlZqbSnJ2Mul56aTlpnEw/yAeVg9CvEMI8QohxDsEd6s7hWWFFJQWUFhWiM1qY2CHgfRo2wOLqloUL7s4m7X71/JL6i90D+zOmPPG0MmvU4Py4RAH249sZ3XKalanrGZj+kZCfULpG9yXviF9iQiJwKqsHCk4QkZhBhkFGRSWnXg/VTeLG8FewZV59/XwZc+xPfyW8Ru/Zf7GrsxdeLp5EuYXRphvGB19OxLqE1r5PkO8Qjiv7Xn4eZxdf9Kh/EOs2LuC7/d+z4q9K0jPS6ezX2eiQ6OJaR9DTGgMw7sMJ9Qn9KzOU8EEhSaquBi++EK3pnz9tZ2SEisdOx7hT38KYvp0K+3auSZfhWWFbDuyjS7+XWjv3R51UkQqLi8mPS+dTn6dcLe613s8ESG7OJsDuQf0P2hBBhmFGRwtPIqnmycBngGVWwffDnT170qAZ8Ap5z0Tm9I34RAHg8MG15u21F7K3V/ezTvx7+Bh9eBPg//E7Atn0867YX+InOIcUnJSSM5OJi03DR93n8oLR7BXMGl5aSQcSiDhcALxh+JxiINBHQYxOGwwgzsOpmtAV/Zl7SPpWBJJR5NIzk6m2F5Mqb2UMnsZDnFwa/StXNPnmhrPv3z3cpbsXIJVWbFZbdgsNpRSHC08qi+KhRlkFmZid9hP2K+NrQ0BngH4e/gT4BlAz7Y9mdJ/Cn1D+p6Qbk3KGp5e+zTf7v72hOfdre6UO8pxSP23avf38GdQx0GcF3gemw5uYsvBLQgnXnd6B/VmTPcxXNztYkZ2HUmQV9VidXaHndUpq/lo60cs3bWUzMJMALr4d+H8sPPJLMxke8Z2jhScukanm8UNb5v3Cd+rUnvpKYECoI1bGyJCIugT3IdSeylpuWmk56WTnpdOmaPslPe0ePJiLul+ySnHSctN45EfHyE5O5ns4mxySnLILs6mzH7iMQrKCgAIahPEpd0vpX+7/uzM3En8oXh2Zu7ELvpv1iuoF6O6jmJ0+GguCr+IDr4d6vy8a2OCQhOTnQ2vvQb//a9uVu/QASZPhosuWoSf3xTatbueiIgPsVjqv+A2JrvDznsJ7/HoykdJz0sHwNfdlx5te9DZvzOH8w+TkpPCofxDAAR6BnJ93+uZGjmVkV1HYlEWsoqyWJ+6np8P/EzC4QSSs5NJzk4mrzTvtPLi5+FHV/+utG3TljJHGWX2Mkrtpbhb3enXrh/R7aOJCY0hun00gW1q7nv4MPFDblt6GwAfXPcBk/tNrvV8RwuPct2i64hLiePB4Q9ypOAI7ya8Sxu3Ntx3/n1c3O3iymBWPbBVlEAP5R8ipySnQe8t0DOQ6NBoLMrCxvSN5Jbk1pguxCsEb3dvbBYbNquN3JJcUnNTuXfIvTw/5nk83HQ/T3F5MQ9+/yBzfp2Dn4cfNout8jMThLZt2laWbIO9grFZbJXnEISisiKyi7Mrt33Z+3CIg6j2UUztP5UebXvw0vqX+OnAT4R4hXDvkHsZEjaEjr4dCfMLI9AzEIc4yCrOIqNAfz5ljjK8bd54u3vjZfOioLSAjekb2Zi+kQ3pG9h9bDcDOgxgVNdRjOo6ivM7nc/uY7srS8txKXGVF+vIdpGMDh+NRVlYtH0RB/MP4uPuwzW9r+HS7pcyOnw04QHhJ3x2mYWZ7MzciUIR4h1CO+92+Hv411jQKCwrrPx75pbk0i2gG10Dup5Qo6ngEAfZxdknfBf+seof7MzcyZtXv8m0mGmVaVfsXcHUxVMpLCtkUMdBlQUffw//UwpT7bzbcWn3S4kJjTnlvMXlxSQeTiQuJY7VKatZk7KGnJIc7h96Py9e9mLtX7Q6mKDQBJSX63sBvP02vP667isYO1b3fV58se5rBThw4CX27LmfoKCr6dfvEyyWM+jgrWbLwS388as/opSio29HOvrof+TOfp0JDwgnPCCcDr4d+HHfjzzw3QMkHE7g/LDzmTl0JhkFGbrUeiyJ1NxUQn1C6erflfCAcEJ9QlmVvIrPd35OQVkBYb5h+Hv681vGbwBYlZWIkAi6B3av3KeLfxfaebejnXc7QrxCCGwTSKm9tPJilFWURXpeemVpu6J0ZbPacLe6Y7PYKCovIvFwYmVJUKG4JfoWnrr4qcpmBxHh2Z+e5aEfHuKi8Isoc5Tx0/6feP2q17lr0KnLPu/K3MW4j8aRmpvK/GvmMzVyauXzT6x+goXbFp5QmrUoC0FtgvT7OF4TaOfdjvCA8Mr3GuYXRkFpQWXQyCjMoL13e6JDo+ns17ny4uQQB0lHk9iQvoHU3FS6B3anZ9ue9GjbA1+PE5vYSspLmL1iNi//8jIDOwzk4+s/ptReyo2LbyTxcCL3DbmPZ8c8i6fb2fWdHMo/xKLti1iwbQHrU9cDuiQ+a9gsbh9wO142r7M6fkOUlJewIX0Dq5N109BPB36i3FHOuJ7jmBo5lXE9x9HG1sbp+WiInOIcrv/kelbsXcGjIx7l8dGP81TcUzyx+gn6hvTl08mf0ie4T6Odz+6wk3A4AT8PP3q0reHGIw1ggoILOBw6AKxapYf279gBJSX6vgKTJ8Pf/qaHztckLe1VkpJmEBh4Gf37f4bVqv8Jc0tyeT/hfcocZYzqOoqo9lFYLdZa87By30quWXgNfh5+RIREVLYBn1yirShZhgeE88wlzzC53+QGN90UlhWybNcyPt6uL1DDOg9jWOdhDO44GG937wYd40wcyj9EwqEElu9ZzisbXsGqrPzfBf/HA8Me4OEfHubVja9yY/8befuat7GLnUmfTOLrpK95+uKnmX3hbOxiZ03KGpbuWso78e/gbnXn8xs+Z1jnU+eM/H70dw7lH6osbQd6Btb5uTvbF7u+YNrn0yhzlGF32PFx9+Hta95mXK9xjX6uvVl72ZW5i0u7X4rNaqt/BycptZdS7ig/JwHpTJTZy7jny3uYHz+fzn6dOZB7gD9E/YHXxr3m1P+DM2WCwjmWmwu33KLvaRIWBpGR0L+/3kaOhG7d6j/GwYPz2bXrTgICRhHY5Q1e2TSPNza9cUJTg7+HPyO6jmBM9zFM6jvphPbFT7Z/ws1LbqZn2558e/O3J3TeFZQWsD9n/wkl8k5+nZg+cHplk0RzkpydzEM/PMTCbQvxsHpQYi9h1rBZPHPpM5VV8TJ7GdOWTuOjrR8xOnw0CYcSyCrOwsPqwdjzxjLnijmnNEE0Zftz9nP70ttpY2vD3KvmnnHbstF4RISn1zzNC+te4LlLn+POgXc2Sr+YM5igcA7t3AnXXqubiv7zHz0S80y/F3tT3+Ler6az/DAIikl9J/HAsAcI9QmtrFavSl5F0rEkLMrCReEXMTVyKrkludy//H6GdR7GshuX1drm3tL8kvoLT615iit7XMkfB//xlNcd4mDWd7NY9NsiLgq/iAl9JjD2vLFOHfpntD4i0mSDQQUTFM6RpUvhD3/Qw+E/+QRGjao7/bYj21ifup5pMdNws5w4Lj+nOIerFlzFzwd+ZkJHxc3nhXHZ+T/g5XVqG+KOjB0s2LaABdsWsPvYbgDG9x7PwokLm0y7q2EYTYcJCk7mcMBTT8E//gGxsfDZZ3oicF0+SPyAu5bdRVF5EbEdY3l3wruVQwAzCzO5/IPLSTicwIfXfcjlnbuydetVAERGfoWf35AajykibEzfyN6svUzsO/GUQGMYhgENDwqnjr8y6pWfrzuO//EP3Y+wZk3dAaHUXsq9X9/LH5b8gcFhg5k/fj77svYx8I2BvPjzi6TmpjLqnVFsz9jO0huWMrnfZPz8zmfAgJ+xWv2Ijx9Nevo8HI6SU46tlGJw2GCm9J9iAoJhGGfN1BROU3IyXHONHl30/PMwdfphMgqPVA6xzCnJwaIsleO13SxuPPLjI/x84GfuH3o/z1z6DDarjcP5h7n7y7tZumsp7lZ33K3uLLtxGaPDR59wvtLSw2zbNoHc3PXYbO3o2PEuOna8Bw8PM/PWMIyGM81HTrBuHYwfD6VSyF0vf8ovZfNYs39Nvft527yZf838UyZSiQgfJH7Aaxtf46XLXuL8TufXuL+IkJW1grS0/3H06JcoZaVdu5s477wXcHcPbpT3ZhhGy2aCQiNbsQLG33gE25jHcfT/kPyyXHq27cm0mGn0Cup1wsxFhzgoKCuoXAcmIiSCLv5dGiUfRUV7SUv7H2lpr+DmFkivXm8QEjKhUY5tGEbL1dCgYBqhG2DpUpg0tRC3O8dRHJzI5IjJ3DngTkZ2HXnOh6G1adOdHj1eIjT0NnbunMb27dfSrt1N9Ow5B5ut7TnNi2EYLY8JCvVYsABu/oODgDunkdV2E59P/pzxvce7Olv4+EQxcOAv7N//b1JS/kl29o9ERX2Dj0+0q7NmGEYzZkYf1WH+fLjpJuh885Mc6/AJz176bJMICBUsFhvh4Y8xcOCvKGUlPv4icnNdP1zXMIzmywSFWixcqG8DHHnjQlK6PcFtMbfxwLCmeRcvX98BxMTE4ebmT0LCJeTk/OTqLBmG0Uw5NSgopS5XSu1SSu1WSs2uJc1kpdRvSqntSqmPnJmfhlq2TM9Sjhq/ml0R0xjRZQSvjXutSU9jb9OmGzExa3B3b09CwmVkZa10dZYMw2iGnBYUlFJW4BXgCqAvcKNSqu9JaXoCDwHDRaQfMNNZ+WmoH36A6+/ajd/tN5AwYDRhfmEsnry4WSwa5+nZiZiYODw9w9m69UoyMha7OkuGYTQzzqwpDAF2i8heESkFFgIn3z5qOvCKiGQBiMipt046h75cdYjL/zeD0rsiKO6yjEdGPMLmuzYT4h3iymydFg+PUGJiVuHjE8P27dezd+/DiNjr39EwDAPnjj4KAw5Ue5wKnDw7qxeAUuonwAo8LiLf4gJ7jx7gmm+G4IjO5Nb+0/n35X9vtksTu7sHExOziqSke9m//9/k528hIuIjbLbWsXKqYRhnztUdzW5AT2A0cCPwplIq4ORESqm7lFIblVIbMzIyGj0ThWWFjHlrAg5rAS/33cA7k15ttgGhgsXiQe/ec+nV6w2ysn5g06ZY8vO3ujpbhmE0cc4MCmlA9WXiOh1/rrpU4AsRKRORfcDv6CBxAhGZKyKxIhIbEtK4TTkiwu1Lb2dv4RbarvyIGRNjGvX4rtax413ExKzG4Shi8+YLTD+DYRh1cmZQ2AD0VEp1U0q5AzcAX5yU5nN0LQGlVDC6OWmvE/N0iqfXPM3H2z+GH/7NjDFX4dYCp/P5+1/AoEEb8fGJZPv269m37++IOFydLcMwmiCnBQURKQf+DCwHdgCLRGS7UupJpVTFDLDlwFGl1G/ASmCWiBx1Vp5O9vnOz3l05aNEyc3w09+4445zdeZzz8OjIzExqwgNvZ2UlKfYtm0C5eW59e9oGEar0moXxNubtZeo16LoG9KPg0+vpl9vT751SRf3uSUipKe/yu7dM1HKHR+fGHx9B+HrOwg/v+E13uXNMIzmz9xkpw4iwvRl07FarPw5ZDGpyZ5Mn+7qXJ0bSinCwmYQE7OGDh2mo5SFgwfns3PnNH79tRe7dk2ntNSlI4MNw3ChFtiCXr/5W+bz474feeOqN/jsn51o317fJ6E18fcfir//UABE7BQW/s7Bg/NIS5vDkSOfEB7+OGFhM7BYbC7OqWEY51Krqymk56Xzf9/9H6O6jmJchzv58kuYNg1srfjap5QVb+8IevR4kdjYrfj7X8CePX9l48ZosrPXujp7hmGcQ60qKIgIM76eQYm9hDevfpN337Fgt+uF7wzN27sPkZFf07//MhyOIuLjR5KUNBO7vcDVWTMM4xxoVUFh8Y7FfL7zc54c/STnBfZk3jy46CLoYfpWT6CUIjj4KmJjtxIWNoO0tP+yYUMU2dmrXZ01wzCcrNUEhWNFx/jz139mUIdB/PWCv7JjB+zbBzff7OqcNV1ubj707Pk/YmJWAYr4+NHs2fOgWUvJMFqwVhMUvt39LVnFWbw1/i3cLG4kJurnY+sdoGUEBIxi8OAEOnS4mwMHniMx8UrKyo65OluGYThBqwkKUyOnsue+PUSH6ttVJiaCmxv06ePijDUTVqs3vXu/Tq9ec8nOXsmmTYPNWkqG0QK1mqAA0MmvU+XvW7dCRAS4u7swQ81Qx47TT1hLKTX1f5SVZbk6W4ZhNJJWFRSqS0yEyEhX56J5qlhLydd3ILt338fPP4eyffskMjO/wOEodXX2DMM4C61y8lpWFhw4AFFRrs5J86XXUlpNfv5mDh16nyNHPiIj41OsVl/8/M7Hz++C49tQcx8Hw2hGWmVQ2LZN/zRB4ewopSrXTTrvvOc5dmw5x459TW7uOlJS/gU4UMqDPn3epn37G12dXcMwGqBVBoWKkUcmKDQei8VGcPBVBAdfBUB5eT55eRtITn6CHTumUlaWQadO97k4l4Zh1KdV9ikkJkJgIHTs6OqctFxubj4EBl5EVNS3BAdfx+7df2Hv3kdobqvyGkZr02prClFRoJSrc9LyWa2e9Ou3iN9//xP79z9NaelhOnW6D6vVF6vV5/hPT1dn0zCM41pdUHA4dJ/Cbbe5Oieth1JWevV6HXf3UFJSnuTQobdOeD04eCIREe9htXq5KIeGYVRodUEhORny801/wrmmlKJbtycICrqKkpL9lJfnYbfnU1y8j9TUl0hISCcychk2W5Crs2oYrVqrCwoVncxmjoJr+PkNBgaf8Jy//3B++20qmzcPJyrqW9q0CUdEyMmJIz19Lvn58YSETKRDhzvw9OzqmowbRivh1I5mpdTlSqldSqndSqnZdaSbqJQSpZTTVyJKTNR9Cf36OftMRkOFhFxHdPT3lJUdZsuWC0hOfpJff+1DfPxojh79CpstmJSUp1i/vhuJiVeSmbkUEYers20YLZLTgoJSygq8AlwB9AVuVEr1rSGdL/AX4Bdn5aW6rVvhvPPAx+dcnM1oqICAEQwYsBal3EhO/gc2WzB9+rzDsGHpDBiwmqFD99G166Pk5yewbdsENm0aRFbWD67OtmG0OM6sKQwBdovIXhEpBRYC19SQ7p/As0CxE/NSqWLkkdH0eHv3Y9CgLQwZspOBA38iNPTWys5nT8+udOv2JEOHphAR8QFlZVkkJFxKYuI4Cgq2uzjnhtFyODMohAEHqj1OPf5cJaXUQKCziHzlxHxUKiyEpCTTn9CUubsH4+XVu9bXLeGHbiQAABFpSURBVBY32re/iSFDdtK9+3Pk5PzEhg1RbN9+A9nZcWYehGGcJZdNXlNKWYD/AP/XgLR3KaU2KqU2ZmRknPE5t28HEVNTaAmsVk+6dJnF+efvplOnv3Ls2LfEx49iw4ZI0tJepbw8z9VZNIxmyZlBIQ3oXO1xp+PPVfAF+gOrlFLJwFDgi5o6m0VkrojEikhsSEjIGWdo6/Hl/01QaDnc3YPp0eMFhg1Lp3fvt7BYPElKmsHGjVEUFv7u6uwZRrPjzKCwAeiplOqmlHIHbgC+qHhRRHJEJFhEwkUkHFgPjBeRjc7KUGIieHlB9+7OOoPhKlarFx063E5s7EZiYlZhtxewefMwcnPPyfgFw2gxnBYURKQc+DOwHNgBLBKR7UqpJ5VS45113rokJkL//mBplSs+tR4BAaMYMOBn3NwCiI+/iMzML+tMX1aWzeHDCyktPfOmScNoKVRz65iLjY2VjRtPvzIhAiEhcO218OabTsiY0eSUlh5h69Zx5OVtpnv3pwkIGI2nZzg2WzvAwbFj33P48LtkZCxBpIQ2bXoRHf09np5dXJ11w2h0SqlNIlLvXLBWM6P50CE4etT0J7Qm7u7tiI5eyW+/TWbv3qq5kxaLJxZLG8rLs3BzC6RDhzvx8xtKUtIMtmy5kOjoFXh59XJhzg3DdVpNUDD3UGid3Nx8iIz8isLC3yguTqa4OJmion2Ulx8jKGgcQUFXYbF4AHqeRGLiZWzZMoLo6O/x8TFfFqP1aTVBwc8PJk0ycxRaI6UU3t798Paue20TX98BxMTEkZg4hvj4UfTq9Tpt216Om5v/OcqpYbheq+lTMIyGKi5OISFhLEVFvwNW/PyGEBg4Bj+/IVit/litPri5+eLmFojN1tbV2TWMBjF9CoZxhjw9uzJ48FZyc9dx7Nj3ZGV9T0rKU8Cpi/C5uQXh7R2Bl1cEXl59ad/+Jtzdz3wujWG4mqkpGEYDlJUdo7BwF3Z7/vEtj7KyTAoLd1FYuIPCwh2UlWVitfrSpctsOnWaaW4aZDQppqZgGI3IZmuLv/8FdaYpKNjB3r0PsW/fI6SlvUK3bv8kNPRW9ILBhtE8mGlchtFIvL0jiIz8nJiYODw8OrNr1x2sW9eVPXtmU1Dwm6uzZxgNYpqPDMMJRITMzKUcOvQWR49+A9jx9Y0lMHAsNltb3Nza4uYWiJdXz3pHRRlGYzDNR4bhQkopQkImEBIygdLSwxw+vIDDh99j//5/A9ULYopu3Z6iS5eHUEq5KruGUckEBcNwMnf39nTuPJPOnWci4jjeSX2M8vIsDhx4kX37HiE/P5E+feabzmnD5UxQMIxzSCkLbm7+xyfEdSMi4gN8fKLYu/chioqS6N//c9zd21NQ8Bv5+VsoKNiKxdIGD49OeHh0xsOjExaLO+XlOZSX52C352CztSMgYLSpaRiNwgQFw3AhpRRdujz4/9u7++A46vuO4+/vPeh00unBekSyZAsb2WA3WHYbsAsFB5rguA5JGhqCgbqtO25T/sAJ0yYuEFNmkrSdTBMmIQ3ESZO05AkSpxQmKWAoM7jFlmOb2FjYhkq2nmzJkiydJEsn3X37x66uJ1nYxkjeFfq+Zm50+9vV+nPaO39vf7v7W3JyltLQsJ76+iWkUgmcO9hCIBAllUoAyXOup6hoLbW1XycatXHhzbtjRcEYHygpWceKFa9y/PiXycqqIBZbTl7eCqLRKwBIJE4wPNzC0FAzqiOEQoXpPY7u7udoavoC9fVLmTfvb6mu/muCwWyPX5GZqezsI2PeA4aHW3nzzc/S2flTsrNrKCpaQ17eteTnryQnZxHO3W/NbHahZx9ZUTDmPaS7+3mam/+Rvr7dJJN9AASDeQSDeeOWCwSyCYXyCQYLCIXyyc39LaqqPjPpEB2JRCc9Pc9RVLSGcLj4krwOM/WsKBgzi6mmGBx8g76+XfT37yWVGsqYp6RSQySTfekD1gMDBwgGc6iq2kxV1X2Ew4UMDR2jufkrtLd/h1TqDOFwGYsWfZPS0k94+MrMxbKiYIy5YAMDDTQ1baWz80lCoTkUFNxAd/ezAJSX301JyR/S1LSV/v69lJR8gkWLHiUrq9zj1OadsKJgjHnH4vF9NDY+SF/fTsrLN1BdfR/Z2dUApFKjNDd/haamhwgGcyktvY1odCHZ2QuIRheQk3OlXWfhY74oCiKyBngECALbVPXvJ8z/LPDnwCjQCfyZqh471zqtKBjjrYGBN3jrrc8Qj+9hZORUul0kQmHhaoqL11Fc/AdEo5dP+vuJRAfd3b+kt/d/KC+/i8LC6y9V9FnN86IgztCQR4APAi1APXCHqh7KWOYDwC5VHRSRTwOrVfX2c63XioIx/jE62sfQUCNnzrxFb+8rdHU9696cCLKza4hE5hGJVJKVVUkgEKGnZwfxeD2giISAAEuW/HDS4xSqimqSQMDOnJ8KfigKq4CHVPUWd3oLgKp++W2WXw58Q1WvO9d6rSgY42+Dg0fp6nqWeHwXw8NtJBJtDA+3kkoNkZf3/vSeRHb2fA4c+Ah9fa9SW/soc+d+GnCKQVfXf9DY+CCDg4fIybmKWGw5sVgd+fmrKChY6fErnJn8MCDeXKA5Y7oFuPYcy28EfjnZDBHZBGwCmDdv3lTlM8ZMg5ycWnJyNo9rc771JwgEIuPaly17gUOHbufo0b8ikWijoOAGGhsfIB7fTTR6BXPn3svg4CF6ep7n5MkfAFBW9ilqa79xUafHJhInGRw8SkHBKrvPxdvwxX6ZiNwF/A5w42TzVfVx4HFw9hQuYTRjzBQQEUQiZ7UHgzksXbqdI0f+wr3lKUQi1SxevI3y8j8mEAinl00kTtLW9m2OHXuYnp6XWLz4cUpKbgUglRqhr+9VTp9+kVBoDoWFN5Kb+770RXt9fbtpbf06HR0/RTVBJDKfyspNVFRstLOoJpjOotAKVGdMV7lt44jI7wP3Azeq6vA05jHG+FAgEGLx4m3EYlcjEqaiYuNZexTgjDZbU/MAJSUfoaFhAwcPfpTS0ttIpUY4ffpFksk4IIwNTT52am0icYJ4fBfBYIzKyk3k5V3DiRPfo7HxfpqaHqKk5OMUFd1CQcF1RKOLZv3AgtN5TCGEc6D5ZpxiUA+sV9XXM5ZZDjwFrFHVoxeyXjumYIxJpRIcO/ZFjh//EpFIFXPm3EJR0YcoLLyJZLKP06dfdh//RSAQobLyL7nssg2EQvnpdQwOHqat7VucOPGvjI52ARAOl5KfvxKRLEZGTjE62sXISBdZWRUUF6+luHgdeXnvn5HDhnh+oNkNsRb4Gs4pqd9V1S+KyMPAHlV9WkReAN4HtLu/clxVbz3XOq0oGGPGpFIJRMLv6tv92NXfvb076e3dSTy+CwgQDhcTDhcTChVz5swRent3AinC4VIKC28kEqkmK6uCSKSSUKiYZLKXkZEuRka6SSZ7iUTmE4vVEYtdPa4YecUXRWE6WFEwxnhhZKSb7u5f0dX1DPF4PcPDbaRSg5MuKxIhszc8O3uBWyDqiMWWEYvVEYlUX1AxSyYHSCQ6GRnpJBwuedvrP87HD2cfGWPMe0Y4XER5+XrKy9cDzhlVyWSc4eE2Rke7CAYL3L2LIkSySCTa6O/fT3//a+mfp05tZ+yYR3b2QmpqHqSs7M5x12KMjvbT3v4YbW3fZni4eVzhqa7+HAsXjrsGeMrZnoIxxlwio6P9DAwcoL9/H+3t2+jv30c0uoiamq0UFd1Ca+s3aWl5hNHRLgoLVxOLrSArq4xwuJRwuIzc3KuIRhde1L9t3UfGGONjqsqpU7+gqWkrAwMHgACQorh4HfPm3T/lF+lZ95ExxviYiFBa+nFKSj5KZ+dT9Pb+N5dd9ifk5dV5msuKgjHGeEgkQFnZJykr+6TXUQBnf8UYY4wBrCgYY4zJYEXBGGNMmhUFY4wxaVYUjDHGpFlRMMYYk2ZFwRhjTJoVBWOMMWkzbpgLEekEjl3kr5cAp6YwznSaKVkt59SbKVkt59Sa7pzzVbX0fAvNuKLwbojIngsZ+8MPZkpWyzn1ZkpWyzm1/JLTuo+MMcakWVEwxhiTNtuKwuNeB3gHZkpWyzn1ZkpWyzm1fJFzVh1TMMYYc26zbU/BGGPMOcyaoiAia0TksIi8KSKf9zpPJhH5roh0iMjBjLYiEXleRI66P+d4nLFaRF4SkUMi8rqI3OvHnG6mbBHZLSKvuVn/zm2/XER2ue+Bn4hIltdZAUQkKCL7ROQZd9p3OUWkSUQOiMh+Ednjtvlx2xeKyFMi8oaINIjIKp/mXOz+LccefSKy2Q9ZZ0VREJEg8CjwYWAJcIeILPE21TjfA9ZMaPs8sENVa4Ed7rSXRoH7VHUJsBK4x/0b+i0nwDBwk6ouA+qANSKyEvgH4KuqegXQA2z0MGOme4GGjGm/5vyAqtZlnDbpx23/CPArVb0SWIbzd/VdTlU97P4t64DfBgaB7fghq6q+5x/AKuA/M6a3AFu8zjUhYw1wMGP6MFDhPq8ADnudcULefwc+OANy5gB7gWtxLgwKTfae8DBfFc6H/ybgGUB8mrMJKJnQ5qttDxQAjbjHSv2ac5LcHwJ2+iXrrNhTAOYCzRnTLW6bn5Wrarv7/ARQ7mWYTCJSAywHduHTnG6XzH6gA3geeAs4raqj7iJ+eQ98DfgbIOVOF+PPnAo8JyK/FpFNbpvftv3lQCfwL2533DYRycV/OSf6FPAj97nnWWdLUZjR1Pna4IvTxEQkBvwM2KyqfZnz/JRTVZPq7JpXAdcAV3oc6Swisg7oUNVfe53lAlyvqitwumDvEZEbMmf6ZNuHgBXAP6vqcmCACd0vPsmZ5h4vuhV4cuI8r7LOlqLQClRnTFe5bX52UkQqANyfHR7nQUTCOAXhCVX9udvsu5yZVPU08BJON0yhiITcWX54D1wH3CoiTcCPcbqQHsF/OVHVVvdnB07f9zX4b9u3AC2qusudfgqnSPgtZ6YPA3tV9aQ77XnW2VIU6oFa96yOLJzdtac9znQ+TwMb3OcbcPrwPSMiAnwHaFDVf8qY5aucACJSKiKF7vMozrGPBpzicJu7mOdZVXWLqlapag3Oe/JFVb0Tn+UUkVwRyRt7jtMHfhCfbXtVPQE0i8hit+lm4BA+yznBHfx/1xH4IavXB1ku4cGctcARnL7l+73OMyHbj4B2YATn285GnL7lHcBR4AWgyOOM1+Psyv4G2O8+1votp5v1amCfm/Ug8AW3fQGwG3gTZ3c94nXWjMyrgWf8mNPN85r7eH3s8+PTbV8H7HG3/S+AOX7M6WbNBbqAgow2z7PaFc3GGGPSZkv3kTHGmAtgRcEYY0yaFQVjjDFpVhSMMcakWVEwxhiTZkXBmEtIRFaPjYZqjB9ZUTDGGJNmRcGYSYjIXe49GfaLyGPuAHv9IvJV9x4NO0Sk1F22TkReFZHfiMj2sTHwReQKEXnBva/DXhFZ6K4+ljHm/xPu1eLG+IIVBWMmEJGrgNuB69QZVC8J3IlzBeoeVV0KvAxsdX/lB8DnVPVq4EBG+xPAo+rc1+F3ca5aB2eE2c049/ZYgDMGkjG+EDr/IsbMOjfj3Pik3v0SH8UZmCwF/MRd5t+An4tIAVCoqi+77d8HnnTHCpqrqtsBVHUIwF3fblVtcaf349xL45Xpf1nGnJ8VBWPOJsD3VXXLuEaRBycsd7FjxAxnPE9in0PjI9Z9ZMzZdgC3iUgZpO9FPB/n8zI2eul64BVV7QV6ROT33Pa7gZdVNQ60iMjH3HVERCTnkr4KYy6CfUMxZgJVPSQiD+DcaSyAM3rtPTg3bbnGndeBc9wBnCGOv+X+p/+/wJ+67XcDj4nIw+46/ugSvgxjLoqNkmrMBRKRflWNeZ3DmOlk3UfGGGPSbE/BGGNMmu0pGGOMSbOiYIwxJs2KgjHGmDQrCsYYY9KsKBhjjEmzomCMMSbt/wAGBTrKOSQ+sQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560/1560 [==============================] - 3s 2ms/sample - loss: 0.7398 - acc: 0.7513\n",
      "Loss: 0.7398333268287854 Accuracy: 0.75128204\n",
      "\n",
      "Train on 4680 samples, validate on 1560 samples\n",
      "Epoch 1/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.7479 - acc: 0.2423\n",
      "Epoch 00001: val_loss improved from inf to 1.59961, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/001-1.5996.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 1.7477 - acc: 0.2427 - val_loss: 1.5996 - val_acc: 0.3833\n",
      "Epoch 2/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.4590 - acc: 0.4212\n",
      "Epoch 00002: val_loss improved from 1.59961 to 1.28607, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/002-1.2861.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.4594 - acc: 0.4209 - val_loss: 1.2861 - val_acc: 0.5269\n",
      "Epoch 3/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.2681 - acc: 0.5092\n",
      "Epoch 00003: val_loss improved from 1.28607 to 1.14542, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/003-1.1454.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.2680 - acc: 0.5096 - val_loss: 1.1454 - val_acc: 0.5833\n",
      "Epoch 4/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.1566 - acc: 0.5741\n",
      "Epoch 00004: val_loss improved from 1.14542 to 1.04395, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/004-1.0440.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.1570 - acc: 0.5735 - val_loss: 1.0440 - val_acc: 0.6141\n",
      "Epoch 5/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.0819 - acc: 0.6008\n",
      "Epoch 00005: val_loss improved from 1.04395 to 0.98056, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/005-0.9806.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.0817 - acc: 0.6009 - val_loss: 0.9806 - val_acc: 0.6378\n",
      "Epoch 6/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.0266 - acc: 0.6216\n",
      "Epoch 00006: val_loss improved from 0.98056 to 0.94554, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/006-0.9455.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.0256 - acc: 0.6220 - val_loss: 0.9455 - val_acc: 0.6500\n",
      "Epoch 7/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9838 - acc: 0.6475\n",
      "Epoch 00007: val_loss improved from 0.94554 to 0.92101, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/007-0.9210.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.9851 - acc: 0.6466 - val_loss: 0.9210 - val_acc: 0.6615\n",
      "Epoch 8/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9472 - acc: 0.6640\n",
      "Epoch 00008: val_loss improved from 0.92101 to 0.88477, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/008-0.8848.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.9460 - acc: 0.6645 - val_loss: 0.8848 - val_acc: 0.6750\n",
      "Epoch 9/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9000 - acc: 0.6794\n",
      "Epoch 00009: val_loss improved from 0.88477 to 0.84525, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/009-0.8452.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8992 - acc: 0.6797 - val_loss: 0.8452 - val_acc: 0.6949\n",
      "Epoch 10/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8767 - acc: 0.6896\n",
      "Epoch 00010: val_loss improved from 0.84525 to 0.84049, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/010-0.8405.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8761 - acc: 0.6900 - val_loss: 0.8405 - val_acc: 0.7058\n",
      "Epoch 11/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8523 - acc: 0.7059\n",
      "Epoch 00011: val_loss did not improve from 0.84049\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8518 - acc: 0.7062 - val_loss: 0.8426 - val_acc: 0.6994\n",
      "Epoch 12/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8406 - acc: 0.7048\n",
      "Epoch 00012: val_loss improved from 0.84049 to 0.80149, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/012-0.8015.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8406 - acc: 0.7047 - val_loss: 0.8015 - val_acc: 0.7186\n",
      "Epoch 13/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8093 - acc: 0.7121\n",
      "Epoch 00013: val_loss improved from 0.80149 to 0.78359, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/013-0.7836.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8086 - acc: 0.7124 - val_loss: 0.7836 - val_acc: 0.7288\n",
      "Epoch 14/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7952 - acc: 0.7147\n",
      "Epoch 00014: val_loss improved from 0.78359 to 0.77741, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/014-0.7774.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7948 - acc: 0.7147 - val_loss: 0.7774 - val_acc: 0.7276\n",
      "Epoch 15/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7957 - acc: 0.7179\n",
      "Epoch 00015: val_loss improved from 0.77741 to 0.76649, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/015-0.7665.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7962 - acc: 0.7179 - val_loss: 0.7665 - val_acc: 0.7282\n",
      "Epoch 16/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7592 - acc: 0.7307\n",
      "Epoch 00016: val_loss did not improve from 0.76649\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7590 - acc: 0.7308 - val_loss: 0.8131 - val_acc: 0.7064\n",
      "Epoch 17/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7527 - acc: 0.7329\n",
      "Epoch 00017: val_loss did not improve from 0.76649\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7526 - acc: 0.7329 - val_loss: 0.7722 - val_acc: 0.7327\n",
      "Epoch 18/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7277 - acc: 0.7464\n",
      "Epoch 00018: val_loss improved from 0.76649 to 0.73656, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/018-0.7366.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7283 - acc: 0.7464 - val_loss: 0.7366 - val_acc: 0.7506\n",
      "Epoch 19/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7368 - acc: 0.7387\n",
      "Epoch 00019: val_loss improved from 0.73656 to 0.73203, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/019-0.7320.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7366 - acc: 0.7387 - val_loss: 0.7320 - val_acc: 0.7513\n",
      "Epoch 20/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7082 - acc: 0.7515\n",
      "Epoch 00020: val_loss improved from 0.73203 to 0.71650, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/020-0.7165.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7097 - acc: 0.7511 - val_loss: 0.7165 - val_acc: 0.7532\n",
      "Epoch 21/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6936 - acc: 0.7571\n",
      "Epoch 00021: val_loss did not improve from 0.71650\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6942 - acc: 0.7573 - val_loss: 0.7297 - val_acc: 0.7321\n",
      "Epoch 22/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6961 - acc: 0.7594\n",
      "Epoch 00022: val_loss improved from 0.71650 to 0.70434, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/022-0.7043.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6951 - acc: 0.7598 - val_loss: 0.7043 - val_acc: 0.7571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6638 - acc: 0.7699\n",
      "Epoch 00023: val_loss improved from 0.70434 to 0.67298, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/023-0.6730.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6630 - acc: 0.7703 - val_loss: 0.6730 - val_acc: 0.7679\n",
      "Epoch 24/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6495 - acc: 0.7791\n",
      "Epoch 00024: val_loss did not improve from 0.67298\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6491 - acc: 0.7793 - val_loss: 0.7108 - val_acc: 0.7468\n",
      "Epoch 25/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6496 - acc: 0.7703\n",
      "Epoch 00025: val_loss improved from 0.67298 to 0.65666, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/025-0.6567.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6494 - acc: 0.7705 - val_loss: 0.6567 - val_acc: 0.7750\n",
      "Epoch 26/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6331 - acc: 0.7827\n",
      "Epoch 00026: val_loss did not improve from 0.65666\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6326 - acc: 0.7831 - val_loss: 0.6630 - val_acc: 0.7583\n",
      "Epoch 27/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.7832\n",
      "Epoch 00027: val_loss improved from 0.65666 to 0.64726, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/027-0.6473.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6224 - acc: 0.7835 - val_loss: 0.6473 - val_acc: 0.7769\n",
      "Epoch 28/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6062 - acc: 0.7885\n",
      "Epoch 00028: val_loss improved from 0.64726 to 0.64178, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/028-0.6418.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6058 - acc: 0.7887 - val_loss: 0.6418 - val_acc: 0.7699\n",
      "Epoch 29/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5945 - acc: 0.7945\n",
      "Epoch 00029: val_loss did not improve from 0.64178\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5947 - acc: 0.7947 - val_loss: 0.6497 - val_acc: 0.7647\n",
      "Epoch 30/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5972 - acc: 0.7956\n",
      "Epoch 00030: val_loss improved from 0.64178 to 0.62556, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/030-0.6256.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5971 - acc: 0.7957 - val_loss: 0.6256 - val_acc: 0.7782\n",
      "Epoch 31/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5873 - acc: 0.7979\n",
      "Epoch 00031: val_loss improved from 0.62556 to 0.62454, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/031-0.6245.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5875 - acc: 0.7979 - val_loss: 0.6245 - val_acc: 0.7814\n",
      "Epoch 32/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5692 - acc: 0.8057\n",
      "Epoch 00032: val_loss did not improve from 0.62454\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5695 - acc: 0.8056 - val_loss: 0.6544 - val_acc: 0.7750\n",
      "Epoch 33/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5744 - acc: 0.8016\n",
      "Epoch 00033: val_loss improved from 0.62454 to 0.60964, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/033-0.6096.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5736 - acc: 0.8019 - val_loss: 0.6096 - val_acc: 0.7897\n",
      "Epoch 34/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.8093\n",
      "Epoch 00034: val_loss did not improve from 0.60964\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5537 - acc: 0.8094 - val_loss: 0.6225 - val_acc: 0.7872\n",
      "Epoch 35/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.8159\n",
      "Epoch 00035: val_loss improved from 0.60964 to 0.59176, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/035-0.5918.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5368 - acc: 0.8158 - val_loss: 0.5918 - val_acc: 0.7974\n",
      "Epoch 36/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.8164\n",
      "Epoch 00036: val_loss did not improve from 0.59176\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5341 - acc: 0.8160 - val_loss: 0.5960 - val_acc: 0.7917\n",
      "Epoch 37/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5231 - acc: 0.8198\n",
      "Epoch 00037: val_loss improved from 0.59176 to 0.58752, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/037-0.5875.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5231 - acc: 0.8199 - val_loss: 0.5875 - val_acc: 0.8000\n",
      "Epoch 38/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8219\n",
      "Epoch 00038: val_loss improved from 0.58752 to 0.57689, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/038-0.5769.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5199 - acc: 0.8220 - val_loss: 0.5769 - val_acc: 0.7955\n",
      "Epoch 39/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8292\n",
      "Epoch 00039: val_loss did not improve from 0.57689\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5141 - acc: 0.8293 - val_loss: 0.5805 - val_acc: 0.7897\n",
      "Epoch 40/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.8221\n",
      "Epoch 00040: val_loss improved from 0.57689 to 0.56732, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/040-0.5673.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5093 - acc: 0.8222 - val_loss: 0.5673 - val_acc: 0.8013\n",
      "Epoch 41/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4863 - acc: 0.8303\n",
      "Epoch 00041: val_loss did not improve from 0.56732\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4862 - acc: 0.8303 - val_loss: 0.5705 - val_acc: 0.8006\n",
      "Epoch 42/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.8350\n",
      "Epoch 00042: val_loss did not improve from 0.56732\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4840 - acc: 0.8350 - val_loss: 0.5793 - val_acc: 0.8058\n",
      "Epoch 43/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4873 - acc: 0.8322\n",
      "Epoch 00043: val_loss did not improve from 0.56732\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4871 - acc: 0.8321 - val_loss: 0.5739 - val_acc: 0.7942\n",
      "Epoch 44/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.8322\n",
      "Epoch 00044: val_loss improved from 0.56732 to 0.56217, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/044-0.5622.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4782 - acc: 0.8323 - val_loss: 0.5622 - val_acc: 0.8064\n",
      "Epoch 45/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4601 - acc: 0.8397\n",
      "Epoch 00045: val_loss did not improve from 0.56217\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4615 - acc: 0.8397 - val_loss: 0.5787 - val_acc: 0.7910\n",
      "Epoch 46/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4664 - acc: 0.8382\n",
      "Epoch 00046: val_loss did not improve from 0.56217\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4662 - acc: 0.8385 - val_loss: 0.5704 - val_acc: 0.8038\n",
      "Epoch 47/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.8423\n",
      "Epoch 00047: val_loss did not improve from 0.56217\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4525 - acc: 0.8421 - val_loss: 0.5760 - val_acc: 0.7974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4589 - acc: 0.8348\n",
      "Epoch 00048: val_loss improved from 0.56217 to 0.54231, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/048-0.5423.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4583 - acc: 0.8350 - val_loss: 0.5423 - val_acc: 0.8128\n",
      "Epoch 49/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4387 - acc: 0.8497\n",
      "Epoch 00049: val_loss improved from 0.54231 to 0.54041, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/049-0.5404.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4402 - acc: 0.8498 - val_loss: 0.5404 - val_acc: 0.8115\n",
      "Epoch 50/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4309 - acc: 0.8530\n",
      "Epoch 00050: val_loss did not improve from 0.54041\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4306 - acc: 0.8532 - val_loss: 0.5443 - val_acc: 0.8115\n",
      "Epoch 51/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.8478\n",
      "Epoch 00051: val_loss did not improve from 0.54041\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4290 - acc: 0.8481 - val_loss: 0.5651 - val_acc: 0.7955\n",
      "Epoch 52/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4241 - acc: 0.8542\n",
      "Epoch 00052: val_loss improved from 0.54041 to 0.54002, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/052-0.5400.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4245 - acc: 0.8541 - val_loss: 0.5400 - val_acc: 0.8115\n",
      "Epoch 53/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4217 - acc: 0.8560\n",
      "Epoch 00053: val_loss improved from 0.54002 to 0.53379, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/053-0.5338.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4212 - acc: 0.8560 - val_loss: 0.5338 - val_acc: 0.8147\n",
      "Epoch 54/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8639\n",
      "Epoch 00054: val_loss did not improve from 0.53379\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4021 - acc: 0.8639 - val_loss: 0.5513 - val_acc: 0.8083\n",
      "Epoch 55/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8602\n",
      "Epoch 00055: val_loss did not improve from 0.53379\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4043 - acc: 0.8605 - val_loss: 0.5691 - val_acc: 0.8000\n",
      "Epoch 56/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8604\n",
      "Epoch 00056: val_loss improved from 0.53379 to 0.53303, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/056-0.5330.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4096 - acc: 0.8607 - val_loss: 0.5330 - val_acc: 0.8141\n",
      "Epoch 57/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3843 - acc: 0.8694\n",
      "Epoch 00057: val_loss did not improve from 0.53303\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3838 - acc: 0.8697 - val_loss: 0.5406 - val_acc: 0.8077\n",
      "Epoch 58/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8549\n",
      "Epoch 00058: val_loss did not improve from 0.53303\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4084 - acc: 0.8551 - val_loss: 0.5419 - val_acc: 0.8122\n",
      "Epoch 59/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.8658\n",
      "Epoch 00059: val_loss did not improve from 0.53303\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3903 - acc: 0.8658 - val_loss: 0.5367 - val_acc: 0.8154\n",
      "Epoch 60/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4043 - acc: 0.8613\n",
      "Epoch 00060: val_loss improved from 0.53303 to 0.52799, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/060-0.5280.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4044 - acc: 0.8611 - val_loss: 0.5280 - val_acc: 0.8160\n",
      "Epoch 61/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8688\n",
      "Epoch 00061: val_loss did not improve from 0.52799\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3764 - acc: 0.8690 - val_loss: 0.5306 - val_acc: 0.8128\n",
      "Epoch 62/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8609\n",
      "Epoch 00062: val_loss did not improve from 0.52799\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3845 - acc: 0.8611 - val_loss: 0.5288 - val_acc: 0.8224\n",
      "Epoch 63/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3734 - acc: 0.8705\n",
      "Epoch 00063: val_loss did not improve from 0.52799\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3733 - acc: 0.8703 - val_loss: 0.5531 - val_acc: 0.8122\n",
      "Epoch 64/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3664 - acc: 0.8714\n",
      "Epoch 00064: val_loss improved from 0.52799 to 0.51686, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/064-0.5169.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3662 - acc: 0.8714 - val_loss: 0.5169 - val_acc: 0.8218\n",
      "Epoch 65/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8737\n",
      "Epoch 00065: val_loss did not improve from 0.51686\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3622 - acc: 0.8737 - val_loss: 0.5315 - val_acc: 0.8160\n",
      "Epoch 66/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8739\n",
      "Epoch 00066: val_loss improved from 0.51686 to 0.50668, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/066-0.5067.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3550 - acc: 0.8737 - val_loss: 0.5067 - val_acc: 0.8212\n",
      "Epoch 67/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8709\n",
      "Epoch 00067: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3652 - acc: 0.8709 - val_loss: 0.5442 - val_acc: 0.8135\n",
      "Epoch 68/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3505 - acc: 0.8759\n",
      "Epoch 00068: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3506 - acc: 0.8756 - val_loss: 0.5212 - val_acc: 0.8250\n",
      "Epoch 69/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8806\n",
      "Epoch 00069: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3493 - acc: 0.8803 - val_loss: 0.5122 - val_acc: 0.8186\n",
      "Epoch 70/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3634 - acc: 0.8724\n",
      "Epoch 00070: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3639 - acc: 0.8720 - val_loss: 0.5819 - val_acc: 0.8103\n",
      "Epoch 71/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8797\n",
      "Epoch 00071: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3502 - acc: 0.8797 - val_loss: 0.5192 - val_acc: 0.8218\n",
      "Epoch 72/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3458 - acc: 0.8767\n",
      "Epoch 00072: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3456 - acc: 0.8767 - val_loss: 0.5225 - val_acc: 0.8186\n",
      "Epoch 73/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.8825\n",
      "Epoch 00073: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3391 - acc: 0.8827 - val_loss: 0.5519 - val_acc: 0.8135\n",
      "Epoch 74/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.8917\n",
      "Epoch 00074: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3175 - acc: 0.8919 - val_loss: 0.5118 - val_acc: 0.8256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8870\n",
      "Epoch 00075: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3213 - acc: 0.8868 - val_loss: 0.5077 - val_acc: 0.8269\n",
      "Epoch 76/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.8891\n",
      "Epoch 00076: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3232 - acc: 0.8893 - val_loss: 0.5320 - val_acc: 0.8244\n",
      "Epoch 77/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3205 - acc: 0.8855\n",
      "Epoch 00077: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3205 - acc: 0.8857 - val_loss: 0.5321 - val_acc: 0.8179\n",
      "Epoch 78/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.8943\n",
      "Epoch 00078: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3063 - acc: 0.8940 - val_loss: 0.5883 - val_acc: 0.8109\n",
      "Epoch 79/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8791\n",
      "Epoch 00079: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3324 - acc: 0.8786 - val_loss: 0.5107 - val_acc: 0.8269\n",
      "Epoch 80/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.8921\n",
      "Epoch 00080: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3094 - acc: 0.8919 - val_loss: 0.5092 - val_acc: 0.8288\n",
      "Epoch 81/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.8885\n",
      "Epoch 00081: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3082 - acc: 0.8885 - val_loss: 0.5092 - val_acc: 0.8308\n",
      "Epoch 82/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.8908\n",
      "Epoch 00082: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3108 - acc: 0.8908 - val_loss: 0.5374 - val_acc: 0.8218\n",
      "Epoch 83/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.8936\n",
      "Epoch 00083: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3033 - acc: 0.8938 - val_loss: 0.5223 - val_acc: 0.8308\n",
      "Epoch 84/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.8964\n",
      "Epoch 00084: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2996 - acc: 0.8966 - val_loss: 0.5114 - val_acc: 0.8256\n",
      "Epoch 85/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9015\n",
      "Epoch 00085: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2943 - acc: 0.9015 - val_loss: 0.5267 - val_acc: 0.8237\n",
      "Epoch 86/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.8932\n",
      "Epoch 00086: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.3040 - acc: 0.8927 - val_loss: 0.5128 - val_acc: 0.8244\n",
      "Epoch 87/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.8981\n",
      "Epoch 00087: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2912 - acc: 0.8983 - val_loss: 0.5453 - val_acc: 0.8269\n",
      "Epoch 88/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.8970\n",
      "Epoch 00088: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2960 - acc: 0.8972 - val_loss: 0.5073 - val_acc: 0.8308\n",
      "Epoch 89/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9045\n",
      "Epoch 00089: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2789 - acc: 0.9045 - val_loss: 0.5168 - val_acc: 0.8269\n",
      "Epoch 90/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2796 - acc: 0.9048\n",
      "Epoch 00090: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2800 - acc: 0.9045 - val_loss: 0.5117 - val_acc: 0.8321\n",
      "Epoch 91/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9020\n",
      "Epoch 00091: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2705 - acc: 0.9021 - val_loss: 0.5250 - val_acc: 0.8218\n",
      "Epoch 92/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.8975\n",
      "Epoch 00092: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2792 - acc: 0.8972 - val_loss: 0.5105 - val_acc: 0.8244\n",
      "Epoch 93/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2771 - acc: 0.9041\n",
      "Epoch 00093: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2768 - acc: 0.9043 - val_loss: 0.5331 - val_acc: 0.8186\n",
      "Epoch 94/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.9090\n",
      "Epoch 00094: val_loss did not improve from 0.50668\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2651 - acc: 0.9092 - val_loss: 0.5297 - val_acc: 0.8212\n",
      "Epoch 95/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9043\n",
      "Epoch 00095: val_loss improved from 0.50668 to 0.50608, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/095-0.5061.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2686 - acc: 0.9043 - val_loss: 0.5061 - val_acc: 0.8276\n",
      "Epoch 96/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9110\n",
      "Epoch 00096: val_loss did not improve from 0.50608\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2665 - acc: 0.9109 - val_loss: 0.5331 - val_acc: 0.8250\n",
      "Epoch 97/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9062\n",
      "Epoch 00097: val_loss improved from 0.50608 to 0.50479, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/097-0.5048.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2654 - acc: 0.9060 - val_loss: 0.5048 - val_acc: 0.8282\n",
      "Epoch 98/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9120\n",
      "Epoch 00098: val_loss did not improve from 0.50479\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2706 - acc: 0.9122 - val_loss: 0.5052 - val_acc: 0.8269\n",
      "Epoch 99/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9073\n",
      "Epoch 00099: val_loss did not improve from 0.50479\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2593 - acc: 0.9071 - val_loss: 0.5568 - val_acc: 0.8103\n",
      "Epoch 100/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9037\n",
      "Epoch 00100: val_loss did not improve from 0.50479\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2704 - acc: 0.9038 - val_loss: 0.5141 - val_acc: 0.8288\n",
      "Epoch 101/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9142\n",
      "Epoch 00101: val_loss did not improve from 0.50479\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2413 - acc: 0.9143 - val_loss: 0.5201 - val_acc: 0.8295\n",
      "Epoch 102/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.9058\n",
      "Epoch 00102: val_loss did not improve from 0.50479\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2643 - acc: 0.9060 - val_loss: 0.5359 - val_acc: 0.8237\n",
      "Epoch 103/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9084\n",
      "Epoch 00103: val_loss did not improve from 0.50479\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2559 - acc: 0.9085 - val_loss: 0.5141 - val_acc: 0.8327\n",
      "Epoch 104/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9148\n",
      "Epoch 00104: val_loss did not improve from 0.50479\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2401 - acc: 0.9150 - val_loss: 0.5137 - val_acc: 0.8353\n",
      "Epoch 105/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9129\n",
      "Epoch 00105: val_loss did not improve from 0.50479\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2464 - acc: 0.9128 - val_loss: 0.5079 - val_acc: 0.8295\n",
      "Epoch 106/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9077\n",
      "Epoch 00106: val_loss did not improve from 0.50479\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2491 - acc: 0.9077 - val_loss: 0.5459 - val_acc: 0.8212\n",
      "Epoch 107/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9202\n",
      "Epoch 00107: val_loss did not improve from 0.50479\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2327 - acc: 0.9199 - val_loss: 0.5092 - val_acc: 0.8276\n",
      "Epoch 108/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9180\n",
      "Epoch 00108: val_loss did not improve from 0.50479\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2261 - acc: 0.9182 - val_loss: 0.5214 - val_acc: 0.8256\n",
      "Epoch 109/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9120\n",
      "Epoch 00109: val_loss improved from 0.50479 to 0.50241, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/109-0.5024.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2439 - acc: 0.9122 - val_loss: 0.5024 - val_acc: 0.8282\n",
      "Epoch 110/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9161\n",
      "Epoch 00110: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2359 - acc: 0.9160 - val_loss: 0.5168 - val_acc: 0.8321\n",
      "Epoch 111/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9283\n",
      "Epoch 00111: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2203 - acc: 0.9284 - val_loss: 0.5277 - val_acc: 0.8288\n",
      "Epoch 112/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9225\n",
      "Epoch 00112: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2198 - acc: 0.9226 - val_loss: 0.5148 - val_acc: 0.8359\n",
      "Epoch 113/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9163\n",
      "Epoch 00113: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2277 - acc: 0.9160 - val_loss: 0.5190 - val_acc: 0.8256\n",
      "Epoch 114/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9195\n",
      "Epoch 00114: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2274 - acc: 0.9194 - val_loss: 0.5082 - val_acc: 0.8269\n",
      "Epoch 115/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9185\n",
      "Epoch 00115: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2333 - acc: 0.9184 - val_loss: 0.5172 - val_acc: 0.8301\n",
      "Epoch 116/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9180\n",
      "Epoch 00116: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2254 - acc: 0.9182 - val_loss: 0.5365 - val_acc: 0.8327\n",
      "Epoch 117/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9309\n",
      "Epoch 00117: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2014 - acc: 0.9310 - val_loss: 0.5332 - val_acc: 0.8256\n",
      "Epoch 118/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9262\n",
      "Epoch 00118: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2149 - acc: 0.9263 - val_loss: 0.5055 - val_acc: 0.8308\n",
      "Epoch 119/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9324\n",
      "Epoch 00119: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2060 - acc: 0.9323 - val_loss: 0.5426 - val_acc: 0.8276\n",
      "Epoch 120/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9321\n",
      "Epoch 00120: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2024 - acc: 0.9321 - val_loss: 0.5249 - val_acc: 0.8295\n",
      "Epoch 121/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9253\n",
      "Epoch 00121: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2131 - acc: 0.9252 - val_loss: 0.5222 - val_acc: 0.8327\n",
      "Epoch 122/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9330\n",
      "Epoch 00122: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1959 - acc: 0.9329 - val_loss: 0.5172 - val_acc: 0.8288\n",
      "Epoch 123/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9294\n",
      "Epoch 00123: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2012 - acc: 0.9293 - val_loss: 0.5267 - val_acc: 0.8340\n",
      "Epoch 124/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9255\n",
      "Epoch 00124: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2053 - acc: 0.9256 - val_loss: 0.5086 - val_acc: 0.8340\n",
      "Epoch 125/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9381\n",
      "Epoch 00125: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1865 - acc: 0.9380 - val_loss: 0.5155 - val_acc: 0.8333\n",
      "Epoch 126/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9292\n",
      "Epoch 00126: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1982 - acc: 0.9291 - val_loss: 0.5132 - val_acc: 0.8365\n",
      "Epoch 127/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9339\n",
      "Epoch 00127: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1866 - acc: 0.9338 - val_loss: 0.5121 - val_acc: 0.8263\n",
      "Epoch 128/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9330\n",
      "Epoch 00128: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1944 - acc: 0.9331 - val_loss: 0.5428 - val_acc: 0.8212\n",
      "Epoch 129/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9307\n",
      "Epoch 00129: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1961 - acc: 0.9306 - val_loss: 0.5156 - val_acc: 0.8346\n",
      "Epoch 130/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9339\n",
      "Epoch 00130: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1834 - acc: 0.9338 - val_loss: 0.5242 - val_acc: 0.8353\n",
      "Epoch 131/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9358\n",
      "Epoch 00131: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1864 - acc: 0.9359 - val_loss: 0.5400 - val_acc: 0.8308\n",
      "Epoch 132/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9309\n",
      "Epoch 00132: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1953 - acc: 0.9308 - val_loss: 0.5473 - val_acc: 0.8327\n",
      "Epoch 133/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9360\n",
      "Epoch 00133: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1880 - acc: 0.9361 - val_loss: 0.5492 - val_acc: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9339\n",
      "Epoch 00134: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1857 - acc: 0.9338 - val_loss: 0.5216 - val_acc: 0.8353\n",
      "Epoch 135/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9330\n",
      "Epoch 00135: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1804 - acc: 0.9329 - val_loss: 0.5123 - val_acc: 0.8340\n",
      "Epoch 136/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9364\n",
      "Epoch 00136: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1808 - acc: 0.9363 - val_loss: 0.5091 - val_acc: 0.8397\n",
      "Epoch 137/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9364\n",
      "Epoch 00137: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1801 - acc: 0.9365 - val_loss: 0.5519 - val_acc: 0.8244\n",
      "Epoch 138/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9371\n",
      "Epoch 00138: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1764 - acc: 0.9370 - val_loss: 0.5253 - val_acc: 0.8346\n",
      "Epoch 139/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9332\n",
      "Epoch 00139: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1896 - acc: 0.9331 - val_loss: 0.5220 - val_acc: 0.8359\n",
      "Epoch 140/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9443\n",
      "Epoch 00140: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1616 - acc: 0.9444 - val_loss: 0.5319 - val_acc: 0.8378\n",
      "Epoch 141/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9358\n",
      "Epoch 00141: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1726 - acc: 0.9359 - val_loss: 0.5453 - val_acc: 0.8288\n",
      "Epoch 142/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9358\n",
      "Epoch 00142: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1769 - acc: 0.9355 - val_loss: 0.5732 - val_acc: 0.8321\n",
      "Epoch 143/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.9405\n",
      "Epoch 00143: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1681 - acc: 0.9406 - val_loss: 0.5429 - val_acc: 0.8327\n",
      "Epoch 144/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9422\n",
      "Epoch 00144: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1615 - acc: 0.9421 - val_loss: 0.5651 - val_acc: 0.8288\n",
      "Epoch 145/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9401\n",
      "Epoch 00145: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1669 - acc: 0.9400 - val_loss: 0.5168 - val_acc: 0.8378\n",
      "Epoch 146/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9405\n",
      "Epoch 00146: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1707 - acc: 0.9406 - val_loss: 0.5427 - val_acc: 0.8282\n",
      "Epoch 147/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9441\n",
      "Epoch 00147: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1600 - acc: 0.9442 - val_loss: 0.5204 - val_acc: 0.8404\n",
      "Epoch 148/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9441\n",
      "Epoch 00148: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1626 - acc: 0.9440 - val_loss: 0.5137 - val_acc: 0.8417\n",
      "Epoch 149/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9521\n",
      "Epoch 00149: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1512 - acc: 0.9521 - val_loss: 0.5323 - val_acc: 0.8365\n",
      "Epoch 150/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9420\n",
      "Epoch 00150: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1632 - acc: 0.9421 - val_loss: 0.5379 - val_acc: 0.8410\n",
      "Epoch 151/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9422\n",
      "Epoch 00151: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1600 - acc: 0.9423 - val_loss: 0.5203 - val_acc: 0.8385\n",
      "Epoch 152/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9518\n",
      "Epoch 00152: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1459 - acc: 0.9519 - val_loss: 0.5431 - val_acc: 0.8385\n",
      "Epoch 153/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9469\n",
      "Epoch 00153: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1544 - acc: 0.9468 - val_loss: 0.5640 - val_acc: 0.8340\n",
      "Epoch 154/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9392\n",
      "Epoch 00154: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1674 - acc: 0.9391 - val_loss: 0.5557 - val_acc: 0.8397\n",
      "Epoch 155/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9407\n",
      "Epoch 00155: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1642 - acc: 0.9404 - val_loss: 0.5497 - val_acc: 0.8333\n",
      "Epoch 156/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9461\n",
      "Epoch 00156: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1549 - acc: 0.9462 - val_loss: 0.5528 - val_acc: 0.8288\n",
      "Epoch 157/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9514\n",
      "Epoch 00157: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1476 - acc: 0.9515 - val_loss: 0.5435 - val_acc: 0.8391\n",
      "Epoch 158/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9482\n",
      "Epoch 00158: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1463 - acc: 0.9483 - val_loss: 0.5792 - val_acc: 0.8288\n",
      "Epoch 159/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9476\n",
      "Epoch 00159: val_loss did not improve from 0.50241\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1515 - acc: 0.9474 - val_loss: 0.5290 - val_acc: 0.8333\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560/1560 [==============================] - 3s 2ms/sample - loss: 0.5024 - acc: 0.8282\n",
      "Loss: 0.502409255810273 Accuracy: 0.8282051\n",
      "\n",
      "Train on 4680 samples, validate on 1560 samples\n",
      "Epoch 1/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.7415 - acc: 0.2468\n",
      "Epoch 00001: val_loss improved from inf to 1.56497, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/001-1.5650.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 1.7412 - acc: 0.2476 - val_loss: 1.5650 - val_acc: 0.4346\n",
      "Epoch 2/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.4840 - acc: 0.3838\n",
      "Epoch 00002: val_loss improved from 1.56497 to 1.31791, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/002-1.3179.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.4839 - acc: 0.3838 - val_loss: 1.3179 - val_acc: 0.4840\n",
      "Epoch 3/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.3965 - acc: 0.4317\n",
      "Epoch 00003: val_loss improved from 1.31791 to 1.27875, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/003-1.2788.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.3969 - acc: 0.4318 - val_loss: 1.2788 - val_acc: 0.5019\n",
      "Epoch 4/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.3426 - acc: 0.4557\n",
      "Epoch 00004: val_loss improved from 1.27875 to 1.20820, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/004-1.2082.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.3429 - acc: 0.4553 - val_loss: 1.2082 - val_acc: 0.5263\n",
      "Epoch 5/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.2794 - acc: 0.4867\n",
      "Epoch 00005: val_loss improved from 1.20820 to 1.14685, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/005-1.1468.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.2790 - acc: 0.4870 - val_loss: 1.1468 - val_acc: 0.5417\n",
      "Epoch 6/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.2386 - acc: 0.5152\n",
      "Epoch 00006: val_loss improved from 1.14685 to 1.11974, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/006-1.1197.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.2394 - acc: 0.5150 - val_loss: 1.1197 - val_acc: 0.5814\n",
      "Epoch 7/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.1916 - acc: 0.5340\n",
      "Epoch 00007: val_loss improved from 1.11974 to 1.07645, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/007-1.0765.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.1919 - acc: 0.5335 - val_loss: 1.0765 - val_acc: 0.5974\n",
      "Epoch 8/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.1617 - acc: 0.5565\n",
      "Epoch 00008: val_loss improved from 1.07645 to 1.02631, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/008-1.0263.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.1612 - acc: 0.5566 - val_loss: 1.0263 - val_acc: 0.6199\n",
      "Epoch 9/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.1082 - acc: 0.5760\n",
      "Epoch 00009: val_loss improved from 1.02631 to 0.98926, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/009-0.9893.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.1075 - acc: 0.5765 - val_loss: 0.9893 - val_acc: 0.6256\n",
      "Epoch 10/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.0760 - acc: 0.5978\n",
      "Epoch 00010: val_loss improved from 0.98926 to 0.94755, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/010-0.9476.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.0754 - acc: 0.5979 - val_loss: 0.9476 - val_acc: 0.6481\n",
      "Epoch 11/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.0529 - acc: 0.6092\n",
      "Epoch 00011: val_loss improved from 0.94755 to 0.93387, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/011-0.9339.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.0520 - acc: 0.6096 - val_loss: 0.9339 - val_acc: 0.6577\n",
      "Epoch 12/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.0126 - acc: 0.6267\n",
      "Epoch 00012: val_loss improved from 0.93387 to 0.89475, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/012-0.8947.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 1.0131 - acc: 0.6263 - val_loss: 0.8947 - val_acc: 0.6788\n",
      "Epoch 13/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9792 - acc: 0.6381\n",
      "Epoch 00013: val_loss did not improve from 0.89475\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.9804 - acc: 0.6372 - val_loss: 0.9265 - val_acc: 0.6551\n",
      "Epoch 14/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9634 - acc: 0.6402\n",
      "Epoch 00014: val_loss improved from 0.89475 to 0.87018, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/014-0.8702.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.9633 - acc: 0.6404 - val_loss: 0.8702 - val_acc: 0.6885\n",
      "Epoch 15/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9592 - acc: 0.6528\n",
      "Epoch 00015: val_loss improved from 0.87018 to 0.86217, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/015-0.8622.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.9590 - acc: 0.6530 - val_loss: 0.8622 - val_acc: 0.6917\n",
      "Epoch 16/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9061 - acc: 0.6721\n",
      "Epoch 00016: val_loss improved from 0.86217 to 0.81749, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/016-0.8175.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.9063 - acc: 0.6718 - val_loss: 0.8175 - val_acc: 0.7083\n",
      "Epoch 17/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9044 - acc: 0.6751\n",
      "Epoch 00017: val_loss did not improve from 0.81749\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.9041 - acc: 0.6750 - val_loss: 0.8385 - val_acc: 0.6955\n",
      "Epoch 18/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8796 - acc: 0.6845\n",
      "Epoch 00018: val_loss did not improve from 0.81749\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8787 - acc: 0.6848 - val_loss: 0.8213 - val_acc: 0.7000\n",
      "Epoch 19/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8319 - acc: 0.6999\n",
      "Epoch 00019: val_loss improved from 0.81749 to 0.77309, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/019-0.7731.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8322 - acc: 0.7000 - val_loss: 0.7731 - val_acc: 0.7269\n",
      "Epoch 20/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8313 - acc: 0.7044\n",
      "Epoch 00020: val_loss did not improve from 0.77309\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8317 - acc: 0.7038 - val_loss: 0.7963 - val_acc: 0.7141\n",
      "Epoch 21/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8236 - acc: 0.7104\n",
      "Epoch 00021: val_loss did not improve from 0.77309\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8240 - acc: 0.7100 - val_loss: 0.8086 - val_acc: 0.7058\n",
      "Epoch 22/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7945 - acc: 0.7170\n",
      "Epoch 00022: val_loss did not improve from 0.77309\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7952 - acc: 0.7169 - val_loss: 0.8019 - val_acc: 0.7083\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8339 - acc: 0.7001\n",
      "Epoch 00023: val_loss improved from 0.77309 to 0.76955, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/023-0.7695.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.8341 - acc: 0.7000 - val_loss: 0.7695 - val_acc: 0.7237\n",
      "Epoch 24/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7660 - acc: 0.7226\n",
      "Epoch 00024: val_loss improved from 0.76955 to 0.70988, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/024-0.7099.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7671 - acc: 0.7222 - val_loss: 0.7099 - val_acc: 0.7455\n",
      "Epoch 25/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7470 - acc: 0.7361\n",
      "Epoch 00025: val_loss improved from 0.70988 to 0.67588, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/025-0.6759.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7463 - acc: 0.7365 - val_loss: 0.6759 - val_acc: 0.7500\n",
      "Epoch 26/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7548 - acc: 0.7269\n",
      "Epoch 00026: val_loss did not improve from 0.67588\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7548 - acc: 0.7267 - val_loss: 0.7212 - val_acc: 0.7372\n",
      "Epoch 27/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7227 - acc: 0.7432\n",
      "Epoch 00027: val_loss improved from 0.67588 to 0.66589, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/027-0.6659.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7235 - acc: 0.7429 - val_loss: 0.6659 - val_acc: 0.7577\n",
      "Epoch 28/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7140 - acc: 0.7481\n",
      "Epoch 00028: val_loss improved from 0.66589 to 0.64917, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/028-0.6492.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.7133 - acc: 0.7483 - val_loss: 0.6492 - val_acc: 0.7622\n",
      "Epoch 29/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7004 - acc: 0.7502\n",
      "Epoch 00029: val_loss improved from 0.64917 to 0.64382, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/029-0.6438.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6995 - acc: 0.7506 - val_loss: 0.6438 - val_acc: 0.7705\n",
      "Epoch 30/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6847 - acc: 0.7641\n",
      "Epoch 00030: val_loss did not improve from 0.64382\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6847 - acc: 0.7641 - val_loss: 0.6701 - val_acc: 0.7647\n",
      "Epoch 31/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6800 - acc: 0.7613\n",
      "Epoch 00031: val_loss did not improve from 0.64382\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6806 - acc: 0.7611 - val_loss: 0.6480 - val_acc: 0.7647\n",
      "Epoch 32/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6689 - acc: 0.7637\n",
      "Epoch 00032: val_loss improved from 0.64382 to 0.61382, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/032-0.6138.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6694 - acc: 0.7635 - val_loss: 0.6138 - val_acc: 0.7769\n",
      "Epoch 33/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6451 - acc: 0.7712\n",
      "Epoch 00033: val_loss did not improve from 0.61382\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6446 - acc: 0.7714 - val_loss: 0.6202 - val_acc: 0.7705\n",
      "Epoch 34/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6296 - acc: 0.7793\n",
      "Epoch 00034: val_loss did not improve from 0.61382\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6297 - acc: 0.7793 - val_loss: 0.7373 - val_acc: 0.7231\n",
      "Epoch 35/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6162 - acc: 0.7806\n",
      "Epoch 00035: val_loss improved from 0.61382 to 0.60827, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/035-0.6083.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6156 - acc: 0.7810 - val_loss: 0.6083 - val_acc: 0.7801\n",
      "Epoch 36/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6052 - acc: 0.7838\n",
      "Epoch 00036: val_loss improved from 0.60827 to 0.59153, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/036-0.5915.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6043 - acc: 0.7842 - val_loss: 0.5915 - val_acc: 0.7859\n",
      "Epoch 37/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6145 - acc: 0.7834\n",
      "Epoch 00037: val_loss did not improve from 0.59153\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6147 - acc: 0.7831 - val_loss: 0.6141 - val_acc: 0.7763\n",
      "Epoch 38/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6034 - acc: 0.7812\n",
      "Epoch 00038: val_loss improved from 0.59153 to 0.57985, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/038-0.5799.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.6039 - acc: 0.7812 - val_loss: 0.5799 - val_acc: 0.7904\n",
      "Epoch 39/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5736 - acc: 0.7977\n",
      "Epoch 00039: val_loss did not improve from 0.57985\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5744 - acc: 0.7972 - val_loss: 0.6114 - val_acc: 0.7737\n",
      "Epoch 40/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5703 - acc: 0.8016\n",
      "Epoch 00040: val_loss did not improve from 0.57985\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5698 - acc: 0.8017 - val_loss: 0.6390 - val_acc: 0.7635\n",
      "Epoch 41/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5752 - acc: 0.7937\n",
      "Epoch 00041: val_loss did not improve from 0.57985\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5751 - acc: 0.7936 - val_loss: 0.6307 - val_acc: 0.7635\n",
      "Epoch 42/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5808 - acc: 0.7932\n",
      "Epoch 00042: val_loss improved from 0.57985 to 0.55786, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/042-0.5579.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5809 - acc: 0.7932 - val_loss: 0.5579 - val_acc: 0.7929\n",
      "Epoch 43/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5686 - acc: 0.7945\n",
      "Epoch 00043: val_loss improved from 0.55786 to 0.55546, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/043-0.5555.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5682 - acc: 0.7949 - val_loss: 0.5555 - val_acc: 0.7955\n",
      "Epoch 44/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.8095\n",
      "Epoch 00044: val_loss did not improve from 0.55546\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5417 - acc: 0.8096 - val_loss: 0.5687 - val_acc: 0.7974\n",
      "Epoch 45/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.8065\n",
      "Epoch 00045: val_loss did not improve from 0.55546\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5529 - acc: 0.8064 - val_loss: 0.5818 - val_acc: 0.7801\n",
      "Epoch 46/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.8134\n",
      "Epoch 00046: val_loss did not improve from 0.55546\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5443 - acc: 0.8132 - val_loss: 0.5724 - val_acc: 0.7776\n",
      "Epoch 47/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.8009\n",
      "Epoch 00047: val_loss improved from 0.55546 to 0.55505, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/047-0.5550.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5439 - acc: 0.8011 - val_loss: 0.5550 - val_acc: 0.7962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5330 - acc: 0.8084\n",
      "Epoch 00048: val_loss did not improve from 0.55505\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5341 - acc: 0.8083 - val_loss: 0.6230 - val_acc: 0.7782\n",
      "Epoch 49/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.8157\n",
      "Epoch 00049: val_loss improved from 0.55505 to 0.55113, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/049-0.5511.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5232 - acc: 0.8160 - val_loss: 0.5511 - val_acc: 0.7974\n",
      "Epoch 50/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.8082\n",
      "Epoch 00050: val_loss improved from 0.55113 to 0.52920, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/050-0.5292.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5299 - acc: 0.8085 - val_loss: 0.5292 - val_acc: 0.8090\n",
      "Epoch 51/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.8155\n",
      "Epoch 00051: val_loss improved from 0.52920 to 0.52044, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/051-0.5204.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.5200 - acc: 0.8154 - val_loss: 0.5204 - val_acc: 0.8077\n",
      "Epoch 52/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4993 - acc: 0.8206\n",
      "Epoch 00052: val_loss did not improve from 0.52044\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4999 - acc: 0.8203 - val_loss: 0.5299 - val_acc: 0.8071\n",
      "Epoch 53/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4946 - acc: 0.8273\n",
      "Epoch 00053: val_loss improved from 0.52044 to 0.51363, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/053-0.5136.hdf5\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.4947 - acc: 0.8271 - val_loss: 0.5136 - val_acc: 0.8058\n",
      "Epoch 54/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4757 - acc: 0.8305\n",
      "Epoch 00054: val_loss did not improve from 0.51363\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.4757 - acc: 0.8306 - val_loss: 0.5635 - val_acc: 0.7872\n",
      "Epoch 55/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8202\n",
      "Epoch 00055: val_loss improved from 0.51363 to 0.49839, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/055-0.4984.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.5032 - acc: 0.8201 - val_loss: 0.4984 - val_acc: 0.8147\n",
      "Epoch 56/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4880 - acc: 0.8271\n",
      "Epoch 00056: val_loss did not improve from 0.49839\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4877 - acc: 0.8271 - val_loss: 0.5119 - val_acc: 0.8205\n",
      "Epoch 57/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4598 - acc: 0.8384\n",
      "Epoch 00057: val_loss did not improve from 0.49839\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4594 - acc: 0.8385 - val_loss: 0.5533 - val_acc: 0.8103\n",
      "Epoch 58/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4649 - acc: 0.8309\n",
      "Epoch 00058: val_loss did not improve from 0.49839\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4643 - acc: 0.8312 - val_loss: 0.5189 - val_acc: 0.8103\n",
      "Epoch 59/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4802 - acc: 0.8360\n",
      "Epoch 00059: val_loss did not improve from 0.49839\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4797 - acc: 0.8361 - val_loss: 0.5396 - val_acc: 0.8096\n",
      "Epoch 60/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4629 - acc: 0.8350\n",
      "Epoch 00060: val_loss improved from 0.49839 to 0.49555, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/060-0.4956.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4629 - acc: 0.8350 - val_loss: 0.4956 - val_acc: 0.8231\n",
      "Epoch 61/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4554 - acc: 0.8384\n",
      "Epoch 00061: val_loss did not improve from 0.49555\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4558 - acc: 0.8382 - val_loss: 0.5264 - val_acc: 0.8019\n",
      "Epoch 62/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4504 - acc: 0.8395\n",
      "Epoch 00062: val_loss did not improve from 0.49555\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4515 - acc: 0.8393 - val_loss: 0.5006 - val_acc: 0.8103\n",
      "Epoch 63/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.8410\n",
      "Epoch 00063: val_loss improved from 0.49555 to 0.48011, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/063-0.4801.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4493 - acc: 0.8402 - val_loss: 0.4801 - val_acc: 0.8321\n",
      "Epoch 64/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.8384\n",
      "Epoch 00064: val_loss did not improve from 0.48011\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4478 - acc: 0.8385 - val_loss: 0.4857 - val_acc: 0.8218\n",
      "Epoch 65/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4280 - acc: 0.8470\n",
      "Epoch 00065: val_loss did not improve from 0.48011\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4277 - acc: 0.8470 - val_loss: 0.4973 - val_acc: 0.8167\n",
      "Epoch 66/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8521\n",
      "Epoch 00066: val_loss did not improve from 0.48011\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4238 - acc: 0.8524 - val_loss: 0.6005 - val_acc: 0.7891\n",
      "Epoch 67/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4374 - acc: 0.8444\n",
      "Epoch 00067: val_loss did not improve from 0.48011\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4371 - acc: 0.8444 - val_loss: 0.4852 - val_acc: 0.8231\n",
      "Epoch 68/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.8489\n",
      "Epoch 00068: val_loss did not improve from 0.48011\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4248 - acc: 0.8487 - val_loss: 0.4943 - val_acc: 0.8179\n",
      "Epoch 69/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8474\n",
      "Epoch 00069: val_loss improved from 0.48011 to 0.47756, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/069-0.4776.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4143 - acc: 0.8472 - val_loss: 0.4776 - val_acc: 0.8346\n",
      "Epoch 70/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8474\n",
      "Epoch 00070: val_loss did not improve from 0.47756\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4172 - acc: 0.8476 - val_loss: 0.4911 - val_acc: 0.8256\n",
      "Epoch 71/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4067 - acc: 0.8534\n",
      "Epoch 00071: val_loss improved from 0.47756 to 0.45468, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/071-0.4547.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4064 - acc: 0.8536 - val_loss: 0.4547 - val_acc: 0.8442\n",
      "Epoch 72/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8658\n",
      "Epoch 00072: val_loss did not improve from 0.45468\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.3909 - acc: 0.8656 - val_loss: 0.4602 - val_acc: 0.8365\n",
      "Epoch 73/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8589\n",
      "Epoch 00073: val_loss did not improve from 0.45468\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4045 - acc: 0.8588 - val_loss: 0.4755 - val_acc: 0.8295\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8553\n",
      "Epoch 00074: val_loss did not improve from 0.45468\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4113 - acc: 0.8553 - val_loss: 0.5358 - val_acc: 0.8051\n",
      "Epoch 75/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8583\n",
      "Epoch 00075: val_loss did not improve from 0.45468\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4053 - acc: 0.8583 - val_loss: 0.4667 - val_acc: 0.8301\n",
      "Epoch 76/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3764 - acc: 0.8654\n",
      "Epoch 00076: val_loss did not improve from 0.45468\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3767 - acc: 0.8652 - val_loss: 0.4759 - val_acc: 0.8346\n",
      "Epoch 77/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8583\n",
      "Epoch 00077: val_loss did not improve from 0.45468\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3966 - acc: 0.8585 - val_loss: 0.4645 - val_acc: 0.8359\n",
      "Epoch 78/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8572\n",
      "Epoch 00078: val_loss did not improve from 0.45468\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4077 - acc: 0.8575 - val_loss: 0.5128 - val_acc: 0.8128\n",
      "Epoch 79/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8701\n",
      "Epoch 00079: val_loss improved from 0.45468 to 0.44790, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/079-0.4479.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3783 - acc: 0.8697 - val_loss: 0.4479 - val_acc: 0.8487\n",
      "Epoch 80/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3825 - acc: 0.8619\n",
      "Epoch 00080: val_loss did not improve from 0.44790\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3830 - acc: 0.8620 - val_loss: 0.4485 - val_acc: 0.8397\n",
      "Epoch 81/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3734 - acc: 0.8652\n",
      "Epoch 00081: val_loss did not improve from 0.44790\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3743 - acc: 0.8643 - val_loss: 0.4601 - val_acc: 0.8404\n",
      "Epoch 82/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3722 - acc: 0.8662\n",
      "Epoch 00082: val_loss improved from 0.44790 to 0.44421, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/082-0.4442.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3725 - acc: 0.8662 - val_loss: 0.4442 - val_acc: 0.8481\n",
      "Epoch 83/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3875 - acc: 0.8626\n",
      "Epoch 00083: val_loss did not improve from 0.44421\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3875 - acc: 0.8626 - val_loss: 0.4871 - val_acc: 0.8244\n",
      "Epoch 84/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8649\n",
      "Epoch 00084: val_loss did not improve from 0.44421\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3857 - acc: 0.8650 - val_loss: 0.4748 - val_acc: 0.8378\n",
      "Epoch 85/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8673\n",
      "Epoch 00085: val_loss did not improve from 0.44421\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3780 - acc: 0.8669 - val_loss: 0.4607 - val_acc: 0.8301\n",
      "Epoch 86/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8711\n",
      "Epoch 00086: val_loss did not improve from 0.44421\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3669 - acc: 0.8714 - val_loss: 0.5058 - val_acc: 0.8205\n",
      "Epoch 87/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8741\n",
      "Epoch 00087: val_loss did not improve from 0.44421\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3563 - acc: 0.8741 - val_loss: 0.4496 - val_acc: 0.8417\n",
      "Epoch 88/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3837 - acc: 0.8579\n",
      "Epoch 00088: val_loss did not improve from 0.44421\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3832 - acc: 0.8581 - val_loss: 0.4625 - val_acc: 0.8397\n",
      "Epoch 89/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8774\n",
      "Epoch 00089: val_loss improved from 0.44421 to 0.42474, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/089-0.4247.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3462 - acc: 0.8774 - val_loss: 0.4247 - val_acc: 0.8545\n",
      "Epoch 90/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.8759\n",
      "Epoch 00090: val_loss did not improve from 0.42474\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3430 - acc: 0.8759 - val_loss: 0.4348 - val_acc: 0.8494\n",
      "Epoch 91/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8774\n",
      "Epoch 00091: val_loss did not improve from 0.42474\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3505 - acc: 0.8774 - val_loss: 0.4546 - val_acc: 0.8365\n",
      "Epoch 92/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3447 - acc: 0.8795\n",
      "Epoch 00092: val_loss did not improve from 0.42474\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3450 - acc: 0.8795 - val_loss: 0.4487 - val_acc: 0.8487\n",
      "Epoch 93/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8859\n",
      "Epoch 00093: val_loss did not improve from 0.42474\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3359 - acc: 0.8859 - val_loss: 0.4421 - val_acc: 0.8455\n",
      "Epoch 94/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8763\n",
      "Epoch 00094: val_loss did not improve from 0.42474\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3487 - acc: 0.8763 - val_loss: 0.4697 - val_acc: 0.8404\n",
      "Epoch 95/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3328 - acc: 0.8855\n",
      "Epoch 00095: val_loss did not improve from 0.42474\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3326 - acc: 0.8855 - val_loss: 0.4521 - val_acc: 0.8423\n",
      "Epoch 96/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8799\n",
      "Epoch 00096: val_loss did not improve from 0.42474\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3404 - acc: 0.8799 - val_loss: 0.4623 - val_acc: 0.8372\n",
      "Epoch 97/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8825\n",
      "Epoch 00097: val_loss did not improve from 0.42474\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3269 - acc: 0.8821 - val_loss: 0.4311 - val_acc: 0.8551\n",
      "Epoch 98/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.8836\n",
      "Epoch 00098: val_loss did not improve from 0.42474\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3232 - acc: 0.8838 - val_loss: 0.4304 - val_acc: 0.8545\n",
      "Epoch 99/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.8898\n",
      "Epoch 00099: val_loss improved from 0.42474 to 0.41969, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/099-0.4197.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3138 - acc: 0.8900 - val_loss: 0.4197 - val_acc: 0.8558\n",
      "Epoch 100/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8842\n",
      "Epoch 00100: val_loss improved from 0.41969 to 0.41879, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/100-0.4188.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3181 - acc: 0.8842 - val_loss: 0.4188 - val_acc: 0.8571\n",
      "Epoch 101/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.8889\n",
      "Epoch 00101: val_loss did not improve from 0.41879\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3067 - acc: 0.8891 - val_loss: 0.4866 - val_acc: 0.8314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.8881\n",
      "Epoch 00102: val_loss improved from 0.41879 to 0.41679, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/102-0.4168.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3142 - acc: 0.8882 - val_loss: 0.4168 - val_acc: 0.8603\n",
      "Epoch 103/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.8923\n",
      "Epoch 00103: val_loss did not improve from 0.41679\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3024 - acc: 0.8925 - val_loss: 0.4365 - val_acc: 0.8551\n",
      "Epoch 104/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.8919\n",
      "Epoch 00104: val_loss did not improve from 0.41679\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3061 - acc: 0.8921 - val_loss: 0.4417 - val_acc: 0.8449\n",
      "Epoch 105/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.9013\n",
      "Epoch 00105: val_loss did not improve from 0.41679\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2893 - acc: 0.9013 - val_loss: 0.4230 - val_acc: 0.8583\n",
      "Epoch 106/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3079 - acc: 0.8928\n",
      "Epoch 00106: val_loss did not improve from 0.41679\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3074 - acc: 0.8929 - val_loss: 0.4823 - val_acc: 0.8353\n",
      "Epoch 107/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.8936\n",
      "Epoch 00107: val_loss did not improve from 0.41679\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2984 - acc: 0.8938 - val_loss: 0.4379 - val_acc: 0.8462\n",
      "Epoch 108/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.8970\n",
      "Epoch 00108: val_loss did not improve from 0.41679\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2895 - acc: 0.8970 - val_loss: 0.4184 - val_acc: 0.8564\n",
      "Epoch 109/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.9020\n",
      "Epoch 00109: val_loss did not improve from 0.41679\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2759 - acc: 0.9019 - val_loss: 0.4839 - val_acc: 0.8365\n",
      "Epoch 110/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.8981\n",
      "Epoch 00110: val_loss did not improve from 0.41679\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2890 - acc: 0.8983 - val_loss: 0.6352 - val_acc: 0.7981\n",
      "Epoch 111/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.8887\n",
      "Epoch 00111: val_loss did not improve from 0.41679\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3162 - acc: 0.8889 - val_loss: 0.4590 - val_acc: 0.8429\n",
      "Epoch 112/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.8998\n",
      "Epoch 00112: val_loss did not improve from 0.41679\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2914 - acc: 0.8998 - val_loss: 0.4320 - val_acc: 0.8564\n",
      "Epoch 113/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9018\n",
      "Epoch 00113: val_loss did not improve from 0.41679\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2762 - acc: 0.9017 - val_loss: 0.4488 - val_acc: 0.8481\n",
      "Epoch 114/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9045\n",
      "Epoch 00114: val_loss improved from 0.41679 to 0.41014, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/114-0.4101.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2641 - acc: 0.9043 - val_loss: 0.4101 - val_acc: 0.8577\n",
      "Epoch 115/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.8977\n",
      "Epoch 00115: val_loss did not improve from 0.41014\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2795 - acc: 0.8976 - val_loss: 0.4197 - val_acc: 0.8583\n",
      "Epoch 116/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9062\n",
      "Epoch 00116: val_loss did not improve from 0.41014\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2665 - acc: 0.9064 - val_loss: 0.4265 - val_acc: 0.8481\n",
      "Epoch 117/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9084\n",
      "Epoch 00117: val_loss did not improve from 0.41014\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2613 - acc: 0.9085 - val_loss: 0.4147 - val_acc: 0.8641\n",
      "Epoch 118/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2678 - acc: 0.9062\n",
      "Epoch 00118: val_loss did not improve from 0.41014\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2674 - acc: 0.9064 - val_loss: 0.6449 - val_acc: 0.7955\n",
      "Epoch 119/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9007\n",
      "Epoch 00119: val_loss did not improve from 0.41014\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2847 - acc: 0.9009 - val_loss: 0.4413 - val_acc: 0.8538\n",
      "Epoch 120/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9041\n",
      "Epoch 00120: val_loss did not improve from 0.41014\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2612 - acc: 0.9043 - val_loss: 0.4229 - val_acc: 0.8577\n",
      "Epoch 121/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9107\n",
      "Epoch 00121: val_loss did not improve from 0.41014\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2550 - acc: 0.9107 - val_loss: 0.4148 - val_acc: 0.8615\n",
      "Epoch 122/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9099\n",
      "Epoch 00122: val_loss did not improve from 0.41014\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2642 - acc: 0.9096 - val_loss: 0.4200 - val_acc: 0.8538\n",
      "Epoch 123/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.8936\n",
      "Epoch 00123: val_loss did not improve from 0.41014\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2967 - acc: 0.8936 - val_loss: 0.4227 - val_acc: 0.8474\n",
      "Epoch 124/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9024\n",
      "Epoch 00124: val_loss did not improve from 0.41014\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2685 - acc: 0.9026 - val_loss: 0.4161 - val_acc: 0.8609\n",
      "Epoch 125/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.9065\n",
      "Epoch 00125: val_loss did not improve from 0.41014\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2572 - acc: 0.9064 - val_loss: 0.4234 - val_acc: 0.8583\n",
      "Epoch 126/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9112\n",
      "Epoch 00126: val_loss improved from 0.41014 to 0.40558, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/126-0.4056.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2564 - acc: 0.9111 - val_loss: 0.4056 - val_acc: 0.8603\n",
      "Epoch 127/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9157\n",
      "Epoch 00127: val_loss did not improve from 0.40558\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2392 - acc: 0.9156 - val_loss: 0.4257 - val_acc: 0.8571\n",
      "Epoch 128/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9122\n",
      "Epoch 00128: val_loss did not improve from 0.40558\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2478 - acc: 0.9124 - val_loss: 0.4356 - val_acc: 0.8558\n",
      "Epoch 129/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9157\n",
      "Epoch 00129: val_loss did not improve from 0.40558\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2413 - acc: 0.9154 - val_loss: 0.4143 - val_acc: 0.8603\n",
      "Epoch 130/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9105\n",
      "Epoch 00130: val_loss did not improve from 0.40558\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2546 - acc: 0.9105 - val_loss: 0.4578 - val_acc: 0.8513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.9118\n",
      "Epoch 00131: val_loss did not improve from 0.40558\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2491 - acc: 0.9120 - val_loss: 0.4423 - val_acc: 0.8481\n",
      "Epoch 132/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9148\n",
      "Epoch 00132: val_loss did not improve from 0.40558\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2412 - acc: 0.9150 - val_loss: 0.4414 - val_acc: 0.8494\n",
      "Epoch 133/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9118\n",
      "Epoch 00133: val_loss did not improve from 0.40558\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2498 - acc: 0.9118 - val_loss: 0.4415 - val_acc: 0.8481\n",
      "Epoch 134/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9236\n",
      "Epoch 00134: val_loss did not improve from 0.40558\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2205 - acc: 0.9237 - val_loss: 0.4164 - val_acc: 0.8641\n",
      "Epoch 135/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9191\n",
      "Epoch 00135: val_loss improved from 0.40558 to 0.39738, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/135-0.3974.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2257 - acc: 0.9190 - val_loss: 0.3974 - val_acc: 0.8673\n",
      "Epoch 136/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9249\n",
      "Epoch 00136: val_loss did not improve from 0.39738\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2168 - acc: 0.9246 - val_loss: 0.4429 - val_acc: 0.8513\n",
      "Epoch 137/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9137\n",
      "Epoch 00137: val_loss did not improve from 0.39738\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2394 - acc: 0.9139 - val_loss: 0.4088 - val_acc: 0.8615\n",
      "Epoch 138/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9161\n",
      "Epoch 00138: val_loss did not improve from 0.39738\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2316 - acc: 0.9162 - val_loss: 0.4358 - val_acc: 0.8615\n",
      "Epoch 139/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9238\n",
      "Epoch 00139: val_loss improved from 0.39738 to 0.39593, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/139-0.3959.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2162 - acc: 0.9239 - val_loss: 0.3959 - val_acc: 0.8756\n",
      "Epoch 140/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9191\n",
      "Epoch 00140: val_loss did not improve from 0.39593\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2276 - acc: 0.9190 - val_loss: 0.4468 - val_acc: 0.8455\n",
      "Epoch 141/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9236\n",
      "Epoch 00141: val_loss did not improve from 0.39593\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2163 - acc: 0.9235 - val_loss: 0.4674 - val_acc: 0.8429\n",
      "Epoch 142/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9229\n",
      "Epoch 00142: val_loss did not improve from 0.39593\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2176 - acc: 0.9231 - val_loss: 0.4509 - val_acc: 0.8449\n",
      "Epoch 143/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9148\n",
      "Epoch 00143: val_loss did not improve from 0.39593\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2295 - acc: 0.9150 - val_loss: 0.4085 - val_acc: 0.8660\n",
      "Epoch 144/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9234\n",
      "Epoch 00144: val_loss did not improve from 0.39593\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2081 - acc: 0.9235 - val_loss: 0.4408 - val_acc: 0.8558\n",
      "Epoch 145/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9247\n",
      "Epoch 00145: val_loss did not improve from 0.39593\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2107 - acc: 0.9248 - val_loss: 0.4076 - val_acc: 0.8712\n",
      "Epoch 146/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9257\n",
      "Epoch 00146: val_loss did not improve from 0.39593\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2100 - acc: 0.9259 - val_loss: 0.4100 - val_acc: 0.8647\n",
      "Epoch 147/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9253\n",
      "Epoch 00147: val_loss did not improve from 0.39593\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2061 - acc: 0.9254 - val_loss: 0.4241 - val_acc: 0.8615\n",
      "Epoch 148/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9229\n",
      "Epoch 00148: val_loss did not improve from 0.39593\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2145 - acc: 0.9231 - val_loss: 0.4128 - val_acc: 0.8596\n",
      "Epoch 149/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9251\n",
      "Epoch 00149: val_loss improved from 0.39593 to 0.39515, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/149-0.3952.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2105 - acc: 0.9252 - val_loss: 0.3952 - val_acc: 0.8737\n",
      "Epoch 150/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9328\n",
      "Epoch 00150: val_loss did not improve from 0.39515\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1956 - acc: 0.9329 - val_loss: 0.4265 - val_acc: 0.8558\n",
      "Epoch 151/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9304\n",
      "Epoch 00151: val_loss did not improve from 0.39515\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2024 - acc: 0.9303 - val_loss: 0.4168 - val_acc: 0.8647\n",
      "Epoch 152/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9292\n",
      "Epoch 00152: val_loss did not improve from 0.39515\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1993 - acc: 0.9293 - val_loss: 0.4402 - val_acc: 0.8609\n",
      "Epoch 153/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9244\n",
      "Epoch 00153: val_loss did not improve from 0.39515\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1987 - acc: 0.9246 - val_loss: 0.4895 - val_acc: 0.8468\n",
      "Epoch 154/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9259\n",
      "Epoch 00154: val_loss did not improve from 0.39515\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2053 - acc: 0.9261 - val_loss: 0.4426 - val_acc: 0.8564\n",
      "Epoch 155/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9330\n",
      "Epoch 00155: val_loss did not improve from 0.39515\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1961 - acc: 0.9331 - val_loss: 0.4054 - val_acc: 0.8699\n",
      "Epoch 156/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9244\n",
      "Epoch 00156: val_loss did not improve from 0.39515\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2040 - acc: 0.9246 - val_loss: 0.4282 - val_acc: 0.8628\n",
      "Epoch 157/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9339\n",
      "Epoch 00157: val_loss did not improve from 0.39515\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1931 - acc: 0.9338 - val_loss: 0.4553 - val_acc: 0.8571\n",
      "Epoch 158/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9187\n",
      "Epoch 00158: val_loss improved from 0.39515 to 0.39511, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/158-0.3951.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2228 - acc: 0.9188 - val_loss: 0.3951 - val_acc: 0.8686\n",
      "Epoch 159/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9396\n",
      "Epoch 00159: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1841 - acc: 0.9397 - val_loss: 0.4268 - val_acc: 0.8571\n",
      "Epoch 160/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9274\n",
      "Epoch 00160: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.2016 - acc: 0.9276 - val_loss: 0.4721 - val_acc: 0.8532\n",
      "Epoch 161/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9366\n",
      "Epoch 00161: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1866 - acc: 0.9368 - val_loss: 0.4657 - val_acc: 0.8494\n",
      "Epoch 162/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9375\n",
      "Epoch 00162: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1764 - acc: 0.9374 - val_loss: 0.4365 - val_acc: 0.8609\n",
      "Epoch 163/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9377\n",
      "Epoch 00163: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1723 - acc: 0.9378 - val_loss: 0.4320 - val_acc: 0.8647\n",
      "Epoch 164/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9371\n",
      "Epoch 00164: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1804 - acc: 0.9372 - val_loss: 0.4431 - val_acc: 0.8545\n",
      "Epoch 165/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9313\n",
      "Epoch 00165: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1892 - acc: 0.9310 - val_loss: 0.4172 - val_acc: 0.8654\n",
      "Epoch 166/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9375\n",
      "Epoch 00166: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1763 - acc: 0.9374 - val_loss: 0.4105 - val_acc: 0.8628\n",
      "Epoch 167/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9300\n",
      "Epoch 00167: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1927 - acc: 0.9301 - val_loss: 0.4130 - val_acc: 0.8647\n",
      "Epoch 168/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9388\n",
      "Epoch 00168: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1801 - acc: 0.9389 - val_loss: 0.4105 - val_acc: 0.8673\n",
      "Epoch 169/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9399\n",
      "Epoch 00169: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1646 - acc: 0.9397 - val_loss: 0.4222 - val_acc: 0.8718\n",
      "Epoch 170/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9360\n",
      "Epoch 00170: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1793 - acc: 0.9361 - val_loss: 0.4266 - val_acc: 0.8609\n",
      "Epoch 171/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9356\n",
      "Epoch 00171: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1670 - acc: 0.9355 - val_loss: 0.4208 - val_acc: 0.8724\n",
      "Epoch 172/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9401\n",
      "Epoch 00172: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1758 - acc: 0.9397 - val_loss: 0.4210 - val_acc: 0.8660\n",
      "Epoch 173/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9369\n",
      "Epoch 00173: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1806 - acc: 0.9370 - val_loss: 0.4285 - val_acc: 0.8615\n",
      "Epoch 174/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9469\n",
      "Epoch 00174: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1511 - acc: 0.9470 - val_loss: 0.4276 - val_acc: 0.8654\n",
      "Epoch 175/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9399\n",
      "Epoch 00175: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1756 - acc: 0.9400 - val_loss: 0.4295 - val_acc: 0.8692\n",
      "Epoch 176/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9401\n",
      "Epoch 00176: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1661 - acc: 0.9402 - val_loss: 0.4731 - val_acc: 0.8571\n",
      "Epoch 177/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9426\n",
      "Epoch 00177: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1602 - acc: 0.9425 - val_loss: 0.4212 - val_acc: 0.8731\n",
      "Epoch 178/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9422\n",
      "Epoch 00178: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1675 - acc: 0.9423 - val_loss: 0.4131 - val_acc: 0.8660\n",
      "Epoch 179/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9349\n",
      "Epoch 00179: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1805 - acc: 0.9348 - val_loss: 0.4289 - val_acc: 0.8679\n",
      "Epoch 180/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9448\n",
      "Epoch 00180: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1578 - acc: 0.9449 - val_loss: 0.4340 - val_acc: 0.8718\n",
      "Epoch 181/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9431\n",
      "Epoch 00181: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1638 - acc: 0.9429 - val_loss: 0.4181 - val_acc: 0.8667\n",
      "Epoch 182/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9435\n",
      "Epoch 00182: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1556 - acc: 0.9436 - val_loss: 0.4570 - val_acc: 0.8603\n",
      "Epoch 183/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9431\n",
      "Epoch 00183: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1603 - acc: 0.9432 - val_loss: 0.4471 - val_acc: 0.8654\n",
      "Epoch 184/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9469\n",
      "Epoch 00184: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1482 - acc: 0.9470 - val_loss: 0.4270 - val_acc: 0.8667\n",
      "Epoch 185/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9518\n",
      "Epoch 00185: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1442 - acc: 0.9517 - val_loss: 0.4383 - val_acc: 0.8635\n",
      "Epoch 186/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9495\n",
      "Epoch 00186: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1435 - acc: 0.9496 - val_loss: 0.4205 - val_acc: 0.8731\n",
      "Epoch 187/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9499\n",
      "Epoch 00187: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1457 - acc: 0.9500 - val_loss: 0.4472 - val_acc: 0.8667\n",
      "Epoch 188/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9461\n",
      "Epoch 00188: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1523 - acc: 0.9462 - val_loss: 0.4097 - val_acc: 0.8724\n",
      "Epoch 189/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9473\n",
      "Epoch 00189: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1492 - acc: 0.9474 - val_loss: 0.4086 - val_acc: 0.8763\n",
      "Epoch 190/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9491\n",
      "Epoch 00190: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1423 - acc: 0.9491 - val_loss: 0.3962 - val_acc: 0.8763\n",
      "Epoch 191/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9486\n",
      "Epoch 00191: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1374 - acc: 0.9485 - val_loss: 0.4414 - val_acc: 0.8647\n",
      "Epoch 192/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9411\n",
      "Epoch 00192: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1725 - acc: 0.9412 - val_loss: 0.4272 - val_acc: 0.8750\n",
      "Epoch 193/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9516\n",
      "Epoch 00193: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1350 - acc: 0.9517 - val_loss: 0.5086 - val_acc: 0.8564\n",
      "Epoch 194/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9478\n",
      "Epoch 00194: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1476 - acc: 0.9476 - val_loss: 0.4772 - val_acc: 0.8532\n",
      "Epoch 195/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9508\n",
      "Epoch 00195: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1526 - acc: 0.9509 - val_loss: 0.4224 - val_acc: 0.8699\n",
      "Epoch 196/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9497\n",
      "Epoch 00196: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1428 - acc: 0.9498 - val_loss: 0.4160 - val_acc: 0.8705\n",
      "Epoch 197/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9458\n",
      "Epoch 00197: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1502 - acc: 0.9459 - val_loss: 0.4239 - val_acc: 0.8686\n",
      "Epoch 198/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9540\n",
      "Epoch 00198: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1357 - acc: 0.9541 - val_loss: 0.4276 - val_acc: 0.8699\n",
      "Epoch 199/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9497\n",
      "Epoch 00199: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1416 - acc: 0.9496 - val_loss: 0.4159 - val_acc: 0.8667\n",
      "Epoch 200/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9533\n",
      "Epoch 00200: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1368 - acc: 0.9532 - val_loss: 0.4107 - val_acc: 0.8737\n",
      "Epoch 201/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1532 - acc: 0.9429\n",
      "Epoch 00201: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1533 - acc: 0.9427 - val_loss: 0.4172 - val_acc: 0.8731\n",
      "Epoch 202/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9491\n",
      "Epoch 00202: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1441 - acc: 0.9491 - val_loss: 0.4723 - val_acc: 0.8571\n",
      "Epoch 203/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9521\n",
      "Epoch 00203: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1372 - acc: 0.9521 - val_loss: 0.4241 - val_acc: 0.8712\n",
      "Epoch 204/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9585\n",
      "Epoch 00204: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1229 - acc: 0.9583 - val_loss: 0.5127 - val_acc: 0.8558\n",
      "Epoch 205/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9452\n",
      "Epoch 00205: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1532 - acc: 0.9453 - val_loss: 0.4495 - val_acc: 0.8692\n",
      "Epoch 206/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9576\n",
      "Epoch 00206: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1283 - acc: 0.9577 - val_loss: 0.4405 - val_acc: 0.8712\n",
      "Epoch 207/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9578\n",
      "Epoch 00207: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1289 - acc: 0.9579 - val_loss: 0.4134 - val_acc: 0.8737\n",
      "Epoch 208/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9561\n",
      "Epoch 00208: val_loss did not improve from 0.39511\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1271 - acc: 0.9562 - val_loss: 0.4605 - val_acc: 0.8628\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560/1560 [==============================] - 4s 2ms/sample - loss: 0.3951 - acc: 0.8686\n",
      "Loss: 0.3951078218527329 Accuracy: 0.86858976\n",
      "\n",
      "Train on 4680 samples, validate on 1560 samples\n",
      "Epoch 1/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.7113 - acc: 0.2583\n",
      "Epoch 00001: val_loss improved from inf to 1.49941, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/001-1.4994.hdf5\n",
      "4680/4680 [==============================] - 18s 4ms/sample - loss: 1.7108 - acc: 0.2588 - val_loss: 1.4994 - val_acc: 0.4026\n",
      "Epoch 2/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.4652 - acc: 0.4018\n",
      "Epoch 00002: val_loss improved from 1.49941 to 1.33362, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/002-1.3336.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 1.4649 - acc: 0.4013 - val_loss: 1.3336 - val_acc: 0.4859\n",
      "Epoch 3/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.3429 - acc: 0.4636\n",
      "Epoch 00003: val_loss improved from 1.33362 to 1.20347, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/003-1.2035.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 1.3424 - acc: 0.4639 - val_loss: 1.2035 - val_acc: 0.5462\n",
      "Epoch 4/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.2638 - acc: 0.5054\n",
      "Epoch 00004: val_loss improved from 1.20347 to 1.13612, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/004-1.1361.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 1.2633 - acc: 0.5056 - val_loss: 1.1361 - val_acc: 0.5590\n",
      "Epoch 5/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.1984 - acc: 0.5321\n",
      "Epoch 00005: val_loss improved from 1.13612 to 1.05527, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/005-1.0553.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 1.1978 - acc: 0.5325 - val_loss: 1.0553 - val_acc: 0.5949\n",
      "Epoch 6/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.1318 - acc: 0.5741\n",
      "Epoch 00006: val_loss improved from 1.05527 to 1.04240, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/006-1.0424.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 1.1325 - acc: 0.5741 - val_loss: 1.0424 - val_acc: 0.6109\n",
      "Epoch 7/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.0964 - acc: 0.5893\n",
      "Epoch 00007: val_loss improved from 1.04240 to 1.01067, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/007-1.0107.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 1.0963 - acc: 0.5891 - val_loss: 1.0107 - val_acc: 0.6186\n",
      "Epoch 8/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.0404 - acc: 0.6128\n",
      "Epoch 00008: val_loss improved from 1.01067 to 0.94062, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/008-0.9406.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 1.0394 - acc: 0.6135 - val_loss: 0.9406 - val_acc: 0.6423\n",
      "Epoch 9/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 1.0067 - acc: 0.6378\n",
      "Epoch 00009: val_loss improved from 0.94062 to 0.91385, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/009-0.9138.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 1.0066 - acc: 0.6378 - val_loss: 0.9138 - val_acc: 0.6583\n",
      "Epoch 10/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9759 - acc: 0.6430\n",
      "Epoch 00010: val_loss improved from 0.91385 to 0.88423, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/010-0.8842.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.9755 - acc: 0.6429 - val_loss: 0.8842 - val_acc: 0.6756\n",
      "Epoch 11/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9398 - acc: 0.6663\n",
      "Epoch 00011: val_loss improved from 0.88423 to 0.85883, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/011-0.8588.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.9397 - acc: 0.6662 - val_loss: 0.8588 - val_acc: 0.6891\n",
      "Epoch 12/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.9281 - acc: 0.6682\n",
      "Epoch 00012: val_loss improved from 0.85883 to 0.82221, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/012-0.8222.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.9277 - acc: 0.6686 - val_loss: 0.8222 - val_acc: 0.6962\n",
      "Epoch 13/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8726 - acc: 0.6899\n",
      "Epoch 00013: val_loss improved from 0.82221 to 0.80177, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/013-0.8018.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.8725 - acc: 0.6895 - val_loss: 0.8018 - val_acc: 0.7090\n",
      "Epoch 14/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8469 - acc: 0.7033\n",
      "Epoch 00014: val_loss did not improve from 0.80177\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.8463 - acc: 0.7036 - val_loss: 0.8217 - val_acc: 0.6859\n",
      "Epoch 15/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8316 - acc: 0.7006\n",
      "Epoch 00015: val_loss improved from 0.80177 to 0.76281, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/015-0.7628.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.8316 - acc: 0.7004 - val_loss: 0.7628 - val_acc: 0.7231\n",
      "Epoch 16/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.8127 - acc: 0.7177\n",
      "Epoch 00016: val_loss did not improve from 0.76281\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.8120 - acc: 0.7179 - val_loss: 0.7872 - val_acc: 0.7077\n",
      "Epoch 17/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7871 - acc: 0.7226\n",
      "Epoch 00017: val_loss did not improve from 0.76281\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.7868 - acc: 0.7224 - val_loss: 0.8109 - val_acc: 0.6981\n",
      "Epoch 18/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7711 - acc: 0.7230\n",
      "Epoch 00018: val_loss improved from 0.76281 to 0.74223, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/018-0.7422.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.7712 - acc: 0.7231 - val_loss: 0.7422 - val_acc: 0.7179\n",
      "Epoch 19/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7381 - acc: 0.7434\n",
      "Epoch 00019: val_loss improved from 0.74223 to 0.71002, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/019-0.7100.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.7381 - acc: 0.7432 - val_loss: 0.7100 - val_acc: 0.7468\n",
      "Epoch 20/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7407 - acc: 0.7354\n",
      "Epoch 00020: val_loss improved from 0.71002 to 0.70314, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/020-0.7031.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.7411 - acc: 0.7355 - val_loss: 0.7031 - val_acc: 0.7462\n",
      "Epoch 21/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.7182 - acc: 0.7547\n",
      "Epoch 00021: val_loss improved from 0.70314 to 0.69168, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/021-0.6917.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.7185 - acc: 0.7545 - val_loss: 0.6917 - val_acc: 0.7506\n",
      "Epoch 22/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6792 - acc: 0.7571\n",
      "Epoch 00022: val_loss improved from 0.69168 to 0.68149, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/022-0.6815.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.6788 - acc: 0.7573 - val_loss: 0.6815 - val_acc: 0.7468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6611 - acc: 0.7735\n",
      "Epoch 00023: val_loss improved from 0.68149 to 0.64114, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/023-0.6411.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.6610 - acc: 0.7735 - val_loss: 0.6411 - val_acc: 0.7699\n",
      "Epoch 24/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6415 - acc: 0.7735\n",
      "Epoch 00024: val_loss did not improve from 0.64114\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.6416 - acc: 0.7737 - val_loss: 0.6982 - val_acc: 0.7494\n",
      "Epoch 25/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6603 - acc: 0.7665\n",
      "Epoch 00025: val_loss did not improve from 0.64114\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.6600 - acc: 0.7669 - val_loss: 0.6478 - val_acc: 0.7699\n",
      "Epoch 26/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6332 - acc: 0.7849\n",
      "Epoch 00026: val_loss improved from 0.64114 to 0.60788, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/026-0.6079.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.6331 - acc: 0.7850 - val_loss: 0.6079 - val_acc: 0.7744\n",
      "Epoch 27/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5954 - acc: 0.7928\n",
      "Epoch 00027: val_loss did not improve from 0.60788\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.5961 - acc: 0.7923 - val_loss: 0.6081 - val_acc: 0.7853\n",
      "Epoch 28/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.6060 - acc: 0.7894\n",
      "Epoch 00028: val_loss improved from 0.60788 to 0.59587, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/028-0.5959.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.6056 - acc: 0.7893 - val_loss: 0.5959 - val_acc: 0.7833\n",
      "Epoch 29/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5704 - acc: 0.8014\n",
      "Epoch 00029: val_loss did not improve from 0.59587\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.5708 - acc: 0.8013 - val_loss: 0.6668 - val_acc: 0.7513\n",
      "Epoch 30/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5611 - acc: 0.8014\n",
      "Epoch 00030: val_loss did not improve from 0.59587\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.5614 - acc: 0.8013 - val_loss: 0.6296 - val_acc: 0.7692\n",
      "Epoch 31/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5475 - acc: 0.8099\n",
      "Epoch 00031: val_loss did not improve from 0.59587\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.5470 - acc: 0.8100 - val_loss: 0.5965 - val_acc: 0.7827\n",
      "Epoch 32/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.8005\n",
      "Epoch 00032: val_loss did not improve from 0.59587\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.5546 - acc: 0.8006 - val_loss: 0.6156 - val_acc: 0.7763\n",
      "Epoch 33/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.8198\n",
      "Epoch 00033: val_loss improved from 0.59587 to 0.57637, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/033-0.5764.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.5213 - acc: 0.8201 - val_loss: 0.5764 - val_acc: 0.7962\n",
      "Epoch 34/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.8172\n",
      "Epoch 00034: val_loss did not improve from 0.57637\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.5198 - acc: 0.8173 - val_loss: 0.5835 - val_acc: 0.7897\n",
      "Epoch 35/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5203 - acc: 0.8172\n",
      "Epoch 00035: val_loss improved from 0.57637 to 0.56373, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/035-0.5637.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.5204 - acc: 0.8171 - val_loss: 0.5637 - val_acc: 0.7897\n",
      "Epoch 36/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4944 - acc: 0.8232\n",
      "Epoch 00036: val_loss improved from 0.56373 to 0.55329, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/036-0.5533.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4942 - acc: 0.8233 - val_loss: 0.5533 - val_acc: 0.7981\n",
      "Epoch 37/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8315\n",
      "Epoch 00037: val_loss did not improve from 0.55329\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.5031 - acc: 0.8314 - val_loss: 0.6548 - val_acc: 0.7673\n",
      "Epoch 38/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.8181\n",
      "Epoch 00038: val_loss improved from 0.55329 to 0.54005, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/038-0.5400.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.5146 - acc: 0.8179 - val_loss: 0.5400 - val_acc: 0.7987\n",
      "Epoch 39/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4823 - acc: 0.8279\n",
      "Epoch 00039: val_loss improved from 0.54005 to 0.52100, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/039-0.5210.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.4819 - acc: 0.8282 - val_loss: 0.5210 - val_acc: 0.8077\n",
      "Epoch 40/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.8405\n",
      "Epoch 00040: val_loss did not improve from 0.52100\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4604 - acc: 0.8408 - val_loss: 0.5239 - val_acc: 0.8026\n",
      "Epoch 41/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4586 - acc: 0.8354\n",
      "Epoch 00041: val_loss did not improve from 0.52100\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4583 - acc: 0.8355 - val_loss: 0.5582 - val_acc: 0.7974\n",
      "Epoch 42/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4632 - acc: 0.8320\n",
      "Epoch 00042: val_loss did not improve from 0.52100\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4638 - acc: 0.8318 - val_loss: 0.5804 - val_acc: 0.7840\n",
      "Epoch 43/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4515 - acc: 0.8384\n",
      "Epoch 00043: val_loss did not improve from 0.52100\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.4515 - acc: 0.8385 - val_loss: 0.5503 - val_acc: 0.8045\n",
      "Epoch 44/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4343 - acc: 0.8423\n",
      "Epoch 00044: val_loss did not improve from 0.52100\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4354 - acc: 0.8421 - val_loss: 0.6081 - val_acc: 0.7885\n",
      "Epoch 45/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4434 - acc: 0.8408\n",
      "Epoch 00045: val_loss did not improve from 0.52100\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4452 - acc: 0.8402 - val_loss: 0.5259 - val_acc: 0.8109\n",
      "Epoch 46/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4542 - acc: 0.8416\n",
      "Epoch 00046: val_loss improved from 0.52100 to 0.50919, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/046-0.5092.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.4543 - acc: 0.8415 - val_loss: 0.5092 - val_acc: 0.8147\n",
      "Epoch 47/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8587\n",
      "Epoch 00047: val_loss improved from 0.50919 to 0.49895, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/047-0.4990.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4069 - acc: 0.8590 - val_loss: 0.4990 - val_acc: 0.8160\n",
      "Epoch 48/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8596\n",
      "Epoch 00048: val_loss did not improve from 0.49895\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.4115 - acc: 0.8596 - val_loss: 0.5350 - val_acc: 0.8128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8508\n",
      "Epoch 00049: val_loss did not improve from 0.49895\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.4109 - acc: 0.8506 - val_loss: 0.5228 - val_acc: 0.8147\n",
      "Epoch 50/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8647\n",
      "Epoch 00050: val_loss did not improve from 0.49895\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3943 - acc: 0.8647 - val_loss: 0.5700 - val_acc: 0.7929\n",
      "Epoch 51/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8570\n",
      "Epoch 00051: val_loss did not improve from 0.49895\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.3908 - acc: 0.8573 - val_loss: 0.5049 - val_acc: 0.8160\n",
      "Epoch 52/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8632\n",
      "Epoch 00052: val_loss did not improve from 0.49895\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3880 - acc: 0.8635 - val_loss: 0.5766 - val_acc: 0.8006\n",
      "Epoch 53/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8729\n",
      "Epoch 00053: val_loss improved from 0.49895 to 0.49708, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/053-0.4971.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3677 - acc: 0.8731 - val_loss: 0.4971 - val_acc: 0.8154\n",
      "Epoch 54/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.8756\n",
      "Epoch 00054: val_loss did not improve from 0.49708\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3570 - acc: 0.8756 - val_loss: 0.5751 - val_acc: 0.7981\n",
      "Epoch 55/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.8703\n",
      "Epoch 00055: val_loss did not improve from 0.49708\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.3693 - acc: 0.8705 - val_loss: 0.5058 - val_acc: 0.8192\n",
      "Epoch 56/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8602\n",
      "Epoch 00056: val_loss improved from 0.49708 to 0.49395, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/056-0.4940.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3823 - acc: 0.8605 - val_loss: 0.4940 - val_acc: 0.8135\n",
      "Epoch 57/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.8756\n",
      "Epoch 00057: val_loss improved from 0.49395 to 0.49023, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/057-0.4902.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3371 - acc: 0.8759 - val_loss: 0.4902 - val_acc: 0.8256\n",
      "Epoch 58/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.8868\n",
      "Epoch 00058: val_loss did not improve from 0.49023\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3295 - acc: 0.8870 - val_loss: 0.4930 - val_acc: 0.8314\n",
      "Epoch 59/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.8934\n",
      "Epoch 00059: val_loss improved from 0.49023 to 0.48532, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/059-0.4853.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3110 - acc: 0.8936 - val_loss: 0.4853 - val_acc: 0.8276\n",
      "Epoch 60/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.8902\n",
      "Epoch 00060: val_loss did not improve from 0.48532\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.3179 - acc: 0.8902 - val_loss: 0.4937 - val_acc: 0.8256\n",
      "Epoch 61/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.8975\n",
      "Epoch 00061: val_loss improved from 0.48532 to 0.48298, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/061-0.4830.hdf5\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2970 - acc: 0.8974 - val_loss: 0.4830 - val_acc: 0.8212\n",
      "Epoch 62/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.8915\n",
      "Epoch 00062: val_loss did not improve from 0.48298\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3076 - acc: 0.8917 - val_loss: 0.4944 - val_acc: 0.8295\n",
      "Epoch 63/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.8960\n",
      "Epoch 00063: val_loss did not improve from 0.48298\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2975 - acc: 0.8957 - val_loss: 0.5189 - val_acc: 0.8295\n",
      "Epoch 64/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3336 - acc: 0.8846\n",
      "Epoch 00064: val_loss improved from 0.48298 to 0.46759, saving model to model/checkpoint/vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/064-0.4676.hdf5\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.3339 - acc: 0.8846 - val_loss: 0.4676 - val_acc: 0.8340\n",
      "Epoch 65/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.8977\n",
      "Epoch 00065: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2948 - acc: 0.8976 - val_loss: 0.4811 - val_acc: 0.8353\n",
      "Epoch 66/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.8923\n",
      "Epoch 00066: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2923 - acc: 0.8921 - val_loss: 0.4883 - val_acc: 0.8263\n",
      "Epoch 67/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.8936\n",
      "Epoch 00067: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.3005 - acc: 0.8934 - val_loss: 0.5107 - val_acc: 0.8256\n",
      "Epoch 68/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.3008 - acc: 0.8973\n",
      "Epoch 00068: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.3006 - acc: 0.8974 - val_loss: 0.5100 - val_acc: 0.8205\n",
      "Epoch 69/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9073\n",
      "Epoch 00069: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2747 - acc: 0.9073 - val_loss: 0.4854 - val_acc: 0.8327\n",
      "Epoch 70/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9058\n",
      "Epoch 00070: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2727 - acc: 0.9056 - val_loss: 0.5531 - val_acc: 0.8205\n",
      "Epoch 71/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.9065\n",
      "Epoch 00071: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2675 - acc: 0.9064 - val_loss: 0.5486 - val_acc: 0.8141\n",
      "Epoch 72/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.9110\n",
      "Epoch 00072: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2554 - acc: 0.9109 - val_loss: 0.4842 - val_acc: 0.8365\n",
      "Epoch 73/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.9077\n",
      "Epoch 00073: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2534 - acc: 0.9079 - val_loss: 0.4761 - val_acc: 0.8308\n",
      "Epoch 74/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9163\n",
      "Epoch 00074: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2477 - acc: 0.9165 - val_loss: 0.5041 - val_acc: 0.8314\n",
      "Epoch 75/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9142\n",
      "Epoch 00075: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2484 - acc: 0.9141 - val_loss: 0.4774 - val_acc: 0.8314\n",
      "Epoch 76/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9107\n",
      "Epoch 00076: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2451 - acc: 0.9107 - val_loss: 0.4905 - val_acc: 0.8301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9174\n",
      "Epoch 00077: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2279 - acc: 0.9175 - val_loss: 0.4760 - val_acc: 0.8333\n",
      "Epoch 78/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9204\n",
      "Epoch 00078: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2318 - acc: 0.9203 - val_loss: 0.5091 - val_acc: 0.8282\n",
      "Epoch 79/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9206\n",
      "Epoch 00079: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2268 - acc: 0.9205 - val_loss: 0.5062 - val_acc: 0.8212\n",
      "Epoch 80/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9255\n",
      "Epoch 00080: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.2136 - acc: 0.9254 - val_loss: 0.4955 - val_acc: 0.8327\n",
      "Epoch 81/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9283\n",
      "Epoch 00081: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2093 - acc: 0.9284 - val_loss: 0.5130 - val_acc: 0.8359\n",
      "Epoch 82/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9268\n",
      "Epoch 00082: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2098 - acc: 0.9269 - val_loss: 0.4936 - val_acc: 0.8308\n",
      "Epoch 83/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9375\n",
      "Epoch 00083: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1797 - acc: 0.9372 - val_loss: 0.4995 - val_acc: 0.8333\n",
      "Epoch 84/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9341\n",
      "Epoch 00084: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1991 - acc: 0.9342 - val_loss: 0.5579 - val_acc: 0.8147\n",
      "Epoch 85/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9309\n",
      "Epoch 00085: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2022 - acc: 0.9308 - val_loss: 0.5269 - val_acc: 0.8288\n",
      "Epoch 86/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9354\n",
      "Epoch 00086: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1925 - acc: 0.9355 - val_loss: 0.4975 - val_acc: 0.8391\n",
      "Epoch 87/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9332\n",
      "Epoch 00087: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1828 - acc: 0.9333 - val_loss: 0.5616 - val_acc: 0.8224\n",
      "Epoch 88/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9259\n",
      "Epoch 00088: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.2014 - acc: 0.9259 - val_loss: 0.4795 - val_acc: 0.8301\n",
      "Epoch 89/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9375\n",
      "Epoch 00089: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1844 - acc: 0.9376 - val_loss: 0.4914 - val_acc: 0.8481\n",
      "Epoch 90/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9366\n",
      "Epoch 00090: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1761 - acc: 0.9368 - val_loss: 0.5408 - val_acc: 0.8327\n",
      "Epoch 91/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9373\n",
      "Epoch 00091: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1810 - acc: 0.9374 - val_loss: 0.4997 - val_acc: 0.8327\n",
      "Epoch 92/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9422\n",
      "Epoch 00092: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1664 - acc: 0.9417 - val_loss: 0.5108 - val_acc: 0.8423\n",
      "Epoch 93/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9411\n",
      "Epoch 00093: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1683 - acc: 0.9412 - val_loss: 0.5938 - val_acc: 0.8218\n",
      "Epoch 94/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9418\n",
      "Epoch 00094: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1732 - acc: 0.9419 - val_loss: 0.5450 - val_acc: 0.8224\n",
      "Epoch 95/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9347\n",
      "Epoch 00095: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1875 - acc: 0.9346 - val_loss: 0.5033 - val_acc: 0.8417\n",
      "Epoch 96/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9416\n",
      "Epoch 00096: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1624 - acc: 0.9417 - val_loss: 0.5856 - val_acc: 0.8128\n",
      "Epoch 97/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9392\n",
      "Epoch 00097: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1698 - acc: 0.9393 - val_loss: 0.4916 - val_acc: 0.8429\n",
      "Epoch 98/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9471\n",
      "Epoch 00098: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1513 - acc: 0.9470 - val_loss: 0.5504 - val_acc: 0.8359\n",
      "Epoch 99/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9456\n",
      "Epoch 00099: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1555 - acc: 0.9455 - val_loss: 0.5113 - val_acc: 0.8468\n",
      "Epoch 100/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9516\n",
      "Epoch 00100: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1450 - acc: 0.9515 - val_loss: 0.5265 - val_acc: 0.8359\n",
      "Epoch 101/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9435\n",
      "Epoch 00101: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1542 - acc: 0.9436 - val_loss: 0.5151 - val_acc: 0.8385\n",
      "Epoch 102/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9516\n",
      "Epoch 00102: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1360 - acc: 0.9517 - val_loss: 0.5649 - val_acc: 0.8391\n",
      "Epoch 103/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9572\n",
      "Epoch 00103: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 3ms/sample - loss: 0.1297 - acc: 0.9573 - val_loss: 0.5912 - val_acc: 0.8308\n",
      "Epoch 104/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9621\n",
      "Epoch 00104: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1219 - acc: 0.9620 - val_loss: 0.5630 - val_acc: 0.8346\n",
      "Epoch 105/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9497\n",
      "Epoch 00105: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1478 - acc: 0.9496 - val_loss: 0.5675 - val_acc: 0.8391\n",
      "Epoch 106/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9523\n",
      "Epoch 00106: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1399 - acc: 0.9524 - val_loss: 0.5178 - val_acc: 0.8417\n",
      "Epoch 107/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9570\n",
      "Epoch 00107: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1205 - acc: 0.9571 - val_loss: 0.5482 - val_acc: 0.8378\n",
      "Epoch 108/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9595\n",
      "Epoch 00108: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1216 - acc: 0.9596 - val_loss: 0.5525 - val_acc: 0.8378\n",
      "Epoch 109/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9561\n",
      "Epoch 00109: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 16s 4ms/sample - loss: 0.1209 - acc: 0.9560 - val_loss: 0.5248 - val_acc: 0.8410\n",
      "Epoch 110/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9576\n",
      "Epoch 00110: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1286 - acc: 0.9577 - val_loss: 0.5269 - val_acc: 0.8487\n",
      "Epoch 111/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9604\n",
      "Epoch 00111: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1124 - acc: 0.9603 - val_loss: 0.5401 - val_acc: 0.8340\n",
      "Epoch 112/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9643\n",
      "Epoch 00112: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1125 - acc: 0.9643 - val_loss: 0.5627 - val_acc: 0.8436\n",
      "Epoch 113/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9572\n",
      "Epoch 00113: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1298 - acc: 0.9571 - val_loss: 0.5220 - val_acc: 0.8481\n",
      "Epoch 114/500\n",
      "4672/4680 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9628\n",
      "Epoch 00114: val_loss did not improve from 0.46759\n",
      "4680/4680 [==============================] - 17s 4ms/sample - loss: 0.1097 - acc: 0.9628 - val_loss: 0.5108 - val_acc: 0.8487\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560/1560 [==============================] - 4s 2ms/sample - loss: 0.4676 - acc: 0.8340\n",
      "Loss: 0.46759488979975383 Accuracy: 0.83397436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 6):\n",
    "    base = 'vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_2d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train, y_train, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val, y_val], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 46208)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 46208)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 277254    \n",
      "=================================================================\n",
      "Total params: 305,318\n",
      "Trainable params: 305,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1560/1560 [==============================] - 4s 2ms/sample - loss: 0.7398 - acc: 0.7513\n",
      "Loss: 0.7398333268287854 Accuracy: 0.75128204\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 38, 38, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 64902     \n",
      "=================================================================\n",
      "Total params: 144,230\n",
      "Trainable params: 144,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1560/1560 [==============================] - 4s 2ms/sample - loss: 0.5024 - acc: 0.8282\n",
      "Loss: 0.502409255810273 Accuracy: 0.8282051\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 38, 38, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 13, 13, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 9606      \n",
      "=================================================================\n",
      "Total params: 191,398\n",
      "Trainable params: 191,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1560/1560 [==============================] - 4s 2ms/sample - loss: 0.3951 - acc: 0.8686\n",
      "Loss: 0.3951078218527329 Accuracy: 0.86858976\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 38, 38, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 13, 13, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 5, 5, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 389,798\n",
      "Trainable params: 389,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560/1560 [==============================] - 4s 2ms/sample - loss: 0.4676 - acc: 0.8340\n",
      "Loss: 0.46759488979975383 Accuracy: 0.83397436\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = 'vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(2, 6):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 46208)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 46208)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 277254    \n",
      "=================================================================\n",
      "Total params: 305,318\n",
      "Trainable params: 305,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1560/1560 [==============================] - 4s 2ms/sample - loss: 0.9402 - acc: 0.7353\n",
      "Loss: 0.9402459184328715 Accuracy: 0.73525643\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 38, 38, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 64902     \n",
      "=================================================================\n",
      "Total params: 144,230\n",
      "Trainable params: 144,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1560/1560 [==============================] - 4s 2ms/sample - loss: 0.5290 - acc: 0.8333\n",
      "Loss: 0.5289722059017573 Accuracy: 0.8333333\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 38, 38, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 13, 13, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 9606      \n",
      "=================================================================\n",
      "Total params: 191,398\n",
      "Trainable params: 191,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1560/1560 [==============================] - 4s 3ms/sample - loss: 0.4605 - acc: 0.8628\n",
      "Loss: 0.46049604423535173 Accuracy: 0.8628205\n",
      "\n",
      "vis_imagenet_6_class_2D_CNN_custom_ch_32_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 341, 341, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 341, 341, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 341, 341, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 114, 114, 32)      25632     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 114, 114, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 38, 38, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 13, 13, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 5, 5, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 389,798\n",
      "Trainable params: 389,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560/1560 [==============================] - 4s 2ms/sample - loss: 0.5108 - acc: 0.8487\n",
      "Loss: 0.5108203684672331 Accuracy: 0.8487179\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 6):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
