{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from  tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mnist.load_data(path='mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data[0][0][:,:,:,np.newaxis]\n",
    "y_data = data[0][1]\n",
    "x_test = data[1][0][:,:,:,np.newaxis]\n",
    "y_test = data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40200, 28, 28, 1),\n",
       " (40200, 10),\n",
       " (19800, 28, 28, 1),\n",
       " (19800, 10),\n",
       " (10000, 28, 28, 1),\n",
       " (10000, 10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.33, random_state=42, stratify=y_data)\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list = np.unique(data[1][1])\n",
    "y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_list.shape[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# def build_2d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "#     input_layer = Input(shape=input_shape)\n",
    "#     x = input_layer\n",
    "\n",
    "#     for i in range(conv_num):\n",
    "#         x = Conv2D(kernel_size=3, filters=32*(2**(i//2)), strides=(1,1), padding='same')(x)\n",
    "# #         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "#         x = MaxPooling2D(pool_size=2, strides=(2,2), padding='same')(x)\n",
    "\n",
    "#     x = Flatten()(x)\n",
    "  \n",
    "#     x = Dropout(0.75)(x)\n",
    "#     output_layer = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "#     model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    for i in range(conv_num):\n",
    "        x = Conv2D(kernel_size=3, filters=32*(2**(i//2)), strides=(1,1), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling2D(pool_size=2, strides=(2,2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "  \n",
    "    x = Dropout(0.75)(x)\n",
    "    output_layer = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 63,050\n",
      "Trainable params: 63,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                15690     \n",
      "=================================================================\n",
      "Total params: 25,258\n",
      "Trainable params: 25,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 38,314\n",
      "Trainable params: 38,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 67,562\n",
      "Trainable params: 67,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 140,138\n",
      "Trainable params: 140,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model = build_2d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40200 samples, validate on 19800 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 9.3246 - acc: 0.3868\n",
      "Epoch 00001: val_loss improved from inf to 3.39675, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/001-3.3967.hdf5\n",
      "40200/40200 [==============================] - 5s 129us/sample - loss: 9.2950 - acc: 0.3886 - val_loss: 3.3967 - val_acc: 0.7620\n",
      "Epoch 2/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 4.6261 - acc: 0.6814\n",
      "Epoch 00002: val_loss improved from 3.39675 to 1.87175, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/002-1.8717.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 4.6023 - acc: 0.6828 - val_loss: 1.8717 - val_acc: 0.8559\n",
      "Epoch 3/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 2.7850 - acc: 0.7963\n",
      "Epoch 00003: val_loss improved from 1.87175 to 1.19888, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/003-1.1989.hdf5\n",
      "40200/40200 [==============================] - 4s 95us/sample - loss: 2.7836 - acc: 0.7964 - val_loss: 1.1989 - val_acc: 0.9080\n",
      "Epoch 4/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 2.0361 - acc: 0.8465\n",
      "Epoch 00004: val_loss improved from 1.19888 to 0.98015, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/004-0.9801.hdf5\n",
      "40200/40200 [==============================] - 4s 96us/sample - loss: 2.0289 - acc: 0.8470 - val_loss: 0.9801 - val_acc: 0.9227\n",
      "Epoch 5/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 1.6904 - acc: 0.8713\n",
      "Epoch 00005: val_loss improved from 0.98015 to 0.81900, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/005-0.8190.hdf5\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 1.6894 - acc: 0.8714 - val_loss: 0.8190 - val_acc: 0.9350\n",
      "Epoch 6/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 1.4160 - acc: 0.8886\n",
      "Epoch 00006: val_loss improved from 0.81900 to 0.71537, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/006-0.7154.hdf5\n",
      "40200/40200 [==============================] - 4s 95us/sample - loss: 1.4161 - acc: 0.8886 - val_loss: 0.7154 - val_acc: 0.9415\n",
      "Epoch 7/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 1.2445 - acc: 0.8995\n",
      "Epoch 00007: val_loss improved from 0.71537 to 0.61439, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/007-0.6144.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 1.2462 - acc: 0.8992 - val_loss: 0.6144 - val_acc: 0.9484\n",
      "Epoch 8/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 1.0951 - acc: 0.9088\n",
      "Epoch 00008: val_loss improved from 0.61439 to 0.55857, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/008-0.5586.hdf5\n",
      "40200/40200 [==============================] - 4s 95us/sample - loss: 1.0926 - acc: 0.9089 - val_loss: 0.5586 - val_acc: 0.9521\n",
      "Epoch 9/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.9664 - acc: 0.9166\n",
      "Epoch 00009: val_loss improved from 0.55857 to 0.49662, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/009-0.4966.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.9670 - acc: 0.9166 - val_loss: 0.4966 - val_acc: 0.9565\n",
      "Epoch 10/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.8776 - acc: 0.9229\n",
      "Epoch 00010: val_loss improved from 0.49662 to 0.44450, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/010-0.4445.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.8777 - acc: 0.9229 - val_loss: 0.4445 - val_acc: 0.9591\n",
      "Epoch 11/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.8119 - acc: 0.9262\n",
      "Epoch 00011: val_loss improved from 0.44450 to 0.39874, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/011-0.3987.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.8097 - acc: 0.9263 - val_loss: 0.3987 - val_acc: 0.9622\n",
      "Epoch 12/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.7342 - acc: 0.9298\n",
      "Epoch 00012: val_loss improved from 0.39874 to 0.36259, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/012-0.3626.hdf5\n",
      "40200/40200 [==============================] - 4s 96us/sample - loss: 0.7323 - acc: 0.9300 - val_loss: 0.3626 - val_acc: 0.9631\n",
      "Epoch 13/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.6569 - acc: 0.9337\n",
      "Epoch 00013: val_loss improved from 0.36259 to 0.31843, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/013-0.3184.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.6571 - acc: 0.9336 - val_loss: 0.3184 - val_acc: 0.9663\n",
      "Epoch 14/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.5889 - acc: 0.9374\n",
      "Epoch 00014: val_loss improved from 0.31843 to 0.28443, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/014-0.2844.hdf5\n",
      "40200/40200 [==============================] - 4s 90us/sample - loss: 0.5870 - acc: 0.9375 - val_loss: 0.2844 - val_acc: 0.9686\n",
      "Epoch 15/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.5437 - acc: 0.9387\n",
      "Epoch 00015: val_loss improved from 0.28443 to 0.24933, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/015-0.2493.hdf5\n",
      "40200/40200 [==============================] - 3s 85us/sample - loss: 0.5436 - acc: 0.9387 - val_loss: 0.2493 - val_acc: 0.9702\n",
      "Epoch 16/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.4763 - acc: 0.9410\n",
      "Epoch 00016: val_loss improved from 0.24933 to 0.21886, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/016-0.2189.hdf5\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.4750 - acc: 0.9411 - val_loss: 0.2189 - val_acc: 0.9706\n",
      "Epoch 17/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.9430\n",
      "Epoch 00017: val_loss improved from 0.21886 to 0.18766, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/017-0.1877.hdf5\n",
      "40200/40200 [==============================] - 4s 95us/sample - loss: 0.4163 - acc: 0.9430 - val_loss: 0.1877 - val_acc: 0.9721\n",
      "Epoch 18/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.3856 - acc: 0.9441\n",
      "Epoch 00018: val_loss improved from 0.18766 to 0.16527, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/018-0.1653.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.3858 - acc: 0.9441 - val_loss: 0.1653 - val_acc: 0.9734\n",
      "Epoch 19/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.9446\n",
      "Epoch 00019: val_loss improved from 0.16527 to 0.14464, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/019-0.1446.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.3270 - acc: 0.9446 - val_loss: 0.1446 - val_acc: 0.9735\n",
      "Epoch 20/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9469\n",
      "Epoch 00020: val_loss improved from 0.14464 to 0.12480, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/020-0.1248.hdf5\n",
      "40200/40200 [==============================] - 4s 88us/sample - loss: 0.2752 - acc: 0.9468 - val_loss: 0.1248 - val_acc: 0.9741\n",
      "Epoch 21/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.2524 - acc: 0.9462\n",
      "Epoch 00021: val_loss improved from 0.12480 to 0.11132, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/021-0.1113.hdf5\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.2523 - acc: 0.9461 - val_loss: 0.1113 - val_acc: 0.9747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9478\n",
      "Epoch 00022: val_loss improved from 0.11132 to 0.10095, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/022-0.1009.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.2270 - acc: 0.9477 - val_loss: 0.1009 - val_acc: 0.9743\n",
      "Epoch 23/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9492\n",
      "Epoch 00023: val_loss improved from 0.10095 to 0.09270, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/023-0.0927.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.1975 - acc: 0.9490 - val_loss: 0.0927 - val_acc: 0.9750\n",
      "Epoch 24/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9511\n",
      "Epoch 00024: val_loss improved from 0.09270 to 0.08963, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/024-0.0896.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.1773 - acc: 0.9512 - val_loss: 0.0896 - val_acc: 0.9758\n",
      "Epoch 25/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9547\n",
      "Epoch 00025: val_loss improved from 0.08963 to 0.08356, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/025-0.0836.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.1649 - acc: 0.9547 - val_loss: 0.0836 - val_acc: 0.9763\n",
      "Epoch 26/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9565\n",
      "Epoch 00026: val_loss improved from 0.08356 to 0.08150, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/026-0.0815.hdf5\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.1554 - acc: 0.9565 - val_loss: 0.0815 - val_acc: 0.9770\n",
      "Epoch 27/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9565\n",
      "Epoch 00027: val_loss improved from 0.08150 to 0.07908, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/027-0.0791.hdf5\n",
      "40200/40200 [==============================] - 4s 95us/sample - loss: 0.1481 - acc: 0.9564 - val_loss: 0.0791 - val_acc: 0.9774\n",
      "Epoch 28/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9584\n",
      "Epoch 00028: val_loss improved from 0.07908 to 0.07664, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/028-0.0766.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.1394 - acc: 0.9584 - val_loss: 0.0766 - val_acc: 0.9781\n",
      "Epoch 29/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9603\n",
      "Epoch 00029: val_loss improved from 0.07664 to 0.07404, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/029-0.0740.hdf5\n",
      "40200/40200 [==============================] - 4s 97us/sample - loss: 0.1326 - acc: 0.9603 - val_loss: 0.0740 - val_acc: 0.9772\n",
      "Epoch 30/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9623\n",
      "Epoch 00030: val_loss improved from 0.07404 to 0.07329, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/030-0.0733.hdf5\n",
      "40200/40200 [==============================] - 4s 90us/sample - loss: 0.1284 - acc: 0.9623 - val_loss: 0.0733 - val_acc: 0.9785\n",
      "Epoch 31/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9652\n",
      "Epoch 00031: val_loss improved from 0.07329 to 0.07210, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/031-0.0721.hdf5\n",
      "40200/40200 [==============================] - 3s 85us/sample - loss: 0.1198 - acc: 0.9652 - val_loss: 0.0721 - val_acc: 0.9790\n",
      "Epoch 32/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9640\n",
      "Epoch 00032: val_loss improved from 0.07210 to 0.07192, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/032-0.0719.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.1197 - acc: 0.9640 - val_loss: 0.0719 - val_acc: 0.9791\n",
      "Epoch 33/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9656\n",
      "Epoch 00033: val_loss improved from 0.07192 to 0.07070, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/033-0.0707.hdf5\n",
      "40200/40200 [==============================] - 4s 95us/sample - loss: 0.1144 - acc: 0.9655 - val_loss: 0.0707 - val_acc: 0.9791\n",
      "Epoch 34/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9657\n",
      "Epoch 00034: val_loss did not improve from 0.07070\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.1131 - acc: 0.9657 - val_loss: 0.0711 - val_acc: 0.9793\n",
      "Epoch 35/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9652\n",
      "Epoch 00035: val_loss improved from 0.07070 to 0.06889, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/035-0.0689.hdf5\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.1115 - acc: 0.9653 - val_loss: 0.0689 - val_acc: 0.9792\n",
      "Epoch 36/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9666\n",
      "Epoch 00036: val_loss improved from 0.06889 to 0.06794, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/036-0.0679.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.1055 - acc: 0.9667 - val_loss: 0.0679 - val_acc: 0.9795\n",
      "Epoch 37/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9681\n",
      "Epoch 00037: val_loss did not improve from 0.06794\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.1033 - acc: 0.9681 - val_loss: 0.0685 - val_acc: 0.9799\n",
      "Epoch 38/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9687\n",
      "Epoch 00038: val_loss improved from 0.06794 to 0.06672, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/038-0.0667.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.1010 - acc: 0.9687 - val_loss: 0.0667 - val_acc: 0.9801\n",
      "Epoch 39/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9679\n",
      "Epoch 00039: val_loss improved from 0.06672 to 0.06609, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/039-0.0661.hdf5\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.1040 - acc: 0.9680 - val_loss: 0.0661 - val_acc: 0.9804\n",
      "Epoch 40/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9697\n",
      "Epoch 00040: val_loss improved from 0.06609 to 0.06585, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/040-0.0659.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0999 - acc: 0.9697 - val_loss: 0.0659 - val_acc: 0.9803\n",
      "Epoch 41/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9698\n",
      "Epoch 00041: val_loss improved from 0.06585 to 0.06554, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/041-0.0655.hdf5\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0983 - acc: 0.9699 - val_loss: 0.0655 - val_acc: 0.9814\n",
      "Epoch 42/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9688\n",
      "Epoch 00042: val_loss did not improve from 0.06554\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0969 - acc: 0.9689 - val_loss: 0.0660 - val_acc: 0.9805\n",
      "Epoch 43/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9715\n",
      "Epoch 00043: val_loss did not improve from 0.06554\n",
      "40200/40200 [==============================] - 3s 86us/sample - loss: 0.0916 - acc: 0.9715 - val_loss: 0.0660 - val_acc: 0.9810\n",
      "Epoch 44/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9702\n",
      "Epoch 00044: val_loss improved from 0.06554 to 0.06541, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/044-0.0654.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0933 - acc: 0.9703 - val_loss: 0.0654 - val_acc: 0.9813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9705\n",
      "Epoch 00045: val_loss improved from 0.06541 to 0.06327, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/045-0.0633.hdf5\n",
      "40200/40200 [==============================] - 4s 96us/sample - loss: 0.0940 - acc: 0.9704 - val_loss: 0.0633 - val_acc: 0.9820\n",
      "Epoch 46/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9722\n",
      "Epoch 00046: val_loss did not improve from 0.06327\n",
      "40200/40200 [==============================] - 3s 85us/sample - loss: 0.0909 - acc: 0.9722 - val_loss: 0.0636 - val_acc: 0.9815\n",
      "Epoch 47/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9702\n",
      "Epoch 00047: val_loss improved from 0.06327 to 0.06267, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/047-0.0627.hdf5\n",
      "40200/40200 [==============================] - 3s 86us/sample - loss: 0.0923 - acc: 0.9701 - val_loss: 0.0627 - val_acc: 0.9814\n",
      "Epoch 48/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9713\n",
      "Epoch 00048: val_loss improved from 0.06267 to 0.06257, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/048-0.0626.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0911 - acc: 0.9713 - val_loss: 0.0626 - val_acc: 0.9820\n",
      "Epoch 49/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9720\n",
      "Epoch 00049: val_loss did not improve from 0.06257\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0870 - acc: 0.9721 - val_loss: 0.0632 - val_acc: 0.9814\n",
      "Epoch 50/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9729\n",
      "Epoch 00050: val_loss did not improve from 0.06257\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0856 - acc: 0.9729 - val_loss: 0.0643 - val_acc: 0.9814\n",
      "Epoch 51/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9718\n",
      "Epoch 00051: val_loss did not improve from 0.06257\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0871 - acc: 0.9718 - val_loss: 0.0639 - val_acc: 0.9816\n",
      "Epoch 52/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9725\n",
      "Epoch 00052: val_loss did not improve from 0.06257\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0870 - acc: 0.9724 - val_loss: 0.0628 - val_acc: 0.9815\n",
      "Epoch 53/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9734\n",
      "Epoch 00053: val_loss did not improve from 0.06257\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0827 - acc: 0.9732 - val_loss: 0.0635 - val_acc: 0.9822\n",
      "Epoch 54/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9735\n",
      "Epoch 00054: val_loss improved from 0.06257 to 0.06075, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/054-0.0608.hdf5\n",
      "40200/40200 [==============================] - 3s 86us/sample - loss: 0.0830 - acc: 0.9735 - val_loss: 0.0608 - val_acc: 0.9829\n",
      "Epoch 55/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9742\n",
      "Epoch 00055: val_loss did not improve from 0.06075\n",
      "40200/40200 [==============================] - 4s 97us/sample - loss: 0.0809 - acc: 0.9741 - val_loss: 0.0616 - val_acc: 0.9822\n",
      "Epoch 56/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9737\n",
      "Epoch 00056: val_loss did not improve from 0.06075\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0844 - acc: 0.9737 - val_loss: 0.0608 - val_acc: 0.9824\n",
      "Epoch 57/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9736\n",
      "Epoch 00057: val_loss did not improve from 0.06075\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0849 - acc: 0.9736 - val_loss: 0.0615 - val_acc: 0.9818\n",
      "Epoch 58/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9745\n",
      "Epoch 00058: val_loss did not improve from 0.06075\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0789 - acc: 0.9745 - val_loss: 0.0623 - val_acc: 0.9825\n",
      "Epoch 59/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9747\n",
      "Epoch 00059: val_loss improved from 0.06075 to 0.05975, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/059-0.0598.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0799 - acc: 0.9746 - val_loss: 0.0598 - val_acc: 0.9828\n",
      "Epoch 60/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9747\n",
      "Epoch 00060: val_loss did not improve from 0.05975\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0785 - acc: 0.9747 - val_loss: 0.0619 - val_acc: 0.9819\n",
      "Epoch 61/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9747\n",
      "Epoch 00061: val_loss did not improve from 0.05975\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0764 - acc: 0.9747 - val_loss: 0.0598 - val_acc: 0.9832\n",
      "Epoch 62/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9737\n",
      "Epoch 00062: val_loss did not improve from 0.05975\n",
      "40200/40200 [==============================] - 3s 85us/sample - loss: 0.0822 - acc: 0.9738 - val_loss: 0.0611 - val_acc: 0.9821\n",
      "Epoch 63/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9751\n",
      "Epoch 00063: val_loss improved from 0.05975 to 0.05940, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/063-0.0594.hdf5\n",
      "40200/40200 [==============================] - 3s 84us/sample - loss: 0.0780 - acc: 0.9751 - val_loss: 0.0594 - val_acc: 0.9828\n",
      "Epoch 64/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9736\n",
      "Epoch 00064: val_loss improved from 0.05940 to 0.05862, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/064-0.0586.hdf5\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0801 - acc: 0.9736 - val_loss: 0.0586 - val_acc: 0.9827\n",
      "Epoch 65/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9757\n",
      "Epoch 00065: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0759 - acc: 0.9756 - val_loss: 0.0612 - val_acc: 0.9824\n",
      "Epoch 66/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9742\n",
      "Epoch 00066: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0789 - acc: 0.9741 - val_loss: 0.0612 - val_acc: 0.9826\n",
      "Epoch 67/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9738\n",
      "Epoch 00067: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0806 - acc: 0.9739 - val_loss: 0.0598 - val_acc: 0.9823\n",
      "Epoch 68/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9744\n",
      "Epoch 00068: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0785 - acc: 0.9745 - val_loss: 0.0597 - val_acc: 0.9831\n",
      "Epoch 69/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9747\n",
      "Epoch 00069: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0766 - acc: 0.9747 - val_loss: 0.0591 - val_acc: 0.9831\n",
      "Epoch 70/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9758\n",
      "Epoch 00070: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0747 - acc: 0.9759 - val_loss: 0.0590 - val_acc: 0.9833\n",
      "Epoch 71/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9746\n",
      "Epoch 00071: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 90us/sample - loss: 0.0779 - acc: 0.9746 - val_loss: 0.0593 - val_acc: 0.9831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9758\n",
      "Epoch 00072: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 97us/sample - loss: 0.0765 - acc: 0.9759 - val_loss: 0.0591 - val_acc: 0.9828\n",
      "Epoch 73/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9750\n",
      "Epoch 00073: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0768 - acc: 0.9750 - val_loss: 0.0592 - val_acc: 0.9828\n",
      "Epoch 74/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9748\n",
      "Epoch 00074: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0781 - acc: 0.9747 - val_loss: 0.0589 - val_acc: 0.9825\n",
      "Epoch 75/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9742\n",
      "Epoch 00075: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0787 - acc: 0.9743 - val_loss: 0.0603 - val_acc: 0.9826\n",
      "Epoch 76/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9752\n",
      "Epoch 00076: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0766 - acc: 0.9752 - val_loss: 0.0588 - val_acc: 0.9837\n",
      "Epoch 77/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9761\n",
      "Epoch 00077: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0750 - acc: 0.9760 - val_loss: 0.0590 - val_acc: 0.9829\n",
      "Epoch 78/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9755\n",
      "Epoch 00078: val_loss did not improve from 0.05862\n",
      "40200/40200 [==============================] - 4s 90us/sample - loss: 0.0751 - acc: 0.9756 - val_loss: 0.0588 - val_acc: 0.9834\n",
      "Epoch 79/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9778\n",
      "Epoch 00079: val_loss improved from 0.05862 to 0.05845, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/079-0.0584.hdf5\n",
      "40200/40200 [==============================] - 3s 85us/sample - loss: 0.0723 - acc: 0.9778 - val_loss: 0.0584 - val_acc: 0.9832\n",
      "Epoch 80/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9762\n",
      "Epoch 00080: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 87us/sample - loss: 0.0733 - acc: 0.9761 - val_loss: 0.0590 - val_acc: 0.9840\n",
      "Epoch 81/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9762\n",
      "Epoch 00081: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0732 - acc: 0.9762 - val_loss: 0.0592 - val_acc: 0.9835\n",
      "Epoch 82/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9762\n",
      "Epoch 00082: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0731 - acc: 0.9762 - val_loss: 0.0594 - val_acc: 0.9831\n",
      "Epoch 83/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9756\n",
      "Epoch 00083: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0752 - acc: 0.9756 - val_loss: 0.0593 - val_acc: 0.9830\n",
      "Epoch 84/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9763\n",
      "Epoch 00084: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 91us/sample - loss: 0.0716 - acc: 0.9763 - val_loss: 0.0585 - val_acc: 0.9832\n",
      "Epoch 85/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9765\n",
      "Epoch 00085: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0718 - acc: 0.9766 - val_loss: 0.0586 - val_acc: 0.9826\n",
      "Epoch 86/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9763\n",
      "Epoch 00086: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0730 - acc: 0.9764 - val_loss: 0.0593 - val_acc: 0.9828\n",
      "Epoch 87/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9770\n",
      "Epoch 00087: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0711 - acc: 0.9770 - val_loss: 0.0599 - val_acc: 0.9832\n",
      "Epoch 88/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9757\n",
      "Epoch 00088: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0767 - acc: 0.9757 - val_loss: 0.0595 - val_acc: 0.9833\n",
      "Epoch 89/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9762\n",
      "Epoch 00089: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0719 - acc: 0.9763 - val_loss: 0.0588 - val_acc: 0.9837\n",
      "Epoch 90/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9776\n",
      "Epoch 00090: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0693 - acc: 0.9776 - val_loss: 0.0595 - val_acc: 0.9837\n",
      "Epoch 91/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9763\n",
      "Epoch 00091: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0713 - acc: 0.9762 - val_loss: 0.0603 - val_acc: 0.9824\n",
      "Epoch 92/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9757\n",
      "Epoch 00092: val_loss did not improve from 0.05845\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0750 - acc: 0.9757 - val_loss: 0.0595 - val_acc: 0.9827\n",
      "Epoch 93/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9762\n",
      "Epoch 00093: val_loss improved from 0.05845 to 0.05822, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/093-0.0582.hdf5\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0739 - acc: 0.9763 - val_loss: 0.0582 - val_acc: 0.9832\n",
      "Epoch 94/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9771\n",
      "Epoch 00094: val_loss did not improve from 0.05822\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0731 - acc: 0.9770 - val_loss: 0.0596 - val_acc: 0.9823\n",
      "Epoch 95/500\n",
      "39488/40200 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9761\n",
      "Epoch 00095: val_loss did not improve from 0.05822\n",
      "40200/40200 [==============================] - 3s 85us/sample - loss: 0.0713 - acc: 0.9763 - val_loss: 0.0597 - val_acc: 0.9827\n",
      "Epoch 96/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9765\n",
      "Epoch 00096: val_loss did not improve from 0.05822\n",
      "40200/40200 [==============================] - 3s 85us/sample - loss: 0.0725 - acc: 0.9765 - val_loss: 0.0597 - val_acc: 0.9827\n",
      "Epoch 97/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9771\n",
      "Epoch 00097: val_loss did not improve from 0.05822\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0723 - acc: 0.9771 - val_loss: 0.0593 - val_acc: 0.9825\n",
      "Epoch 98/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9779\n",
      "Epoch 00098: val_loss did not improve from 0.05822\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0698 - acc: 0.9779 - val_loss: 0.0596 - val_acc: 0.9832\n",
      "Epoch 99/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9781\n",
      "Epoch 00099: val_loss did not improve from 0.05822\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0703 - acc: 0.9780 - val_loss: 0.0591 - val_acc: 0.9831\n",
      "Epoch 100/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9782\n",
      "Epoch 00100: val_loss improved from 0.05822 to 0.05801, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/100-0.0580.hdf5\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0683 - acc: 0.9782 - val_loss: 0.0580 - val_acc: 0.9839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9775\n",
      "Epoch 00101: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0706 - acc: 0.9775 - val_loss: 0.0595 - val_acc: 0.9836\n",
      "Epoch 102/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9762\n",
      "Epoch 00102: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 91us/sample - loss: 0.0727 - acc: 0.9762 - val_loss: 0.0589 - val_acc: 0.9833\n",
      "Epoch 103/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9768\n",
      "Epoch 00103: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0691 - acc: 0.9768 - val_loss: 0.0582 - val_acc: 0.9838\n",
      "Epoch 104/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9777\n",
      "Epoch 00104: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0684 - acc: 0.9778 - val_loss: 0.0599 - val_acc: 0.9831\n",
      "Epoch 105/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9777\n",
      "Epoch 00105: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0691 - acc: 0.9777 - val_loss: 0.0592 - val_acc: 0.9833\n",
      "Epoch 106/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9776\n",
      "Epoch 00106: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0704 - acc: 0.9774 - val_loss: 0.0583 - val_acc: 0.9836\n",
      "Epoch 107/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9778\n",
      "Epoch 00107: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0671 - acc: 0.9778 - val_loss: 0.0582 - val_acc: 0.9829\n",
      "Epoch 108/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9772\n",
      "Epoch 00108: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0693 - acc: 0.9772 - val_loss: 0.0590 - val_acc: 0.9834\n",
      "Epoch 109/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9776\n",
      "Epoch 00109: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0693 - acc: 0.9776 - val_loss: 0.0599 - val_acc: 0.9828\n",
      "Epoch 110/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9767\n",
      "Epoch 00110: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0695 - acc: 0.9767 - val_loss: 0.0596 - val_acc: 0.9829\n",
      "Epoch 111/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9765\n",
      "Epoch 00111: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 90us/sample - loss: 0.0704 - acc: 0.9764 - val_loss: 0.0589 - val_acc: 0.9834\n",
      "Epoch 112/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9780\n",
      "Epoch 00112: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 3s 83us/sample - loss: 0.0701 - acc: 0.9779 - val_loss: 0.0580 - val_acc: 0.9833\n",
      "Epoch 113/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9772\n",
      "Epoch 00113: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 87us/sample - loss: 0.0704 - acc: 0.9772 - val_loss: 0.0595 - val_acc: 0.9829\n",
      "Epoch 114/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9773\n",
      "Epoch 00114: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0681 - acc: 0.9773 - val_loss: 0.0587 - val_acc: 0.9838\n",
      "Epoch 115/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9774\n",
      "Epoch 00115: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0690 - acc: 0.9774 - val_loss: 0.0610 - val_acc: 0.9826\n",
      "Epoch 116/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9772\n",
      "Epoch 00116: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0669 - acc: 0.9773 - val_loss: 0.0597 - val_acc: 0.9825\n",
      "Epoch 117/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9772\n",
      "Epoch 00117: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0708 - acc: 0.9772 - val_loss: 0.0599 - val_acc: 0.9827\n",
      "Epoch 118/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9763\n",
      "Epoch 00118: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0706 - acc: 0.9763 - val_loss: 0.0587 - val_acc: 0.9835\n",
      "Epoch 119/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9781\n",
      "Epoch 00119: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0681 - acc: 0.9781 - val_loss: 0.0583 - val_acc: 0.9836\n",
      "Epoch 120/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9786\n",
      "Epoch 00120: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0651 - acc: 0.9786 - val_loss: 0.0594 - val_acc: 0.9832\n",
      "Epoch 121/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9776\n",
      "Epoch 00121: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0671 - acc: 0.9776 - val_loss: 0.0588 - val_acc: 0.9833\n",
      "Epoch 122/500\n",
      "39488/40200 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9774\n",
      "Epoch 00122: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0676 - acc: 0.9776 - val_loss: 0.0591 - val_acc: 0.9837\n",
      "Epoch 123/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9786\n",
      "Epoch 00123: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0660 - acc: 0.9785 - val_loss: 0.0593 - val_acc: 0.9833\n",
      "Epoch 124/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9775\n",
      "Epoch 00124: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0695 - acc: 0.9777 - val_loss: 0.0584 - val_acc: 0.9836\n",
      "Epoch 125/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9792\n",
      "Epoch 00125: val_loss did not improve from 0.05801\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0624 - acc: 0.9793 - val_loss: 0.0584 - val_acc: 0.9829\n",
      "Epoch 126/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9782\n",
      "Epoch 00126: val_loss improved from 0.05801 to 0.05713, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/126-0.0571.hdf5\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0688 - acc: 0.9782 - val_loss: 0.0571 - val_acc: 0.9837\n",
      "Epoch 127/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9787\n",
      "Epoch 00127: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0661 - acc: 0.9787 - val_loss: 0.0581 - val_acc: 0.9840\n",
      "Epoch 128/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9784\n",
      "Epoch 00128: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0671 - acc: 0.9784 - val_loss: 0.0604 - val_acc: 0.9832\n",
      "Epoch 129/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9773\n",
      "Epoch 00129: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0701 - acc: 0.9773 - val_loss: 0.0582 - val_acc: 0.9841\n",
      "Epoch 130/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9775\n",
      "Epoch 00130: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0668 - acc: 0.9775 - val_loss: 0.0597 - val_acc: 0.9837\n",
      "Epoch 131/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9791\n",
      "Epoch 00131: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0637 - acc: 0.9792 - val_loss: 0.0589 - val_acc: 0.9836\n",
      "Epoch 132/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9787\n",
      "Epoch 00132: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0666 - acc: 0.9787 - val_loss: 0.0593 - val_acc: 0.9828\n",
      "Epoch 133/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9782\n",
      "Epoch 00133: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0686 - acc: 0.9781 - val_loss: 0.0593 - val_acc: 0.9829\n",
      "Epoch 134/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9777\n",
      "Epoch 00134: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 3s 86us/sample - loss: 0.0684 - acc: 0.9778 - val_loss: 0.0589 - val_acc: 0.9838\n",
      "Epoch 135/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9777\n",
      "Epoch 00135: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 3s 83us/sample - loss: 0.0664 - acc: 0.9776 - val_loss: 0.0585 - val_acc: 0.9835\n",
      "Epoch 136/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9782\n",
      "Epoch 00136: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 90us/sample - loss: 0.0692 - acc: 0.9782 - val_loss: 0.0592 - val_acc: 0.9833\n",
      "Epoch 137/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9769\n",
      "Epoch 00137: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0659 - acc: 0.9770 - val_loss: 0.0587 - val_acc: 0.9834\n",
      "Epoch 138/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9770\n",
      "Epoch 00138: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0697 - acc: 0.9770 - val_loss: 0.0587 - val_acc: 0.9832\n",
      "Epoch 139/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9789\n",
      "Epoch 00139: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0643 - acc: 0.9788 - val_loss: 0.0592 - val_acc: 0.9831\n",
      "Epoch 140/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9773\n",
      "Epoch 00140: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0691 - acc: 0.9773 - val_loss: 0.0587 - val_acc: 0.9830\n",
      "Epoch 141/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9792\n",
      "Epoch 00141: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0629 - acc: 0.9792 - val_loss: 0.0592 - val_acc: 0.9833\n",
      "Epoch 142/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9778\n",
      "Epoch 00142: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0650 - acc: 0.9778 - val_loss: 0.0593 - val_acc: 0.9834\n",
      "Epoch 143/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9774\n",
      "Epoch 00143: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0676 - acc: 0.9774 - val_loss: 0.0589 - val_acc: 0.9830\n",
      "Epoch 144/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9783\n",
      "Epoch 00144: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0660 - acc: 0.9783 - val_loss: 0.0581 - val_acc: 0.9833\n",
      "Epoch 145/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9785\n",
      "Epoch 00145: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0648 - acc: 0.9785 - val_loss: 0.0584 - val_acc: 0.9832\n",
      "Epoch 146/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9784\n",
      "Epoch 00146: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0673 - acc: 0.9784 - val_loss: 0.0580 - val_acc: 0.9833\n",
      "Epoch 147/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9777\n",
      "Epoch 00147: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0673 - acc: 0.9776 - val_loss: 0.0593 - val_acc: 0.9831\n",
      "Epoch 148/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9789\n",
      "Epoch 00148: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0634 - acc: 0.9790 - val_loss: 0.0575 - val_acc: 0.9837\n",
      "Epoch 149/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9784\n",
      "Epoch 00149: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0654 - acc: 0.9785 - val_loss: 0.0576 - val_acc: 0.9844\n",
      "Epoch 150/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9789\n",
      "Epoch 00150: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0626 - acc: 0.9788 - val_loss: 0.0589 - val_acc: 0.9839\n",
      "Epoch 151/500\n",
      "39488/40200 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9783\n",
      "Epoch 00151: val_loss did not improve from 0.05713\n",
      "40200/40200 [==============================] - 3s 83us/sample - loss: 0.0622 - acc: 0.9783 - val_loss: 0.0580 - val_acc: 0.9831\n",
      "Epoch 152/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9789\n",
      "Epoch 00152: val_loss improved from 0.05713 to 0.05698, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv_checkpoint/152-0.0570.hdf5\n",
      "40200/40200 [==============================] - 3s 86us/sample - loss: 0.0657 - acc: 0.9789 - val_loss: 0.0570 - val_acc: 0.9842\n",
      "Epoch 153/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9784\n",
      "Epoch 00153: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0670 - acc: 0.9784 - val_loss: 0.0580 - val_acc: 0.9841\n",
      "Epoch 154/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9793\n",
      "Epoch 00154: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0645 - acc: 0.9794 - val_loss: 0.0578 - val_acc: 0.9837\n",
      "Epoch 155/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9778\n",
      "Epoch 00155: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0658 - acc: 0.9777 - val_loss: 0.0572 - val_acc: 0.9835\n",
      "Epoch 156/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9779\n",
      "Epoch 00156: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0668 - acc: 0.9779 - val_loss: 0.0580 - val_acc: 0.9838\n",
      "Epoch 157/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9785\n",
      "Epoch 00157: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0655 - acc: 0.9785 - val_loss: 0.0587 - val_acc: 0.9834\n",
      "Epoch 158/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9794\n",
      "Epoch 00158: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0624 - acc: 0.9794 - val_loss: 0.0581 - val_acc: 0.9835\n",
      "Epoch 159/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9784\n",
      "Epoch 00159: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0656 - acc: 0.9785 - val_loss: 0.0584 - val_acc: 0.9836\n",
      "Epoch 160/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9785\n",
      "Epoch 00160: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0652 - acc: 0.9785 - val_loss: 0.0589 - val_acc: 0.9833\n",
      "Epoch 161/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9787\n",
      "Epoch 00161: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 91us/sample - loss: 0.0636 - acc: 0.9788 - val_loss: 0.0588 - val_acc: 0.9830\n",
      "Epoch 162/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9790\n",
      "Epoch 00162: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 91us/sample - loss: 0.0640 - acc: 0.9790 - val_loss: 0.0592 - val_acc: 0.9831\n",
      "Epoch 163/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9779\n",
      "Epoch 00163: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0661 - acc: 0.9779 - val_loss: 0.0589 - val_acc: 0.9833\n",
      "Epoch 164/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9785\n",
      "Epoch 00164: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0643 - acc: 0.9785 - val_loss: 0.0577 - val_acc: 0.9837\n",
      "Epoch 165/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9795\n",
      "Epoch 00165: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0629 - acc: 0.9794 - val_loss: 0.0592 - val_acc: 0.9831\n",
      "Epoch 166/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9790\n",
      "Epoch 00166: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 94us/sample - loss: 0.0645 - acc: 0.9790 - val_loss: 0.0597 - val_acc: 0.9823\n",
      "Epoch 167/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9807\n",
      "Epoch 00167: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 3s 85us/sample - loss: 0.0623 - acc: 0.9805 - val_loss: 0.0588 - val_acc: 0.9840\n",
      "Epoch 168/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9793\n",
      "Epoch 00168: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 3s 84us/sample - loss: 0.0645 - acc: 0.9793 - val_loss: 0.0597 - val_acc: 0.9837\n",
      "Epoch 169/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9784\n",
      "Epoch 00169: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 91us/sample - loss: 0.0644 - acc: 0.9785 - val_loss: 0.0589 - val_acc: 0.9838\n",
      "Epoch 170/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9786\n",
      "Epoch 00170: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0624 - acc: 0.9786 - val_loss: 0.0582 - val_acc: 0.9842\n",
      "Epoch 171/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9777\n",
      "Epoch 00171: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0670 - acc: 0.9777 - val_loss: 0.0576 - val_acc: 0.9836\n",
      "Epoch 172/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9795\n",
      "Epoch 00172: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0622 - acc: 0.9795 - val_loss: 0.0591 - val_acc: 0.9833\n",
      "Epoch 173/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9775\n",
      "Epoch 00173: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0667 - acc: 0.9775 - val_loss: 0.0582 - val_acc: 0.9834\n",
      "Epoch 174/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9782\n",
      "Epoch 00174: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0646 - acc: 0.9781 - val_loss: 0.0580 - val_acc: 0.9838\n",
      "Epoch 175/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9782\n",
      "Epoch 00175: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0644 - acc: 0.9782 - val_loss: 0.0592 - val_acc: 0.9828\n",
      "Epoch 176/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9783\n",
      "Epoch 00176: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0651 - acc: 0.9783 - val_loss: 0.0575 - val_acc: 0.9841\n",
      "Epoch 177/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9803\n",
      "Epoch 00177: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0597 - acc: 0.9802 - val_loss: 0.0598 - val_acc: 0.9832\n",
      "Epoch 178/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9787\n",
      "Epoch 00178: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0639 - acc: 0.9787 - val_loss: 0.0596 - val_acc: 0.9837\n",
      "Epoch 179/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9792\n",
      "Epoch 00179: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0632 - acc: 0.9792 - val_loss: 0.0588 - val_acc: 0.9840\n",
      "Epoch 180/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9799\n",
      "Epoch 00180: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0629 - acc: 0.9798 - val_loss: 0.0594 - val_acc: 0.9836\n",
      "Epoch 181/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9795\n",
      "Epoch 00181: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0619 - acc: 0.9796 - val_loss: 0.0592 - val_acc: 0.9832\n",
      "Epoch 182/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9771\n",
      "Epoch 00182: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0676 - acc: 0.9771 - val_loss: 0.0586 - val_acc: 0.9835\n",
      "Epoch 183/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9783\n",
      "Epoch 00183: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0647 - acc: 0.9783 - val_loss: 0.0589 - val_acc: 0.9841\n",
      "Epoch 184/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9781\n",
      "Epoch 00184: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 3s 84us/sample - loss: 0.0655 - acc: 0.9782 - val_loss: 0.0592 - val_acc: 0.9840\n",
      "Epoch 185/500\n",
      "39488/40200 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9790\n",
      "Epoch 00185: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 3s 85us/sample - loss: 0.0636 - acc: 0.9790 - val_loss: 0.0586 - val_acc: 0.9834\n",
      "Epoch 186/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9801\n",
      "Epoch 00186: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0620 - acc: 0.9801 - val_loss: 0.0591 - val_acc: 0.9835\n",
      "Epoch 187/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9788\n",
      "Epoch 00187: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0652 - acc: 0.9786 - val_loss: 0.0593 - val_acc: 0.9836\n",
      "Epoch 188/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9783\n",
      "Epoch 00188: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0662 - acc: 0.9782 - val_loss: 0.0591 - val_acc: 0.9832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9796\n",
      "Epoch 00189: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0629 - acc: 0.9795 - val_loss: 0.0591 - val_acc: 0.9842\n",
      "Epoch 190/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9793\n",
      "Epoch 00190: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0651 - acc: 0.9793 - val_loss: 0.0584 - val_acc: 0.9835\n",
      "Epoch 191/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9799\n",
      "Epoch 00191: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0620 - acc: 0.9799 - val_loss: 0.0596 - val_acc: 0.9831\n",
      "Epoch 192/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9785\n",
      "Epoch 00192: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0622 - acc: 0.9785 - val_loss: 0.0587 - val_acc: 0.9836\n",
      "Epoch 193/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9782\n",
      "Epoch 00193: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0631 - acc: 0.9782 - val_loss: 0.0593 - val_acc: 0.9834\n",
      "Epoch 194/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9789\n",
      "Epoch 00194: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0628 - acc: 0.9789 - val_loss: 0.0592 - val_acc: 0.9828\n",
      "Epoch 195/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9806\n",
      "Epoch 00195: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0615 - acc: 0.9806 - val_loss: 0.0587 - val_acc: 0.9838\n",
      "Epoch 196/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9783\n",
      "Epoch 00196: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0660 - acc: 0.9783 - val_loss: 0.0583 - val_acc: 0.9837\n",
      "Epoch 197/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9788\n",
      "Epoch 00197: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0646 - acc: 0.9789 - val_loss: 0.0591 - val_acc: 0.9834\n",
      "Epoch 198/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9792\n",
      "Epoch 00198: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 93us/sample - loss: 0.0646 - acc: 0.9793 - val_loss: 0.0585 - val_acc: 0.9835\n",
      "Epoch 199/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9795\n",
      "Epoch 00199: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 92us/sample - loss: 0.0624 - acc: 0.9796 - val_loss: 0.0595 - val_acc: 0.9836\n",
      "Epoch 200/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9802\n",
      "Epoch 00200: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 88us/sample - loss: 0.0608 - acc: 0.9800 - val_loss: 0.0583 - val_acc: 0.9841\n",
      "Epoch 201/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9794\n",
      "Epoch 00201: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 3s 82us/sample - loss: 0.0624 - acc: 0.9794 - val_loss: 0.0589 - val_acc: 0.9836\n",
      "Epoch 202/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9799\n",
      "Epoch 00202: val_loss did not improve from 0.05698\n",
      "40200/40200 [==============================] - 4s 88us/sample - loss: 0.0615 - acc: 0.9799 - val_loss: 0.0584 - val_acc: 0.9839\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 79us/sample - loss: 0.0591 - acc: 0.9839\n",
      "Loss: 0.05906217606815044 Accuracy: 0.9839\n",
      "\n",
      "Train on 40200 samples, validate on 19800 samples\n",
      "Epoch 1/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 11.5168 - acc: 0.2321\n",
      "Epoch 00001: val_loss improved from inf to 3.17290, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/001-3.1729.hdf5\n",
      "40200/40200 [==============================] - 5s 128us/sample - loss: 11.5173 - acc: 0.2321 - val_loss: 3.1729 - val_acc: 0.6830\n",
      "Epoch 2/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 3.2643 - acc: 0.5336\n",
      "Epoch 00002: val_loss improved from 3.17290 to 0.57545, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/002-0.5755.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 3.2574 - acc: 0.5339 - val_loss: 0.5755 - val_acc: 0.8344\n",
      "Epoch 3/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 1.1723 - acc: 0.6353\n",
      "Epoch 00003: val_loss improved from 0.57545 to 0.38316, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/003-0.3832.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 1.1716 - acc: 0.6355 - val_loss: 0.3832 - val_acc: 0.8949\n",
      "Epoch 4/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.8669 - acc: 0.7265\n",
      "Epoch 00004: val_loss improved from 0.38316 to 0.28351, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/004-0.2835.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.8661 - acc: 0.7268 - val_loss: 0.2835 - val_acc: 0.9193\n",
      "Epoch 5/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.6968 - acc: 0.7791\n",
      "Epoch 00005: val_loss improved from 0.28351 to 0.23237, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/005-0.2324.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.6958 - acc: 0.7792 - val_loss: 0.2324 - val_acc: 0.9342\n",
      "Epoch 6/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.8197\n",
      "Epoch 00006: val_loss improved from 0.23237 to 0.19384, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/006-0.1938.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.5814 - acc: 0.8197 - val_loss: 0.1938 - val_acc: 0.9435\n",
      "Epoch 7/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.5042 - acc: 0.8442\n",
      "Epoch 00007: val_loss improved from 0.19384 to 0.17280, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/007-0.1728.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.5040 - acc: 0.8441 - val_loss: 0.1728 - val_acc: 0.9503\n",
      "Epoch 8/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.8658\n",
      "Epoch 00008: val_loss improved from 0.17280 to 0.15295, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/008-0.1529.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.4383 - acc: 0.8658 - val_loss: 0.1529 - val_acc: 0.9551\n",
      "Epoch 9/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8823\n",
      "Epoch 00009: val_loss improved from 0.15295 to 0.13489, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/009-0.1349.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.3870 - acc: 0.8824 - val_loss: 0.1349 - val_acc: 0.9593\n",
      "Epoch 10/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.3414 - acc: 0.8959\n",
      "Epoch 00010: val_loss improved from 0.13489 to 0.12297, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/010-0.1230.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.3413 - acc: 0.8958 - val_loss: 0.1230 - val_acc: 0.9631\n",
      "Epoch 11/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.9055\n",
      "Epoch 00011: val_loss improved from 0.12297 to 0.11131, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/011-0.1113.hdf5\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.3092 - acc: 0.9055 - val_loss: 0.1113 - val_acc: 0.9661\n",
      "Epoch 12/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.2777 - acc: 0.9152\n",
      "Epoch 00012: val_loss improved from 0.11131 to 0.10307, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/012-0.1031.hdf5\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.2778 - acc: 0.9154 - val_loss: 0.1031 - val_acc: 0.9686\n",
      "Epoch 13/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9211\n",
      "Epoch 00013: val_loss improved from 0.10307 to 0.09414, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/013-0.0941.hdf5\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.2545 - acc: 0.9211 - val_loss: 0.0941 - val_acc: 0.9711\n",
      "Epoch 14/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9270\n",
      "Epoch 00014: val_loss improved from 0.09414 to 0.08755, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/014-0.0875.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.2357 - acc: 0.9270 - val_loss: 0.0875 - val_acc: 0.9729\n",
      "Epoch 15/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9319\n",
      "Epoch 00015: val_loss improved from 0.08755 to 0.08435, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/015-0.0843.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.2258 - acc: 0.9321 - val_loss: 0.0843 - val_acc: 0.9753\n",
      "Epoch 16/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9346\n",
      "Epoch 00016: val_loss improved from 0.08435 to 0.07657, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/016-0.0766.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.2096 - acc: 0.9347 - val_loss: 0.0766 - val_acc: 0.9767\n",
      "Epoch 17/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9392\n",
      "Epoch 00017: val_loss improved from 0.07657 to 0.07237, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/017-0.0724.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1950 - acc: 0.9390 - val_loss: 0.0724 - val_acc: 0.9776\n",
      "Epoch 18/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9405\n",
      "Epoch 00018: val_loss improved from 0.07237 to 0.06940, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/018-0.0694.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1912 - acc: 0.9407 - val_loss: 0.0694 - val_acc: 0.9796\n",
      "Epoch 19/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9452\n",
      "Epoch 00019: val_loss improved from 0.06940 to 0.06588, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/019-0.0659.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.1772 - acc: 0.9453 - val_loss: 0.0659 - val_acc: 0.9805\n",
      "Epoch 20/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9485\n",
      "Epoch 00020: val_loss improved from 0.06588 to 0.06442, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/020-0.0644.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1664 - acc: 0.9484 - val_loss: 0.0644 - val_acc: 0.9806\n",
      "Epoch 21/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9506\n",
      "Epoch 00021: val_loss improved from 0.06442 to 0.06335, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/021-0.0634.hdf5\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.1598 - acc: 0.9506 - val_loss: 0.0634 - val_acc: 0.9810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9511\n",
      "Epoch 00022: val_loss improved from 0.06335 to 0.06179, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/022-0.0618.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1580 - acc: 0.9513 - val_loss: 0.0618 - val_acc: 0.9816\n",
      "Epoch 23/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9528\n",
      "Epoch 00023: val_loss improved from 0.06179 to 0.05951, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/023-0.0595.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1527 - acc: 0.9528 - val_loss: 0.0595 - val_acc: 0.9820\n",
      "Epoch 24/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9550\n",
      "Epoch 00024: val_loss improved from 0.05951 to 0.05571, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/024-0.0557.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1433 - acc: 0.9550 - val_loss: 0.0557 - val_acc: 0.9837\n",
      "Epoch 25/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9564\n",
      "Epoch 00025: val_loss did not improve from 0.05571\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.1360 - acc: 0.9565 - val_loss: 0.0557 - val_acc: 0.9835\n",
      "Epoch 26/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9578\n",
      "Epoch 00026: val_loss improved from 0.05571 to 0.05458, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/026-0.0546.hdf5\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.1394 - acc: 0.9578 - val_loss: 0.0546 - val_acc: 0.9839\n",
      "Epoch 27/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9583\n",
      "Epoch 00027: val_loss improved from 0.05458 to 0.05397, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/027-0.0540.hdf5\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.1358 - acc: 0.9583 - val_loss: 0.0540 - val_acc: 0.9839\n",
      "Epoch 28/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9598\n",
      "Epoch 00028: val_loss improved from 0.05397 to 0.05336, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/028-0.0534.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1312 - acc: 0.9599 - val_loss: 0.0534 - val_acc: 0.9843\n",
      "Epoch 29/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9602\n",
      "Epoch 00029: val_loss improved from 0.05336 to 0.05025, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/029-0.0502.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.1280 - acc: 0.9602 - val_loss: 0.0502 - val_acc: 0.9852\n",
      "Epoch 30/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9617\n",
      "Epoch 00030: val_loss improved from 0.05025 to 0.05003, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/030-0.0500.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.1224 - acc: 0.9617 - val_loss: 0.0500 - val_acc: 0.9851\n",
      "Epoch 31/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9624\n",
      "Epoch 00031: val_loss improved from 0.05003 to 0.04903, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/031-0.0490.hdf5\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.1212 - acc: 0.9625 - val_loss: 0.0490 - val_acc: 0.9856\n",
      "Epoch 32/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9632\n",
      "Epoch 00032: val_loss improved from 0.04903 to 0.04880, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/032-0.0488.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1178 - acc: 0.9632 - val_loss: 0.0488 - val_acc: 0.9863\n",
      "Epoch 33/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9651\n",
      "Epoch 00033: val_loss did not improve from 0.04880\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.1151 - acc: 0.9651 - val_loss: 0.0494 - val_acc: 0.9852\n",
      "Epoch 34/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9643\n",
      "Epoch 00034: val_loss improved from 0.04880 to 0.04761, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/034-0.0476.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1122 - acc: 0.9642 - val_loss: 0.0476 - val_acc: 0.9859\n",
      "Epoch 35/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9651\n",
      "Epoch 00035: val_loss did not improve from 0.04761\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.1102 - acc: 0.9651 - val_loss: 0.0477 - val_acc: 0.9859\n",
      "Epoch 36/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9657\n",
      "Epoch 00036: val_loss improved from 0.04761 to 0.04548, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/036-0.0455.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1087 - acc: 0.9657 - val_loss: 0.0455 - val_acc: 0.9868\n",
      "Epoch 37/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9665\n",
      "Epoch 00037: val_loss improved from 0.04548 to 0.04547, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/037-0.0455.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1069 - acc: 0.9664 - val_loss: 0.0455 - val_acc: 0.9863\n",
      "Epoch 38/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9670\n",
      "Epoch 00038: val_loss did not improve from 0.04547\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.1054 - acc: 0.9670 - val_loss: 0.0468 - val_acc: 0.9870\n",
      "Epoch 39/500\n",
      "39552/40200 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9678\n",
      "Epoch 00039: val_loss improved from 0.04547 to 0.04530, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/039-0.0453.hdf5\n",
      "40200/40200 [==============================] - 4s 106us/sample - loss: 0.1023 - acc: 0.9678 - val_loss: 0.0453 - val_acc: 0.9866\n",
      "Epoch 40/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9680\n",
      "Epoch 00040: val_loss improved from 0.04530 to 0.04489, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/040-0.0449.hdf5\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.1037 - acc: 0.9680 - val_loss: 0.0449 - val_acc: 0.9871\n",
      "Epoch 41/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9689\n",
      "Epoch 00041: val_loss did not improve from 0.04489\n",
      "40200/40200 [==============================] - 4s 106us/sample - loss: 0.0989 - acc: 0.9689 - val_loss: 0.0460 - val_acc: 0.9857\n",
      "Epoch 42/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9683\n",
      "Epoch 00042: val_loss improved from 0.04489 to 0.04277, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/042-0.0428.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.0991 - acc: 0.9683 - val_loss: 0.0428 - val_acc: 0.9874\n",
      "Epoch 43/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9687\n",
      "Epoch 00043: val_loss did not improve from 0.04277\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0976 - acc: 0.9687 - val_loss: 0.0442 - val_acc: 0.9868\n",
      "Epoch 44/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9709\n",
      "Epoch 00044: val_loss did not improve from 0.04277\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0926 - acc: 0.9710 - val_loss: 0.0443 - val_acc: 0.9877\n",
      "Epoch 45/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9694\n",
      "Epoch 00045: val_loss did not improve from 0.04277\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0975 - acc: 0.9694 - val_loss: 0.0435 - val_acc: 0.9877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9704\n",
      "Epoch 00046: val_loss did not improve from 0.04277\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0924 - acc: 0.9704 - val_loss: 0.0431 - val_acc: 0.9878\n",
      "Epoch 47/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9709\n",
      "Epoch 00047: val_loss improved from 0.04277 to 0.04193, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/047-0.0419.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0925 - acc: 0.9708 - val_loss: 0.0419 - val_acc: 0.9881\n",
      "Epoch 48/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9704\n",
      "Epoch 00048: val_loss did not improve from 0.04193\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0942 - acc: 0.9704 - val_loss: 0.0426 - val_acc: 0.9879\n",
      "Epoch 49/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9721\n",
      "Epoch 00049: val_loss improved from 0.04193 to 0.04190, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/049-0.0419.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0888 - acc: 0.9722 - val_loss: 0.0419 - val_acc: 0.9884\n",
      "Epoch 50/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9717\n",
      "Epoch 00050: val_loss improved from 0.04190 to 0.04169, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/050-0.0417.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.0908 - acc: 0.9717 - val_loss: 0.0417 - val_acc: 0.9880\n",
      "Epoch 51/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9714\n",
      "Epoch 00051: val_loss did not improve from 0.04169\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0878 - acc: 0.9714 - val_loss: 0.0430 - val_acc: 0.9873\n",
      "Epoch 52/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9724\n",
      "Epoch 00052: val_loss did not improve from 0.04169\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0902 - acc: 0.9725 - val_loss: 0.0417 - val_acc: 0.9880\n",
      "Epoch 53/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9717\n",
      "Epoch 00053: val_loss did not improve from 0.04169\n",
      "40200/40200 [==============================] - 4s 106us/sample - loss: 0.0897 - acc: 0.9717 - val_loss: 0.0422 - val_acc: 0.9880\n",
      "Epoch 54/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9711\n",
      "Epoch 00054: val_loss did not improve from 0.04169\n",
      "40200/40200 [==============================] - 4s 98us/sample - loss: 0.0894 - acc: 0.9711 - val_loss: 0.0418 - val_acc: 0.9883\n",
      "Epoch 55/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9738\n",
      "Epoch 00055: val_loss did not improve from 0.04169\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0846 - acc: 0.9738 - val_loss: 0.0422 - val_acc: 0.9879\n",
      "Epoch 56/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9715\n",
      "Epoch 00056: val_loss improved from 0.04169 to 0.04136, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/056-0.0414.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0902 - acc: 0.9715 - val_loss: 0.0414 - val_acc: 0.9876\n",
      "Epoch 57/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9748\n",
      "Epoch 00057: val_loss improved from 0.04136 to 0.04102, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/057-0.0410.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0807 - acc: 0.9747 - val_loss: 0.0410 - val_acc: 0.9882\n",
      "Epoch 58/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9727\n",
      "Epoch 00058: val_loss did not improve from 0.04102\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0855 - acc: 0.9727 - val_loss: 0.0420 - val_acc: 0.9876\n",
      "Epoch 59/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9738\n",
      "Epoch 00059: val_loss improved from 0.04102 to 0.04101, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/059-0.0410.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0826 - acc: 0.9737 - val_loss: 0.0410 - val_acc: 0.9878\n",
      "Epoch 60/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9742\n",
      "Epoch 00060: val_loss improved from 0.04101 to 0.04098, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/060-0.0410.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0823 - acc: 0.9740 - val_loss: 0.0410 - val_acc: 0.9874\n",
      "Epoch 61/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9748\n",
      "Epoch 00061: val_loss did not improve from 0.04098\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0806 - acc: 0.9746 - val_loss: 0.0411 - val_acc: 0.9881\n",
      "Epoch 62/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9750\n",
      "Epoch 00062: val_loss improved from 0.04098 to 0.04047, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/062-0.0405.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0791 - acc: 0.9750 - val_loss: 0.0405 - val_acc: 0.9878\n",
      "Epoch 63/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9741\n",
      "Epoch 00063: val_loss improved from 0.04047 to 0.04002, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/063-0.0400.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0814 - acc: 0.9741 - val_loss: 0.0400 - val_acc: 0.9880\n",
      "Epoch 64/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9744\n",
      "Epoch 00064: val_loss did not improve from 0.04002\n",
      "40200/40200 [==============================] - 4s 106us/sample - loss: 0.0817 - acc: 0.9744 - val_loss: 0.0408 - val_acc: 0.9885\n",
      "Epoch 65/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9742\n",
      "Epoch 00065: val_loss did not improve from 0.04002\n",
      "40200/40200 [==============================] - 4s 105us/sample - loss: 0.0801 - acc: 0.9742 - val_loss: 0.0400 - val_acc: 0.9889\n",
      "Epoch 66/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9753\n",
      "Epoch 00066: val_loss did not improve from 0.04002\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0812 - acc: 0.9753 - val_loss: 0.0423 - val_acc: 0.9874\n",
      "Epoch 67/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9755\n",
      "Epoch 00067: val_loss improved from 0.04002 to 0.03965, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/067-0.0397.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0764 - acc: 0.9755 - val_loss: 0.0397 - val_acc: 0.9884\n",
      "Epoch 68/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9742\n",
      "Epoch 00068: val_loss did not improve from 0.03965\n",
      "40200/40200 [==============================] - 4s 98us/sample - loss: 0.0798 - acc: 0.9741 - val_loss: 0.0398 - val_acc: 0.9884\n",
      "Epoch 69/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9751\n",
      "Epoch 00069: val_loss did not improve from 0.03965\n",
      "40200/40200 [==============================] - 4s 103us/sample - loss: 0.0777 - acc: 0.9750 - val_loss: 0.0402 - val_acc: 0.9886\n",
      "Epoch 70/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9754\n",
      "Epoch 00070: val_loss did not improve from 0.03965\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0761 - acc: 0.9754 - val_loss: 0.0405 - val_acc: 0.9885\n",
      "Epoch 71/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9765\n",
      "Epoch 00071: val_loss did not improve from 0.03965\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0767 - acc: 0.9765 - val_loss: 0.0410 - val_acc: 0.9875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9752\n",
      "Epoch 00072: val_loss improved from 0.03965 to 0.03925, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/072-0.0392.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0756 - acc: 0.9752 - val_loss: 0.0392 - val_acc: 0.9883\n",
      "Epoch 73/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9765\n",
      "Epoch 00073: val_loss did not improve from 0.03925\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0759 - acc: 0.9764 - val_loss: 0.0396 - val_acc: 0.9883\n",
      "Epoch 74/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9769\n",
      "Epoch 00074: val_loss did not improve from 0.03925\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0715 - acc: 0.9770 - val_loss: 0.0399 - val_acc: 0.9886\n",
      "Epoch 75/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9767\n",
      "Epoch 00075: val_loss did not improve from 0.03925\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0742 - acc: 0.9766 - val_loss: 0.0405 - val_acc: 0.9882\n",
      "Epoch 76/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9775\n",
      "Epoch 00076: val_loss improved from 0.03925 to 0.03884, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/076-0.0388.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0728 - acc: 0.9775 - val_loss: 0.0388 - val_acc: 0.9890\n",
      "Epoch 77/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9758\n",
      "Epoch 00077: val_loss did not improve from 0.03884\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0748 - acc: 0.9757 - val_loss: 0.0389 - val_acc: 0.9881\n",
      "Epoch 78/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9767\n",
      "Epoch 00078: val_loss improved from 0.03884 to 0.03837, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/078-0.0384.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0742 - acc: 0.9766 - val_loss: 0.0384 - val_acc: 0.9887\n",
      "Epoch 79/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9758\n",
      "Epoch 00079: val_loss did not improve from 0.03837\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0741 - acc: 0.9758 - val_loss: 0.0389 - val_acc: 0.9889\n",
      "Epoch 80/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9782\n",
      "Epoch 00080: val_loss improved from 0.03837 to 0.03825, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/080-0.0383.hdf5\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0709 - acc: 0.9782 - val_loss: 0.0383 - val_acc: 0.9886\n",
      "Epoch 81/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9764\n",
      "Epoch 00081: val_loss did not improve from 0.03825\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0729 - acc: 0.9764 - val_loss: 0.0397 - val_acc: 0.9885\n",
      "Epoch 82/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9762\n",
      "Epoch 00082: val_loss improved from 0.03825 to 0.03708, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/082-0.0371.hdf5\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.0716 - acc: 0.9762 - val_loss: 0.0371 - val_acc: 0.9889\n",
      "Epoch 83/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9763\n",
      "Epoch 00083: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0729 - acc: 0.9763 - val_loss: 0.0397 - val_acc: 0.9882\n",
      "Epoch 84/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9771\n",
      "Epoch 00084: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0708 - acc: 0.9771 - val_loss: 0.0391 - val_acc: 0.9881\n",
      "Epoch 85/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9769\n",
      "Epoch 00085: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0714 - acc: 0.9768 - val_loss: 0.0391 - val_acc: 0.9885\n",
      "Epoch 86/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9773\n",
      "Epoch 00086: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0699 - acc: 0.9773 - val_loss: 0.0400 - val_acc: 0.9880\n",
      "Epoch 87/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9771\n",
      "Epoch 00087: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0706 - acc: 0.9771 - val_loss: 0.0390 - val_acc: 0.9887\n",
      "Epoch 88/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9771\n",
      "Epoch 00088: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0705 - acc: 0.9772 - val_loss: 0.0403 - val_acc: 0.9878\n",
      "Epoch 89/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9780\n",
      "Epoch 00089: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0689 - acc: 0.9781 - val_loss: 0.0380 - val_acc: 0.9885\n",
      "Epoch 90/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9770\n",
      "Epoch 00090: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0701 - acc: 0.9769 - val_loss: 0.0391 - val_acc: 0.9882\n",
      "Epoch 91/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9774\n",
      "Epoch 00091: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0712 - acc: 0.9774 - val_loss: 0.0390 - val_acc: 0.9885\n",
      "Epoch 92/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9787\n",
      "Epoch 00092: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0667 - acc: 0.9787 - val_loss: 0.0393 - val_acc: 0.9885\n",
      "Epoch 93/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9772\n",
      "Epoch 00093: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0710 - acc: 0.9772 - val_loss: 0.0371 - val_acc: 0.9892\n",
      "Epoch 94/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9785\n",
      "Epoch 00094: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0670 - acc: 0.9785 - val_loss: 0.0383 - val_acc: 0.9886\n",
      "Epoch 95/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9794\n",
      "Epoch 00095: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0653 - acc: 0.9793 - val_loss: 0.0376 - val_acc: 0.9887\n",
      "Epoch 96/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9789\n",
      "Epoch 00096: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 97us/sample - loss: 0.0652 - acc: 0.9789 - val_loss: 0.0383 - val_acc: 0.9892\n",
      "Epoch 97/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9775\n",
      "Epoch 00097: val_loss did not improve from 0.03708\n",
      "40200/40200 [==============================] - 4s 103us/sample - loss: 0.0692 - acc: 0.9776 - val_loss: 0.0377 - val_acc: 0.9890\n",
      "Epoch 98/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9787\n",
      "Epoch 00098: val_loss improved from 0.03708 to 0.03667, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/098-0.0367.hdf5\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0672 - acc: 0.9787 - val_loss: 0.0367 - val_acc: 0.9892\n",
      "Epoch 99/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9786\n",
      "Epoch 00099: val_loss did not improve from 0.03667\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0654 - acc: 0.9786 - val_loss: 0.0378 - val_acc: 0.9890\n",
      "Epoch 100/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9772\n",
      "Epoch 00100: val_loss did not improve from 0.03667\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0695 - acc: 0.9771 - val_loss: 0.0368 - val_acc: 0.9891\n",
      "Epoch 101/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9784\n",
      "Epoch 00101: val_loss improved from 0.03667 to 0.03666, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/101-0.0367.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0682 - acc: 0.9785 - val_loss: 0.0367 - val_acc: 0.9893\n",
      "Epoch 102/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9793\n",
      "Epoch 00102: val_loss did not improve from 0.03666\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0664 - acc: 0.9794 - val_loss: 0.0368 - val_acc: 0.9895\n",
      "Epoch 103/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9791\n",
      "Epoch 00103: val_loss did not improve from 0.03666\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0650 - acc: 0.9792 - val_loss: 0.0367 - val_acc: 0.9893\n",
      "Epoch 104/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9787\n",
      "Epoch 00104: val_loss did not improve from 0.03666\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0663 - acc: 0.9787 - val_loss: 0.0376 - val_acc: 0.9887\n",
      "Epoch 105/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9790\n",
      "Epoch 00105: val_loss improved from 0.03666 to 0.03633, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/105-0.0363.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.0654 - acc: 0.9789 - val_loss: 0.0363 - val_acc: 0.9892\n",
      "Epoch 106/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9791\n",
      "Epoch 00106: val_loss improved from 0.03633 to 0.03593, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/106-0.0359.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0646 - acc: 0.9791 - val_loss: 0.0359 - val_acc: 0.9894\n",
      "Epoch 107/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9800\n",
      "Epoch 00107: val_loss improved from 0.03593 to 0.03557, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/107-0.0356.hdf5\n",
      "40200/40200 [==============================] - 4s 110us/sample - loss: 0.0612 - acc: 0.9801 - val_loss: 0.0356 - val_acc: 0.9892\n",
      "Epoch 108/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9791\n",
      "Epoch 00108: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0638 - acc: 0.9791 - val_loss: 0.0376 - val_acc: 0.9890\n",
      "Epoch 109/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9789\n",
      "Epoch 00109: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0648 - acc: 0.9789 - val_loss: 0.0381 - val_acc: 0.9888\n",
      "Epoch 110/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9802\n",
      "Epoch 00110: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.0626 - acc: 0.9802 - val_loss: 0.0372 - val_acc: 0.9889\n",
      "Epoch 111/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9796\n",
      "Epoch 00111: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0627 - acc: 0.9797 - val_loss: 0.0379 - val_acc: 0.9888\n",
      "Epoch 112/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9788\n",
      "Epoch 00112: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0670 - acc: 0.9788 - val_loss: 0.0359 - val_acc: 0.9889\n",
      "Epoch 113/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9800\n",
      "Epoch 00113: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0614 - acc: 0.9800 - val_loss: 0.0374 - val_acc: 0.9894\n",
      "Epoch 114/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9786\n",
      "Epoch 00114: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0646 - acc: 0.9786 - val_loss: 0.0367 - val_acc: 0.9892\n",
      "Epoch 115/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9805\n",
      "Epoch 00115: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0598 - acc: 0.9805 - val_loss: 0.0377 - val_acc: 0.9890\n",
      "Epoch 116/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9792\n",
      "Epoch 00116: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0644 - acc: 0.9792 - val_loss: 0.0372 - val_acc: 0.9896\n",
      "Epoch 117/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9803\n",
      "Epoch 00117: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0631 - acc: 0.9802 - val_loss: 0.0372 - val_acc: 0.9891\n",
      "Epoch 118/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9797\n",
      "Epoch 00118: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0629 - acc: 0.9798 - val_loss: 0.0361 - val_acc: 0.9887\n",
      "Epoch 119/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9801\n",
      "Epoch 00119: val_loss did not improve from 0.03557\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0602 - acc: 0.9801 - val_loss: 0.0356 - val_acc: 0.9895\n",
      "Epoch 120/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9801\n",
      "Epoch 00120: val_loss improved from 0.03557 to 0.03542, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/120-0.0354.hdf5\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0629 - acc: 0.9801 - val_loss: 0.0354 - val_acc: 0.9893\n",
      "Epoch 121/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9792\n",
      "Epoch 00121: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0628 - acc: 0.9793 - val_loss: 0.0363 - val_acc: 0.9896\n",
      "Epoch 122/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9802\n",
      "Epoch 00122: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0617 - acc: 0.9802 - val_loss: 0.0359 - val_acc: 0.9894\n",
      "Epoch 123/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9808\n",
      "Epoch 00123: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0604 - acc: 0.9807 - val_loss: 0.0384 - val_acc: 0.9888\n",
      "Epoch 124/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9806\n",
      "Epoch 00124: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0592 - acc: 0.9806 - val_loss: 0.0376 - val_acc: 0.9894\n",
      "Epoch 125/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9802\n",
      "Epoch 00125: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0603 - acc: 0.9802 - val_loss: 0.0366 - val_acc: 0.9893\n",
      "Epoch 126/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9796\n",
      "Epoch 00126: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0623 - acc: 0.9796 - val_loss: 0.0370 - val_acc: 0.9890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9807\n",
      "Epoch 00127: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0619 - acc: 0.9806 - val_loss: 0.0354 - val_acc: 0.9894\n",
      "Epoch 128/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9815\n",
      "Epoch 00128: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0583 - acc: 0.9815 - val_loss: 0.0372 - val_acc: 0.9891\n",
      "Epoch 129/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9813\n",
      "Epoch 00129: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0598 - acc: 0.9814 - val_loss: 0.0359 - val_acc: 0.9896\n",
      "Epoch 130/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9818\n",
      "Epoch 00130: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0587 - acc: 0.9819 - val_loss: 0.0360 - val_acc: 0.9894\n",
      "Epoch 131/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9808\n",
      "Epoch 00131: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0584 - acc: 0.9809 - val_loss: 0.0367 - val_acc: 0.9894\n",
      "Epoch 132/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9810\n",
      "Epoch 00132: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0605 - acc: 0.9810 - val_loss: 0.0363 - val_acc: 0.9895\n",
      "Epoch 133/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9797\n",
      "Epoch 00133: val_loss did not improve from 0.03542\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0612 - acc: 0.9798 - val_loss: 0.0384 - val_acc: 0.9890\n",
      "Epoch 134/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9794\n",
      "Epoch 00134: val_loss improved from 0.03542 to 0.03521, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv_checkpoint/134-0.0352.hdf5\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0629 - acc: 0.9793 - val_loss: 0.0352 - val_acc: 0.9896\n",
      "Epoch 135/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9812\n",
      "Epoch 00135: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0579 - acc: 0.9812 - val_loss: 0.0387 - val_acc: 0.9889\n",
      "Epoch 136/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9808\n",
      "Epoch 00136: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0609 - acc: 0.9808 - val_loss: 0.0361 - val_acc: 0.9894\n",
      "Epoch 137/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9811\n",
      "Epoch 00137: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0571 - acc: 0.9812 - val_loss: 0.0369 - val_acc: 0.9894\n",
      "Epoch 138/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9813\n",
      "Epoch 00138: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.0575 - acc: 0.9813 - val_loss: 0.0359 - val_acc: 0.9895\n",
      "Epoch 139/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9812\n",
      "Epoch 00139: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 101us/sample - loss: 0.0581 - acc: 0.9811 - val_loss: 0.0366 - val_acc: 0.9892\n",
      "Epoch 140/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9803\n",
      "Epoch 00140: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0575 - acc: 0.9803 - val_loss: 0.0354 - val_acc: 0.9900\n",
      "Epoch 141/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9796\n",
      "Epoch 00141: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0619 - acc: 0.9796 - val_loss: 0.0369 - val_acc: 0.9896\n",
      "Epoch 142/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9817\n",
      "Epoch 00142: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0581 - acc: 0.9817 - val_loss: 0.0365 - val_acc: 0.9893\n",
      "Epoch 143/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9812\n",
      "Epoch 00143: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0576 - acc: 0.9812 - val_loss: 0.0363 - val_acc: 0.9893\n",
      "Epoch 144/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9811\n",
      "Epoch 00144: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0582 - acc: 0.9812 - val_loss: 0.0358 - val_acc: 0.9895\n",
      "Epoch 145/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9806\n",
      "Epoch 00145: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0578 - acc: 0.9805 - val_loss: 0.0362 - val_acc: 0.9899\n",
      "Epoch 146/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9818\n",
      "Epoch 00146: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0570 - acc: 0.9818 - val_loss: 0.0358 - val_acc: 0.9894\n",
      "Epoch 147/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9808\n",
      "Epoch 00147: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0581 - acc: 0.9808 - val_loss: 0.0362 - val_acc: 0.9899\n",
      "Epoch 148/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9815\n",
      "Epoch 00148: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0555 - acc: 0.9816 - val_loss: 0.0359 - val_acc: 0.9897\n",
      "Epoch 149/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9821\n",
      "Epoch 00149: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0548 - acc: 0.9821 - val_loss: 0.0363 - val_acc: 0.9893\n",
      "Epoch 150/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9820\n",
      "Epoch 00150: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 103us/sample - loss: 0.0556 - acc: 0.9819 - val_loss: 0.0371 - val_acc: 0.9889\n",
      "Epoch 151/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9823\n",
      "Epoch 00151: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0565 - acc: 0.9823 - val_loss: 0.0371 - val_acc: 0.9888\n",
      "Epoch 152/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9812\n",
      "Epoch 00152: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 100us/sample - loss: 0.0565 - acc: 0.9812 - val_loss: 0.0382 - val_acc: 0.9890\n",
      "Epoch 153/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9810\n",
      "Epoch 00153: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 99us/sample - loss: 0.0584 - acc: 0.9810 - val_loss: 0.0358 - val_acc: 0.9899\n",
      "Epoch 154/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9822\n",
      "Epoch 00154: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0556 - acc: 0.9821 - val_loss: 0.0354 - val_acc: 0.9894\n",
      "Epoch 155/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9815\n",
      "Epoch 00155: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0547 - acc: 0.9815 - val_loss: 0.0362 - val_acc: 0.9893\n",
      "Epoch 156/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9823\n",
      "Epoch 00156: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0560 - acc: 0.9823 - val_loss: 0.0367 - val_acc: 0.9892\n",
      "Epoch 157/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9816\n",
      "Epoch 00157: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0557 - acc: 0.9816 - val_loss: 0.0354 - val_acc: 0.9897\n",
      "Epoch 158/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9828\n",
      "Epoch 00158: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0538 - acc: 0.9828 - val_loss: 0.0363 - val_acc: 0.9897\n",
      "Epoch 159/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9818\n",
      "Epoch 00159: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0563 - acc: 0.9818 - val_loss: 0.0374 - val_acc: 0.9894\n",
      "Epoch 160/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9820\n",
      "Epoch 00160: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0565 - acc: 0.9820 - val_loss: 0.0365 - val_acc: 0.9895\n",
      "Epoch 161/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9822\n",
      "Epoch 00161: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0566 - acc: 0.9820 - val_loss: 0.0355 - val_acc: 0.9893\n",
      "Epoch 162/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9818\n",
      "Epoch 00162: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0554 - acc: 0.9818 - val_loss: 0.0371 - val_acc: 0.9890\n",
      "Epoch 163/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9827\n",
      "Epoch 00163: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0535 - acc: 0.9827 - val_loss: 0.0382 - val_acc: 0.9887\n",
      "Epoch 164/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9823\n",
      "Epoch 00164: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0546 - acc: 0.9823 - val_loss: 0.0383 - val_acc: 0.9893\n",
      "Epoch 165/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9825\n",
      "Epoch 00165: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0548 - acc: 0.9825 - val_loss: 0.0359 - val_acc: 0.9897\n",
      "Epoch 166/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9831\n",
      "Epoch 00166: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 102us/sample - loss: 0.0547 - acc: 0.9830 - val_loss: 0.0385 - val_acc: 0.9888\n",
      "Epoch 167/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9819\n",
      "Epoch 00167: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 98us/sample - loss: 0.0536 - acc: 0.9819 - val_loss: 0.0364 - val_acc: 0.9896\n",
      "Epoch 168/500\n",
      "39616/40200 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9826\n",
      "Epoch 00168: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0528 - acc: 0.9826 - val_loss: 0.0372 - val_acc: 0.9896\n",
      "Epoch 169/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9806\n",
      "Epoch 00169: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0581 - acc: 0.9806 - val_loss: 0.0361 - val_acc: 0.9897\n",
      "Epoch 170/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9821\n",
      "Epoch 00170: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0552 - acc: 0.9821 - val_loss: 0.0379 - val_acc: 0.9892\n",
      "Epoch 171/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9821\n",
      "Epoch 00171: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0548 - acc: 0.9822 - val_loss: 0.0358 - val_acc: 0.9902\n",
      "Epoch 172/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9824\n",
      "Epoch 00172: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0548 - acc: 0.9824 - val_loss: 0.0377 - val_acc: 0.9889\n",
      "Epoch 173/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9820\n",
      "Epoch 00173: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0560 - acc: 0.9820 - val_loss: 0.0364 - val_acc: 0.9897\n",
      "Epoch 174/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9820\n",
      "Epoch 00174: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0530 - acc: 0.9820 - val_loss: 0.0357 - val_acc: 0.9899\n",
      "Epoch 175/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9831\n",
      "Epoch 00175: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0534 - acc: 0.9830 - val_loss: 0.0386 - val_acc: 0.9889\n",
      "Epoch 176/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9820\n",
      "Epoch 00176: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0535 - acc: 0.9820 - val_loss: 0.0357 - val_acc: 0.9902\n",
      "Epoch 177/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9821\n",
      "Epoch 00177: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0534 - acc: 0.9821 - val_loss: 0.0382 - val_acc: 0.9894\n",
      "Epoch 178/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9814\n",
      "Epoch 00178: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0537 - acc: 0.9814 - val_loss: 0.0382 - val_acc: 0.9893\n",
      "Epoch 179/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9827\n",
      "Epoch 00179: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 109us/sample - loss: 0.0542 - acc: 0.9827 - val_loss: 0.0359 - val_acc: 0.9894\n",
      "Epoch 180/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9818\n",
      "Epoch 00180: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0550 - acc: 0.9818 - val_loss: 0.0354 - val_acc: 0.9902\n",
      "Epoch 181/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9828\n",
      "Epoch 00181: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 97us/sample - loss: 0.0522 - acc: 0.9828 - val_loss: 0.0374 - val_acc: 0.9894\n",
      "Epoch 182/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9817\n",
      "Epoch 00182: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 104us/sample - loss: 0.0575 - acc: 0.9816 - val_loss: 0.0366 - val_acc: 0.9894\n",
      "Epoch 183/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9818\n",
      "Epoch 00183: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 108us/sample - loss: 0.0541 - acc: 0.9818 - val_loss: 0.0371 - val_acc: 0.9897\n",
      "Epoch 184/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9828\n",
      "Epoch 00184: val_loss did not improve from 0.03521\n",
      "40200/40200 [==============================] - 4s 107us/sample - loss: 0.0520 - acc: 0.9829 - val_loss: 0.0369 - val_acc: 0.9896\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 85us/sample - loss: 0.0298 - acc: 0.9901\n",
      "Loss: 0.029803342605679062 Accuracy: 0.9901\n",
      "\n",
      "Train on 40200 samples, validate on 19800 samples\n",
      "Epoch 1/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 6.7029 - acc: 0.3074\n",
      "Epoch 00001: val_loss improved from inf to 0.68222, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/001-0.6822.hdf5\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 6.7019 - acc: 0.3074 - val_loss: 0.6822 - val_acc: 0.8174\n",
      "Epoch 2/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 1.1295 - acc: 0.6383\n",
      "Epoch 00002: val_loss improved from 0.68222 to 0.35702, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/002-0.3570.hdf5\n",
      "40200/40200 [==============================] - 5s 127us/sample - loss: 1.1290 - acc: 0.6384 - val_loss: 0.3570 - val_acc: 0.9013\n",
      "Epoch 3/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.7255 - acc: 0.7660\n",
      "Epoch 00003: val_loss improved from 0.35702 to 0.23081, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/003-0.2308.hdf5\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.7249 - acc: 0.7662 - val_loss: 0.2308 - val_acc: 0.9306\n",
      "Epoch 4/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.5403 - acc: 0.8271\n",
      "Epoch 00004: val_loss improved from 0.23081 to 0.17897, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/004-0.1790.hdf5\n",
      "40200/40200 [==============================] - 5s 127us/sample - loss: 0.5403 - acc: 0.8271 - val_loss: 0.1790 - val_acc: 0.9455\n",
      "Epoch 5/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8660\n",
      "Epoch 00005: val_loss improved from 0.17897 to 0.14159, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/005-0.1416.hdf5\n",
      "40200/40200 [==============================] - 5s 127us/sample - loss: 0.4191 - acc: 0.8659 - val_loss: 0.1416 - val_acc: 0.9566\n",
      "Epoch 6/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8925\n",
      "Epoch 00006: val_loss improved from 0.14159 to 0.11668, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/006-0.1167.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.3442 - acc: 0.8926 - val_loss: 0.1167 - val_acc: 0.9642\n",
      "Epoch 7/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.9086\n",
      "Epoch 00007: val_loss improved from 0.11668 to 0.10187, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/007-0.1019.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.2863 - acc: 0.9087 - val_loss: 0.1019 - val_acc: 0.9689\n",
      "Epoch 8/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.9193\n",
      "Epoch 00008: val_loss improved from 0.10187 to 0.08941, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/008-0.0894.hdf5\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.2518 - acc: 0.9194 - val_loss: 0.0894 - val_acc: 0.9720\n",
      "Epoch 9/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9308\n",
      "Epoch 00009: val_loss improved from 0.08941 to 0.08230, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/009-0.0823.hdf5\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.2198 - acc: 0.9307 - val_loss: 0.0823 - val_acc: 0.9748\n",
      "Epoch 10/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9379\n",
      "Epoch 00010: val_loss improved from 0.08230 to 0.07590, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/010-0.0759.hdf5\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.2027 - acc: 0.9378 - val_loss: 0.0759 - val_acc: 0.9766\n",
      "Epoch 11/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9446\n",
      "Epoch 00011: val_loss improved from 0.07590 to 0.06929, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/011-0.0693.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.1755 - acc: 0.9446 - val_loss: 0.0693 - val_acc: 0.9784\n",
      "Epoch 12/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9481\n",
      "Epoch 00012: val_loss improved from 0.06929 to 0.06500, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/012-0.0650.hdf5\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.1711 - acc: 0.9482 - val_loss: 0.0650 - val_acc: 0.9801\n",
      "Epoch 13/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9528\n",
      "Epoch 00013: val_loss improved from 0.06500 to 0.06147, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/013-0.0615.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.1521 - acc: 0.9528 - val_loss: 0.0615 - val_acc: 0.9813\n",
      "Epoch 14/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9542\n",
      "Epoch 00014: val_loss improved from 0.06147 to 0.05896, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/014-0.0590.hdf5\n",
      "40200/40200 [==============================] - 5s 127us/sample - loss: 0.1437 - acc: 0.9543 - val_loss: 0.0590 - val_acc: 0.9825\n",
      "Epoch 15/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9586\n",
      "Epoch 00015: val_loss improved from 0.05896 to 0.05664, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/015-0.0566.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.1333 - acc: 0.9585 - val_loss: 0.0566 - val_acc: 0.9832\n",
      "Epoch 16/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9610\n",
      "Epoch 00016: val_loss improved from 0.05664 to 0.05341, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/016-0.0534.hdf5\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.1241 - acc: 0.9610 - val_loss: 0.0534 - val_acc: 0.9840\n",
      "Epoch 17/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9641\n",
      "Epoch 00017: val_loss improved from 0.05341 to 0.05142, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/017-0.0514.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.1165 - acc: 0.9641 - val_loss: 0.0514 - val_acc: 0.9849\n",
      "Epoch 18/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9648\n",
      "Epoch 00018: val_loss improved from 0.05142 to 0.05023, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/018-0.0502.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.1131 - acc: 0.9647 - val_loss: 0.0502 - val_acc: 0.9845\n",
      "Epoch 19/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9661\n",
      "Epoch 00019: val_loss improved from 0.05023 to 0.04710, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/019-0.0471.hdf5\n",
      "40200/40200 [==============================] - 5s 127us/sample - loss: 0.1073 - acc: 0.9661 - val_loss: 0.0471 - val_acc: 0.9857\n",
      "Epoch 20/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9676\n",
      "Epoch 00020: val_loss did not improve from 0.04710\n",
      "40200/40200 [==============================] - 5s 120us/sample - loss: 0.1032 - acc: 0.9676 - val_loss: 0.0486 - val_acc: 0.9857\n",
      "Epoch 21/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9693\n",
      "Epoch 00021: val_loss improved from 0.04710 to 0.04453, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/021-0.0445.hdf5\n",
      "40200/40200 [==============================] - 5s 114us/sample - loss: 0.0987 - acc: 0.9693 - val_loss: 0.0445 - val_acc: 0.9864\n",
      "Epoch 22/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9712\n",
      "Epoch 00022: val_loss did not improve from 0.04453\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0934 - acc: 0.9711 - val_loss: 0.0449 - val_acc: 0.9863\n",
      "Epoch 23/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9718\n",
      "Epoch 00023: val_loss improved from 0.04453 to 0.04388, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/023-0.0439.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0895 - acc: 0.9717 - val_loss: 0.0439 - val_acc: 0.9862\n",
      "Epoch 24/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9725\n",
      "Epoch 00024: val_loss did not improve from 0.04388\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0846 - acc: 0.9725 - val_loss: 0.0450 - val_acc: 0.9867\n",
      "Epoch 25/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9724\n",
      "Epoch 00025: val_loss improved from 0.04388 to 0.04246, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/025-0.0425.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0849 - acc: 0.9724 - val_loss: 0.0425 - val_acc: 0.9869\n",
      "Epoch 26/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9757\n",
      "Epoch 00026: val_loss improved from 0.04246 to 0.04127, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/026-0.0413.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0772 - acc: 0.9757 - val_loss: 0.0413 - val_acc: 0.9871\n",
      "Epoch 27/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9751\n",
      "Epoch 00027: val_loss did not improve from 0.04127\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0786 - acc: 0.9752 - val_loss: 0.0421 - val_acc: 0.9867\n",
      "Epoch 28/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9742\n",
      "Epoch 00028: val_loss improved from 0.04127 to 0.03973, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/028-0.0397.hdf5\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0794 - acc: 0.9744 - val_loss: 0.0397 - val_acc: 0.9880\n",
      "Epoch 29/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9761\n",
      "Epoch 00029: val_loss did not improve from 0.03973\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0728 - acc: 0.9762 - val_loss: 0.0425 - val_acc: 0.9872\n",
      "Epoch 30/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9760\n",
      "Epoch 00030: val_loss improved from 0.03973 to 0.03921, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/030-0.0392.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0752 - acc: 0.9759 - val_loss: 0.0392 - val_acc: 0.9883\n",
      "Epoch 31/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9783\n",
      "Epoch 00031: val_loss improved from 0.03921 to 0.03882, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/031-0.0388.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0681 - acc: 0.9782 - val_loss: 0.0388 - val_acc: 0.9878\n",
      "Epoch 32/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9772\n",
      "Epoch 00032: val_loss improved from 0.03882 to 0.03679, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/032-0.0368.hdf5\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0696 - acc: 0.9772 - val_loss: 0.0368 - val_acc: 0.9887\n",
      "Epoch 33/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9796\n",
      "Epoch 00033: val_loss improved from 0.03679 to 0.03661, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/033-0.0366.hdf5\n",
      "40200/40200 [==============================] - 5s 114us/sample - loss: 0.0639 - acc: 0.9796 - val_loss: 0.0366 - val_acc: 0.9887\n",
      "Epoch 34/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9798\n",
      "Epoch 00034: val_loss did not improve from 0.03661\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0650 - acc: 0.9798 - val_loss: 0.0386 - val_acc: 0.9880\n",
      "Epoch 35/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9797\n",
      "Epoch 00035: val_loss did not improve from 0.03661\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0642 - acc: 0.9797 - val_loss: 0.0389 - val_acc: 0.9879\n",
      "Epoch 36/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9802\n",
      "Epoch 00036: val_loss improved from 0.03661 to 0.03573, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/036-0.0357.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0620 - acc: 0.9802 - val_loss: 0.0357 - val_acc: 0.9891\n",
      "Epoch 37/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9800\n",
      "Epoch 00037: val_loss did not improve from 0.03573\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0610 - acc: 0.9800 - val_loss: 0.0364 - val_acc: 0.9890\n",
      "Epoch 38/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9811\n",
      "Epoch 00038: val_loss did not improve from 0.03573\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0574 - acc: 0.9811 - val_loss: 0.0368 - val_acc: 0.9890\n",
      "Epoch 39/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9818\n",
      "Epoch 00039: val_loss improved from 0.03573 to 0.03503, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/039-0.0350.hdf5\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0565 - acc: 0.9818 - val_loss: 0.0350 - val_acc: 0.9890\n",
      "Epoch 40/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9815\n",
      "Epoch 00040: val_loss did not improve from 0.03503\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0557 - acc: 0.9815 - val_loss: 0.0375 - val_acc: 0.9887\n",
      "Epoch 41/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9829\n",
      "Epoch 00041: val_loss did not improve from 0.03503\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0541 - acc: 0.9829 - val_loss: 0.0376 - val_acc: 0.9885\n",
      "Epoch 42/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9823\n",
      "Epoch 00042: val_loss did not improve from 0.03503\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0535 - acc: 0.9823 - val_loss: 0.0363 - val_acc: 0.9889\n",
      "Epoch 43/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9837\n",
      "Epoch 00043: val_loss did not improve from 0.03503\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0496 - acc: 0.9837 - val_loss: 0.0368 - val_acc: 0.9890\n",
      "Epoch 44/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9833\n",
      "Epoch 00044: val_loss did not improve from 0.03503\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0522 - acc: 0.9833 - val_loss: 0.0357 - val_acc: 0.9895\n",
      "Epoch 45/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9833\n",
      "Epoch 00045: val_loss did not improve from 0.03503\n",
      "40200/40200 [==============================] - 4s 112us/sample - loss: 0.0514 - acc: 0.9833 - val_loss: 0.0363 - val_acc: 0.9894\n",
      "Epoch 46/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9847\n",
      "Epoch 00046: val_loss improved from 0.03503 to 0.03498, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/046-0.0350.hdf5\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0498 - acc: 0.9847 - val_loss: 0.0350 - val_acc: 0.9894\n",
      "Epoch 47/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9842\n",
      "Epoch 00047: val_loss did not improve from 0.03498\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0489 - acc: 0.9843 - val_loss: 0.0351 - val_acc: 0.9892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9839\n",
      "Epoch 00048: val_loss improved from 0.03498 to 0.03322, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/048-0.0332.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0489 - acc: 0.9840 - val_loss: 0.0332 - val_acc: 0.9905\n",
      "Epoch 49/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9845\n",
      "Epoch 00049: val_loss did not improve from 0.03322\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0485 - acc: 0.9845 - val_loss: 0.0346 - val_acc: 0.9894\n",
      "Epoch 50/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9847\n",
      "Epoch 00050: val_loss did not improve from 0.03322\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0462 - acc: 0.9847 - val_loss: 0.0337 - val_acc: 0.9901\n",
      "Epoch 51/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9847\n",
      "Epoch 00051: val_loss did not improve from 0.03322\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0469 - acc: 0.9847 - val_loss: 0.0346 - val_acc: 0.9894\n",
      "Epoch 52/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9862\n",
      "Epoch 00052: val_loss did not improve from 0.03322\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0428 - acc: 0.9862 - val_loss: 0.0346 - val_acc: 0.9903\n",
      "Epoch 53/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9847\n",
      "Epoch 00053: val_loss improved from 0.03322 to 0.03287, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/053-0.0329.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0462 - acc: 0.9847 - val_loss: 0.0329 - val_acc: 0.9904\n",
      "Epoch 54/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9853\n",
      "Epoch 00054: val_loss did not improve from 0.03287\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0445 - acc: 0.9854 - val_loss: 0.0332 - val_acc: 0.9905\n",
      "Epoch 55/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9857\n",
      "Epoch 00055: val_loss did not improve from 0.03287\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0431 - acc: 0.9858 - val_loss: 0.0335 - val_acc: 0.9899\n",
      "Epoch 56/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9865\n",
      "Epoch 00056: val_loss did not improve from 0.03287\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0419 - acc: 0.9865 - val_loss: 0.0334 - val_acc: 0.9907\n",
      "Epoch 57/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9861\n",
      "Epoch 00057: val_loss improved from 0.03287 to 0.03280, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/057-0.0328.hdf5\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0411 - acc: 0.9861 - val_loss: 0.0328 - val_acc: 0.9906\n",
      "Epoch 58/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9860\n",
      "Epoch 00058: val_loss did not improve from 0.03280\n",
      "40200/40200 [==============================] - 5s 116us/sample - loss: 0.0404 - acc: 0.9859 - val_loss: 0.0336 - val_acc: 0.9909\n",
      "Epoch 59/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9869\n",
      "Epoch 00059: val_loss did not improve from 0.03280\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0412 - acc: 0.9869 - val_loss: 0.0336 - val_acc: 0.9901\n",
      "Epoch 60/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9862\n",
      "Epoch 00060: val_loss improved from 0.03280 to 0.03265, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/060-0.0326.hdf5\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0421 - acc: 0.9860 - val_loss: 0.0326 - val_acc: 0.9909\n",
      "Epoch 61/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9866\n",
      "Epoch 00061: val_loss improved from 0.03265 to 0.03239, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/061-0.0324.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0396 - acc: 0.9866 - val_loss: 0.0324 - val_acc: 0.9906\n",
      "Epoch 62/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9879\n",
      "Epoch 00062: val_loss improved from 0.03239 to 0.03170, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/062-0.0317.hdf5\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0385 - acc: 0.9879 - val_loss: 0.0317 - val_acc: 0.9911\n",
      "Epoch 63/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9872\n",
      "Epoch 00063: val_loss did not improve from 0.03170\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0406 - acc: 0.9872 - val_loss: 0.0326 - val_acc: 0.9904\n",
      "Epoch 64/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9870\n",
      "Epoch 00064: val_loss did not improve from 0.03170\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0378 - acc: 0.9869 - val_loss: 0.0321 - val_acc: 0.9907\n",
      "Epoch 65/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9887\n",
      "Epoch 00065: val_loss did not improve from 0.03170\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0339 - acc: 0.9886 - val_loss: 0.0325 - val_acc: 0.9912\n",
      "Epoch 66/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9876\n",
      "Epoch 00066: val_loss improved from 0.03170 to 0.03157, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/066-0.0316.hdf5\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0374 - acc: 0.9877 - val_loss: 0.0316 - val_acc: 0.9907\n",
      "Epoch 67/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9874\n",
      "Epoch 00067: val_loss improved from 0.03157 to 0.03120, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/067-0.0312.hdf5\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0379 - acc: 0.9874 - val_loss: 0.0312 - val_acc: 0.9910\n",
      "Epoch 68/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9882\n",
      "Epoch 00068: val_loss did not improve from 0.03120\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0353 - acc: 0.9882 - val_loss: 0.0316 - val_acc: 0.9913\n",
      "Epoch 69/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9880\n",
      "Epoch 00069: val_loss did not improve from 0.03120\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0365 - acc: 0.9880 - val_loss: 0.0331 - val_acc: 0.9905\n",
      "Epoch 70/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9888\n",
      "Epoch 00070: val_loss did not improve from 0.03120\n",
      "40200/40200 [==============================] - 5s 116us/sample - loss: 0.0349 - acc: 0.9889 - val_loss: 0.0314 - val_acc: 0.9915\n",
      "Epoch 71/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9883\n",
      "Epoch 00071: val_loss did not improve from 0.03120\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0369 - acc: 0.9883 - val_loss: 0.0321 - val_acc: 0.9914\n",
      "Epoch 72/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9881\n",
      "Epoch 00072: val_loss did not improve from 0.03120\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0351 - acc: 0.9881 - val_loss: 0.0336 - val_acc: 0.9907\n",
      "Epoch 73/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9897\n",
      "Epoch 00073: val_loss did not improve from 0.03120\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0330 - acc: 0.9897 - val_loss: 0.0320 - val_acc: 0.9908\n",
      "Epoch 74/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9888\n",
      "Epoch 00074: val_loss improved from 0.03120 to 0.03083, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/074-0.0308.hdf5\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0326 - acc: 0.9889 - val_loss: 0.0308 - val_acc: 0.9908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9884\n",
      "Epoch 00075: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0357 - acc: 0.9884 - val_loss: 0.0326 - val_acc: 0.9907\n",
      "Epoch 76/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9876\n",
      "Epoch 00076: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0366 - acc: 0.9876 - val_loss: 0.0328 - val_acc: 0.9906\n",
      "Epoch 77/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9880\n",
      "Epoch 00077: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0354 - acc: 0.9880 - val_loss: 0.0328 - val_acc: 0.9903\n",
      "Epoch 78/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9899\n",
      "Epoch 00078: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0296 - acc: 0.9899 - val_loss: 0.0320 - val_acc: 0.9905\n",
      "Epoch 79/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9901\n",
      "Epoch 00079: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0314 - acc: 0.9901 - val_loss: 0.0319 - val_acc: 0.9911\n",
      "Epoch 80/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9881\n",
      "Epoch 00080: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0352 - acc: 0.9881 - val_loss: 0.0316 - val_acc: 0.9911\n",
      "Epoch 81/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9896\n",
      "Epoch 00081: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0314 - acc: 0.9896 - val_loss: 0.0318 - val_acc: 0.9915\n",
      "Epoch 82/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9901\n",
      "Epoch 00082: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 4s 112us/sample - loss: 0.0310 - acc: 0.9901 - val_loss: 0.0312 - val_acc: 0.9910\n",
      "Epoch 83/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9893\n",
      "Epoch 00083: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0320 - acc: 0.9892 - val_loss: 0.0319 - val_acc: 0.9908\n",
      "Epoch 84/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9902\n",
      "Epoch 00084: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0292 - acc: 0.9902 - val_loss: 0.0327 - val_acc: 0.9911\n",
      "Epoch 85/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9906\n",
      "Epoch 00085: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0285 - acc: 0.9906 - val_loss: 0.0337 - val_acc: 0.9910\n",
      "Epoch 86/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9898\n",
      "Epoch 00086: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0304 - acc: 0.9899 - val_loss: 0.0329 - val_acc: 0.9909\n",
      "Epoch 87/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9914\n",
      "Epoch 00087: val_loss did not improve from 0.03083\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0277 - acc: 0.9914 - val_loss: 0.0311 - val_acc: 0.9916\n",
      "Epoch 88/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9898\n",
      "Epoch 00088: val_loss improved from 0.03083 to 0.03049, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/088-0.0305.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0315 - acc: 0.9898 - val_loss: 0.0305 - val_acc: 0.9921\n",
      "Epoch 89/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9909\n",
      "Epoch 00089: val_loss did not improve from 0.03049\n",
      "40200/40200 [==============================] - 5s 129us/sample - loss: 0.0279 - acc: 0.9909 - val_loss: 0.0308 - val_acc: 0.9918\n",
      "Epoch 90/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9901\n",
      "Epoch 00090: val_loss did not improve from 0.03049\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0297 - acc: 0.9900 - val_loss: 0.0312 - val_acc: 0.9915\n",
      "Epoch 91/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9896\n",
      "Epoch 00091: val_loss did not improve from 0.03049\n",
      "40200/40200 [==============================] - 6s 137us/sample - loss: 0.0307 - acc: 0.9896 - val_loss: 0.0325 - val_acc: 0.9907\n",
      "Epoch 92/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9898\n",
      "Epoch 00092: val_loss did not improve from 0.03049\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0297 - acc: 0.9899 - val_loss: 0.0306 - val_acc: 0.9918\n",
      "Epoch 93/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9906\n",
      "Epoch 00093: val_loss did not improve from 0.03049\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0284 - acc: 0.9907 - val_loss: 0.0317 - val_acc: 0.9912\n",
      "Epoch 94/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9914\n",
      "Epoch 00094: val_loss improved from 0.03049 to 0.03021, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/094-0.0302.hdf5\n",
      "40200/40200 [==============================] - 5s 114us/sample - loss: 0.0252 - acc: 0.9914 - val_loss: 0.0302 - val_acc: 0.9917\n",
      "Epoch 95/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9912\n",
      "Epoch 00095: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0263 - acc: 0.9912 - val_loss: 0.0323 - val_acc: 0.9904\n",
      "Epoch 96/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9909\n",
      "Epoch 00096: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0267 - acc: 0.9909 - val_loss: 0.0320 - val_acc: 0.9914\n",
      "Epoch 97/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9909\n",
      "Epoch 00097: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0266 - acc: 0.9909 - val_loss: 0.0328 - val_acc: 0.9908\n",
      "Epoch 98/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9903\n",
      "Epoch 00098: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0275 - acc: 0.9901 - val_loss: 0.0321 - val_acc: 0.9912\n",
      "Epoch 99/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9913\n",
      "Epoch 00099: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0256 - acc: 0.9913 - val_loss: 0.0326 - val_acc: 0.9909\n",
      "Epoch 100/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9916\n",
      "Epoch 00100: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0250 - acc: 0.9916 - val_loss: 0.0353 - val_acc: 0.9905\n",
      "Epoch 101/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9906\n",
      "Epoch 00101: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0281 - acc: 0.9905 - val_loss: 0.0309 - val_acc: 0.9916\n",
      "Epoch 102/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9906\n",
      "Epoch 00102: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0274 - acc: 0.9906 - val_loss: 0.0323 - val_acc: 0.9905\n",
      "Epoch 103/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9910\n",
      "Epoch 00103: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0264 - acc: 0.9910 - val_loss: 0.0315 - val_acc: 0.9907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9913\n",
      "Epoch 00104: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0263 - acc: 0.9912 - val_loss: 0.0347 - val_acc: 0.9905\n",
      "Epoch 105/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9913\n",
      "Epoch 00105: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0257 - acc: 0.9913 - val_loss: 0.0327 - val_acc: 0.9911\n",
      "Epoch 106/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9907\n",
      "Epoch 00106: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0266 - acc: 0.9908 - val_loss: 0.0343 - val_acc: 0.9908\n",
      "Epoch 107/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9916\n",
      "Epoch 00107: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 128us/sample - loss: 0.0247 - acc: 0.9916 - val_loss: 0.0311 - val_acc: 0.9914\n",
      "Epoch 108/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9921\n",
      "Epoch 00108: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0230 - acc: 0.9921 - val_loss: 0.0333 - val_acc: 0.9910\n",
      "Epoch 109/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9916\n",
      "Epoch 00109: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0244 - acc: 0.9916 - val_loss: 0.0340 - val_acc: 0.9912\n",
      "Epoch 110/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9913\n",
      "Epoch 00110: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0264 - acc: 0.9914 - val_loss: 0.0307 - val_acc: 0.9919\n",
      "Epoch 111/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9911\n",
      "Epoch 00111: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0258 - acc: 0.9912 - val_loss: 0.0325 - val_acc: 0.9915\n",
      "Epoch 112/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9918\n",
      "Epoch 00112: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0239 - acc: 0.9918 - val_loss: 0.0320 - val_acc: 0.9914\n",
      "Epoch 113/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9922\n",
      "Epoch 00113: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0234 - acc: 0.9923 - val_loss: 0.0315 - val_acc: 0.9917\n",
      "Epoch 114/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9916\n",
      "Epoch 00114: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0242 - acc: 0.9916 - val_loss: 0.0315 - val_acc: 0.9911\n",
      "Epoch 115/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9918\n",
      "Epoch 00115: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0236 - acc: 0.9919 - val_loss: 0.0321 - val_acc: 0.9912\n",
      "Epoch 116/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9916\n",
      "Epoch 00116: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0243 - acc: 0.9916 - val_loss: 0.0331 - val_acc: 0.9915\n",
      "Epoch 117/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9918\n",
      "Epoch 00117: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0249 - acc: 0.9918 - val_loss: 0.0329 - val_acc: 0.9916\n",
      "Epoch 118/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9917\n",
      "Epoch 00118: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 114us/sample - loss: 0.0240 - acc: 0.9916 - val_loss: 0.0323 - val_acc: 0.9914\n",
      "Epoch 119/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9919\n",
      "Epoch 00119: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0230 - acc: 0.9918 - val_loss: 0.0316 - val_acc: 0.9918\n",
      "Epoch 120/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9922\n",
      "Epoch 00120: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0233 - acc: 0.9921 - val_loss: 0.0316 - val_acc: 0.9922\n",
      "Epoch 121/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9922\n",
      "Epoch 00121: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0240 - acc: 0.9921 - val_loss: 0.0327 - val_acc: 0.9910\n",
      "Epoch 122/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9921\n",
      "Epoch 00122: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0231 - acc: 0.9921 - val_loss: 0.0318 - val_acc: 0.9913\n",
      "Epoch 123/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9930\n",
      "Epoch 00123: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0207 - acc: 0.9930 - val_loss: 0.0330 - val_acc: 0.9914\n",
      "Epoch 124/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9916\n",
      "Epoch 00124: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0234 - acc: 0.9916 - val_loss: 0.0327 - val_acc: 0.9920\n",
      "Epoch 125/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9928\n",
      "Epoch 00125: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0214 - acc: 0.9927 - val_loss: 0.0328 - val_acc: 0.9918\n",
      "Epoch 126/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9918\n",
      "Epoch 00126: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0239 - acc: 0.9918 - val_loss: 0.0334 - val_acc: 0.9910\n",
      "Epoch 127/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9916\n",
      "Epoch 00127: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0240 - acc: 0.9916 - val_loss: 0.0342 - val_acc: 0.9915\n",
      "Epoch 128/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9929\n",
      "Epoch 00128: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 132us/sample - loss: 0.0216 - acc: 0.9928 - val_loss: 0.0310 - val_acc: 0.9920\n",
      "Epoch 129/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9930\n",
      "Epoch 00129: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0218 - acc: 0.9930 - val_loss: 0.0326 - val_acc: 0.9918\n",
      "Epoch 130/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9923\n",
      "Epoch 00130: val_loss did not improve from 0.03021\n",
      "40200/40200 [==============================] - 5s 118us/sample - loss: 0.0220 - acc: 0.9923 - val_loss: 0.0326 - val_acc: 0.9909\n",
      "Epoch 131/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9919\n",
      "Epoch 00131: val_loss improved from 0.03021 to 0.02925, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/131-0.0293.hdf5\n",
      "40200/40200 [==============================] - 5s 115us/sample - loss: 0.0240 - acc: 0.9919 - val_loss: 0.0293 - val_acc: 0.9921\n",
      "Epoch 132/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9922\n",
      "Epoch 00132: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0210 - acc: 0.9922 - val_loss: 0.0359 - val_acc: 0.9911\n",
      "Epoch 133/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9930\n",
      "Epoch 00133: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0199 - acc: 0.9930 - val_loss: 0.0337 - val_acc: 0.9922\n",
      "Epoch 134/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9929\n",
      "Epoch 00134: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0207 - acc: 0.9929 - val_loss: 0.0334 - val_acc: 0.9914\n",
      "Epoch 135/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9922\n",
      "Epoch 00135: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0213 - acc: 0.9923 - val_loss: 0.0322 - val_acc: 0.9914\n",
      "Epoch 136/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9932\n",
      "Epoch 00136: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0201 - acc: 0.9932 - val_loss: 0.0330 - val_acc: 0.9921\n",
      "Epoch 137/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9931\n",
      "Epoch 00137: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0192 - acc: 0.9931 - val_loss: 0.0329 - val_acc: 0.9919\n",
      "Epoch 138/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9924\n",
      "Epoch 00138: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0211 - acc: 0.9924 - val_loss: 0.0329 - val_acc: 0.9920\n",
      "Epoch 139/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9923\n",
      "Epoch 00139: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0212 - acc: 0.9923 - val_loss: 0.0332 - val_acc: 0.9920\n",
      "Epoch 140/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9932\n",
      "Epoch 00140: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0199 - acc: 0.9932 - val_loss: 0.0326 - val_acc: 0.9916\n",
      "Epoch 141/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9931\n",
      "Epoch 00141: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0208 - acc: 0.9930 - val_loss: 0.0317 - val_acc: 0.9919\n",
      "Epoch 142/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9933\n",
      "Epoch 00142: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0190 - acc: 0.9933 - val_loss: 0.0327 - val_acc: 0.9916\n",
      "Epoch 143/500\n",
      "39680/40200 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9931\n",
      "Epoch 00143: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 112us/sample - loss: 0.0210 - acc: 0.9931 - val_loss: 0.0345 - val_acc: 0.9912\n",
      "Epoch 144/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9927\n",
      "Epoch 00144: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0213 - acc: 0.9927 - val_loss: 0.0362 - val_acc: 0.9908\n",
      "Epoch 145/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9928\n",
      "Epoch 00145: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0205 - acc: 0.9927 - val_loss: 0.0343 - val_acc: 0.9913\n",
      "Epoch 146/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9931\n",
      "Epoch 00146: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0201 - acc: 0.9931 - val_loss: 0.0343 - val_acc: 0.9914\n",
      "Epoch 147/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9934\n",
      "Epoch 00147: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0347 - val_acc: 0.9907\n",
      "Epoch 148/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9934\n",
      "Epoch 00148: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0189 - acc: 0.9935 - val_loss: 0.0333 - val_acc: 0.9914\n",
      "Epoch 149/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9933\n",
      "Epoch 00149: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0191 - acc: 0.9933 - val_loss: 0.0329 - val_acc: 0.9918\n",
      "Epoch 150/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9935\n",
      "Epoch 00150: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0196 - acc: 0.9934 - val_loss: 0.0331 - val_acc: 0.9916\n",
      "Epoch 151/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9936\n",
      "Epoch 00151: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0184 - acc: 0.9937 - val_loss: 0.0333 - val_acc: 0.9916\n",
      "Epoch 152/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9933\n",
      "Epoch 00152: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0198 - acc: 0.9933 - val_loss: 0.0338 - val_acc: 0.9921\n",
      "Epoch 153/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9935\n",
      "Epoch 00153: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0191 - acc: 0.9935 - val_loss: 0.0355 - val_acc: 0.9918\n",
      "Epoch 154/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9928\n",
      "Epoch 00154: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 121us/sample - loss: 0.0200 - acc: 0.9928 - val_loss: 0.0351 - val_acc: 0.9909\n",
      "Epoch 155/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9929\n",
      "Epoch 00155: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 113us/sample - loss: 0.0201 - acc: 0.9929 - val_loss: 0.0324 - val_acc: 0.9918\n",
      "Epoch 156/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9933\n",
      "Epoch 00156: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0177 - acc: 0.9933 - val_loss: 0.0340 - val_acc: 0.9918\n",
      "Epoch 157/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9931\n",
      "Epoch 00157: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0189 - acc: 0.9931 - val_loss: 0.0340 - val_acc: 0.9917\n",
      "Epoch 158/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9931\n",
      "Epoch 00158: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0205 - acc: 0.9931 - val_loss: 0.0350 - val_acc: 0.9913\n",
      "Epoch 159/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9937\n",
      "Epoch 00159: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 122us/sample - loss: 0.0183 - acc: 0.9937 - val_loss: 0.0320 - val_acc: 0.9916\n",
      "Epoch 160/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9937\n",
      "Epoch 00160: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0177 - acc: 0.9937 - val_loss: 0.0337 - val_acc: 0.9916\n",
      "Epoch 161/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9938\n",
      "Epoch 00161: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0185 - acc: 0.9938 - val_loss: 0.0331 - val_acc: 0.9918\n",
      "Epoch 162/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9942\n",
      "Epoch 00162: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0186 - acc: 0.9942 - val_loss: 0.0328 - val_acc: 0.9922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9944\n",
      "Epoch 00163: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0176 - acc: 0.9945 - val_loss: 0.0343 - val_acc: 0.9915\n",
      "Epoch 164/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9937\n",
      "Epoch 00164: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0193 - acc: 0.9937 - val_loss: 0.0342 - val_acc: 0.9915\n",
      "Epoch 165/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9941\n",
      "Epoch 00165: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0173 - acc: 0.9941 - val_loss: 0.0343 - val_acc: 0.9918\n",
      "Epoch 166/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9940\n",
      "Epoch 00166: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0174 - acc: 0.9940 - val_loss: 0.0357 - val_acc: 0.9915\n",
      "Epoch 167/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9944\n",
      "Epoch 00167: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 113us/sample - loss: 0.0168 - acc: 0.9944 - val_loss: 0.0318 - val_acc: 0.9923\n",
      "Epoch 168/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9948\n",
      "Epoch 00168: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 119us/sample - loss: 0.0155 - acc: 0.9949 - val_loss: 0.0416 - val_acc: 0.9904\n",
      "Epoch 169/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9924\n",
      "Epoch 00169: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0218 - acc: 0.9924 - val_loss: 0.0336 - val_acc: 0.9914\n",
      "Epoch 170/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9941\n",
      "Epoch 00170: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0171 - acc: 0.9942 - val_loss: 0.0325 - val_acc: 0.9916\n",
      "Epoch 171/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9945\n",
      "Epoch 00171: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0160 - acc: 0.9945 - val_loss: 0.0349 - val_acc: 0.9915\n",
      "Epoch 172/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9942\n",
      "Epoch 00172: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0175 - acc: 0.9942 - val_loss: 0.0323 - val_acc: 0.9922\n",
      "Epoch 173/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9942\n",
      "Epoch 00173: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0166 - acc: 0.9942 - val_loss: 0.0355 - val_acc: 0.9913\n",
      "Epoch 174/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9938\n",
      "Epoch 00174: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 123us/sample - loss: 0.0188 - acc: 0.9938 - val_loss: 0.0323 - val_acc: 0.9916\n",
      "Epoch 175/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9941\n",
      "Epoch 00175: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0168 - acc: 0.9941 - val_loss: 0.0355 - val_acc: 0.9917\n",
      "Epoch 176/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9938\n",
      "Epoch 00176: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0183 - acc: 0.9938 - val_loss: 0.0363 - val_acc: 0.9913\n",
      "Epoch 177/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9925\n",
      "Epoch 00177: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0203 - acc: 0.9926 - val_loss: 0.0350 - val_acc: 0.9919\n",
      "Epoch 178/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9938\n",
      "Epoch 00178: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 125us/sample - loss: 0.0185 - acc: 0.9938 - val_loss: 0.0346 - val_acc: 0.9910\n",
      "Epoch 179/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9950\n",
      "Epoch 00179: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 114us/sample - loss: 0.0152 - acc: 0.9950 - val_loss: 0.0390 - val_acc: 0.9908\n",
      "Epoch 180/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9944\n",
      "Epoch 00180: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 117us/sample - loss: 0.0169 - acc: 0.9944 - val_loss: 0.0350 - val_acc: 0.9921\n",
      "Epoch 181/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9944\n",
      "Epoch 00181: val_loss did not improve from 0.02925\n",
      "40200/40200 [==============================] - 5s 124us/sample - loss: 0.0159 - acc: 0.9944 - val_loss: 0.0383 - val_acc: 0.9906\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 97us/sample - loss: 0.0235 - acc: 0.9934\n",
      "Loss: 0.023460563211250975 Accuracy: 0.9934\n",
      "\n",
      "Train on 40200 samples, validate on 19800 samples\n",
      "Epoch 1/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 4.7412 - acc: 0.3488\n",
      "Epoch 00001: val_loss improved from inf to 0.70253, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/001-0.7025.hdf5\n",
      "40200/40200 [==============================] - 7s 183us/sample - loss: 4.7408 - acc: 0.3488 - val_loss: 0.7025 - val_acc: 0.8403\n",
      "Epoch 2/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.9745 - acc: 0.6793\n",
      "Epoch 00002: val_loss improved from 0.70253 to 0.28725, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/002-0.2873.hdf5\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.9726 - acc: 0.6800 - val_loss: 0.2873 - val_acc: 0.9215\n",
      "Epoch 3/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.6348 - acc: 0.7961\n",
      "Epoch 00003: val_loss improved from 0.28725 to 0.18837, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/003-0.1884.hdf5\n",
      "40200/40200 [==============================] - 6s 151us/sample - loss: 0.6344 - acc: 0.7962 - val_loss: 0.1884 - val_acc: 0.9468\n",
      "Epoch 4/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.4758 - acc: 0.8513\n",
      "Epoch 00004: val_loss improved from 0.18837 to 0.14295, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/004-0.1429.hdf5\n",
      "40200/40200 [==============================] - 6s 146us/sample - loss: 0.4757 - acc: 0.8515 - val_loss: 0.1429 - val_acc: 0.9588\n",
      "Epoch 5/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8843\n",
      "Epoch 00005: val_loss improved from 0.14295 to 0.11975, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/005-0.1197.hdf5\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.3766 - acc: 0.8846 - val_loss: 0.1197 - val_acc: 0.9641\n",
      "Epoch 6/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.3099 - acc: 0.9061\n",
      "Epoch 00006: val_loss improved from 0.11975 to 0.10169, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/006-0.1017.hdf5\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.3095 - acc: 0.9062 - val_loss: 0.1017 - val_acc: 0.9690\n",
      "Epoch 7/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.2762 - acc: 0.9146\n",
      "Epoch 00007: val_loss improved from 0.10169 to 0.08635, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/007-0.0863.hdf5\n",
      "40200/40200 [==============================] - 5s 135us/sample - loss: 0.2762 - acc: 0.9146 - val_loss: 0.0863 - val_acc: 0.9733\n",
      "Epoch 8/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9273\n",
      "Epoch 00008: val_loss improved from 0.08635 to 0.08458, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/008-0.0846.hdf5\n",
      "40200/40200 [==============================] - 5s 127us/sample - loss: 0.2389 - acc: 0.9273 - val_loss: 0.0846 - val_acc: 0.9738\n",
      "Epoch 9/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9361\n",
      "Epoch 00009: val_loss improved from 0.08458 to 0.07344, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/009-0.0734.hdf5\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.2108 - acc: 0.9360 - val_loss: 0.0734 - val_acc: 0.9776\n",
      "Epoch 10/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9413\n",
      "Epoch 00010: val_loss improved from 0.07344 to 0.06733, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/010-0.0673.hdf5\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.1956 - acc: 0.9413 - val_loss: 0.0673 - val_acc: 0.9793\n",
      "Epoch 11/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9458\n",
      "Epoch 00011: val_loss did not improve from 0.06733\n",
      "40200/40200 [==============================] - 6s 146us/sample - loss: 0.1753 - acc: 0.9457 - val_loss: 0.0675 - val_acc: 0.9795\n",
      "Epoch 12/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9518\n",
      "Epoch 00012: val_loss improved from 0.06733 to 0.05923, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/012-0.0592.hdf5\n",
      "40200/40200 [==============================] - 6s 160us/sample - loss: 0.1610 - acc: 0.9518 - val_loss: 0.0592 - val_acc: 0.9822\n",
      "Epoch 13/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9560\n",
      "Epoch 00013: val_loss improved from 0.05923 to 0.05598, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/013-0.0560.hdf5\n",
      "40200/40200 [==============================] - 6s 160us/sample - loss: 0.1456 - acc: 0.9560 - val_loss: 0.0560 - val_acc: 0.9825\n",
      "Epoch 14/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9599\n",
      "Epoch 00014: val_loss improved from 0.05598 to 0.05321, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/014-0.0532.hdf5\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.1342 - acc: 0.9599 - val_loss: 0.0532 - val_acc: 0.9830\n",
      "Epoch 15/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9614\n",
      "Epoch 00015: val_loss did not improve from 0.05321\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.1267 - acc: 0.9614 - val_loss: 0.0544 - val_acc: 0.9837\n",
      "Epoch 16/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9650\n",
      "Epoch 00016: val_loss did not improve from 0.05321\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.1182 - acc: 0.9650 - val_loss: 0.0548 - val_acc: 0.9841\n",
      "Epoch 17/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9644\n",
      "Epoch 00017: val_loss improved from 0.05321 to 0.04869, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/017-0.0487.hdf5\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.1166 - acc: 0.9644 - val_loss: 0.0487 - val_acc: 0.9851\n",
      "Epoch 18/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9680\n",
      "Epoch 00018: val_loss improved from 0.04869 to 0.04684, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/018-0.0468.hdf5\n",
      "40200/40200 [==============================] - 5s 129us/sample - loss: 0.1077 - acc: 0.9679 - val_loss: 0.0468 - val_acc: 0.9862\n",
      "Epoch 19/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9698\n",
      "Epoch 00019: val_loss improved from 0.04684 to 0.04572, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/019-0.0457.hdf5\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0991 - acc: 0.9698 - val_loss: 0.0457 - val_acc: 0.9866\n",
      "Epoch 20/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9713\n",
      "Epoch 00020: val_loss improved from 0.04572 to 0.04489, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/020-0.0449.hdf5\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0967 - acc: 0.9713 - val_loss: 0.0449 - val_acc: 0.9864\n",
      "Epoch 21/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9724\n",
      "Epoch 00021: val_loss did not improve from 0.04489\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0923 - acc: 0.9724 - val_loss: 0.0457 - val_acc: 0.9868\n",
      "Epoch 22/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9738\n",
      "Epoch 00022: val_loss improved from 0.04489 to 0.04380, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/022-0.0438.hdf5\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0869 - acc: 0.9736 - val_loss: 0.0438 - val_acc: 0.9868\n",
      "Epoch 23/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9759\n",
      "Epoch 00023: val_loss improved from 0.04380 to 0.04002, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/023-0.0400.hdf5\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0826 - acc: 0.9759 - val_loss: 0.0400 - val_acc: 0.9883\n",
      "Epoch 24/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9745\n",
      "Epoch 00024: val_loss improved from 0.04002 to 0.03925, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/024-0.0393.hdf5\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0828 - acc: 0.9746 - val_loss: 0.0393 - val_acc: 0.9888\n",
      "Epoch 25/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9771\n",
      "Epoch 00025: val_loss did not improve from 0.03925\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0734 - acc: 0.9771 - val_loss: 0.0422 - val_acc: 0.9871\n",
      "Epoch 26/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9784\n",
      "Epoch 00026: val_loss did not improve from 0.03925\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0745 - acc: 0.9784 - val_loss: 0.0403 - val_acc: 0.9879\n",
      "Epoch 27/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9789\n",
      "Epoch 00027: val_loss did not improve from 0.03925\n",
      "40200/40200 [==============================] - 6s 159us/sample - loss: 0.0693 - acc: 0.9788 - val_loss: 0.0395 - val_acc: 0.9884\n",
      "Epoch 28/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9796\n",
      "Epoch 00028: val_loss did not improve from 0.03925\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0704 - acc: 0.9796 - val_loss: 0.0407 - val_acc: 0.9886\n",
      "Epoch 29/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9798\n",
      "Epoch 00029: val_loss improved from 0.03925 to 0.03782, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/029-0.0378.hdf5\n",
      "40200/40200 [==============================] - 5s 126us/sample - loss: 0.0670 - acc: 0.9798 - val_loss: 0.0378 - val_acc: 0.9891\n",
      "Epoch 30/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9801\n",
      "Epoch 00030: val_loss did not improve from 0.03782\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0629 - acc: 0.9801 - val_loss: 0.0380 - val_acc: 0.9888\n",
      "Epoch 31/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9821\n",
      "Epoch 00031: val_loss improved from 0.03782 to 0.03681, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/031-0.0368.hdf5\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0587 - acc: 0.9821 - val_loss: 0.0368 - val_acc: 0.9893\n",
      "Epoch 32/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9824\n",
      "Epoch 00032: val_loss did not improve from 0.03681\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0587 - acc: 0.9825 - val_loss: 0.0405 - val_acc: 0.9880\n",
      "Epoch 33/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9834\n",
      "Epoch 00033: val_loss did not improve from 0.03681\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0541 - acc: 0.9835 - val_loss: 0.0373 - val_acc: 0.9888\n",
      "Epoch 34/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9830\n",
      "Epoch 00034: val_loss improved from 0.03681 to 0.03592, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/034-0.0359.hdf5\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0541 - acc: 0.9830 - val_loss: 0.0359 - val_acc: 0.9896\n",
      "Epoch 35/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9834\n",
      "Epoch 00035: val_loss did not improve from 0.03592\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0549 - acc: 0.9834 - val_loss: 0.0410 - val_acc: 0.9887\n",
      "Epoch 36/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9846\n",
      "Epoch 00036: val_loss did not improve from 0.03592\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0504 - acc: 0.9846 - val_loss: 0.0362 - val_acc: 0.9896\n",
      "Epoch 37/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9853\n",
      "Epoch 00037: val_loss did not improve from 0.03592\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0471 - acc: 0.9852 - val_loss: 0.0365 - val_acc: 0.9897\n",
      "Epoch 38/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9856\n",
      "Epoch 00038: val_loss did not improve from 0.03592\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0475 - acc: 0.9856 - val_loss: 0.0388 - val_acc: 0.9885\n",
      "Epoch 39/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9848\n",
      "Epoch 00039: val_loss did not improve from 0.03592\n",
      "40200/40200 [==============================] - 5s 135us/sample - loss: 0.0478 - acc: 0.9848 - val_loss: 0.0398 - val_acc: 0.9894\n",
      "Epoch 40/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9861\n",
      "Epoch 00040: val_loss did not improve from 0.03592\n",
      "40200/40200 [==============================] - 5s 129us/sample - loss: 0.0452 - acc: 0.9860 - val_loss: 0.0403 - val_acc: 0.9886\n",
      "Epoch 41/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9854\n",
      "Epoch 00041: val_loss improved from 0.03592 to 0.03379, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/041-0.0338.hdf5\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0459 - acc: 0.9853 - val_loss: 0.0338 - val_acc: 0.9899\n",
      "Epoch 42/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9865\n",
      "Epoch 00042: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 5s 133us/sample - loss: 0.0432 - acc: 0.9865 - val_loss: 0.0461 - val_acc: 0.9876\n",
      "Epoch 43/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9874\n",
      "Epoch 00043: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0419 - acc: 0.9873 - val_loss: 0.0384 - val_acc: 0.9901\n",
      "Epoch 44/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9872\n",
      "Epoch 00044: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0396 - acc: 0.9873 - val_loss: 0.0371 - val_acc: 0.9900\n",
      "Epoch 45/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9872\n",
      "Epoch 00045: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 5s 136us/sample - loss: 0.0391 - acc: 0.9872 - val_loss: 0.0393 - val_acc: 0.9893\n",
      "Epoch 46/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9874\n",
      "Epoch 00046: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0395 - acc: 0.9874 - val_loss: 0.0352 - val_acc: 0.9908\n",
      "Epoch 47/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9878\n",
      "Epoch 00047: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0401 - acc: 0.9879 - val_loss: 0.0410 - val_acc: 0.9893\n",
      "Epoch 48/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9877\n",
      "Epoch 00048: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0380 - acc: 0.9876 - val_loss: 0.0379 - val_acc: 0.9905\n",
      "Epoch 49/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9894\n",
      "Epoch 00049: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0353 - acc: 0.9895 - val_loss: 0.0393 - val_acc: 0.9901\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9898\n",
      "Epoch 00050: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 5s 135us/sample - loss: 0.0331 - acc: 0.9898 - val_loss: 0.0391 - val_acc: 0.9907\n",
      "Epoch 51/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9886\n",
      "Epoch 00051: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 5s 130us/sample - loss: 0.0343 - acc: 0.9887 - val_loss: 0.0387 - val_acc: 0.9906\n",
      "Epoch 52/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9892\n",
      "Epoch 00052: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0327 - acc: 0.9893 - val_loss: 0.0393 - val_acc: 0.9901\n",
      "Epoch 53/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9897\n",
      "Epoch 00053: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0338 - acc: 0.9896 - val_loss: 0.0357 - val_acc: 0.9909\n",
      "Epoch 54/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9906\n",
      "Epoch 00054: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0292 - acc: 0.9907 - val_loss: 0.0377 - val_acc: 0.9908\n",
      "Epoch 55/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9906\n",
      "Epoch 00055: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0293 - acc: 0.9905 - val_loss: 0.0399 - val_acc: 0.9904\n",
      "Epoch 56/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9903\n",
      "Epoch 00056: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0292 - acc: 0.9902 - val_loss: 0.0392 - val_acc: 0.9910\n",
      "Epoch 57/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9900\n",
      "Epoch 00057: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0298 - acc: 0.9900 - val_loss: 0.0354 - val_acc: 0.9909\n",
      "Epoch 58/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9909\n",
      "Epoch 00058: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0296 - acc: 0.9909 - val_loss: 0.0372 - val_acc: 0.9907\n",
      "Epoch 59/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9914\n",
      "Epoch 00059: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0269 - acc: 0.9914 - val_loss: 0.0455 - val_acc: 0.9901\n",
      "Epoch 60/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9915\n",
      "Epoch 00060: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0283 - acc: 0.9915 - val_loss: 0.0384 - val_acc: 0.9908\n",
      "Epoch 61/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9909\n",
      "Epoch 00061: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 5s 131us/sample - loss: 0.0269 - acc: 0.9909 - val_loss: 0.0353 - val_acc: 0.9910\n",
      "Epoch 62/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9912\n",
      "Epoch 00062: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 5s 133us/sample - loss: 0.0262 - acc: 0.9912 - val_loss: 0.0424 - val_acc: 0.9907\n",
      "Epoch 63/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9913\n",
      "Epoch 00063: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0273 - acc: 0.9912 - val_loss: 0.0410 - val_acc: 0.9904\n",
      "Epoch 64/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9911\n",
      "Epoch 00064: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0273 - acc: 0.9911 - val_loss: 0.0396 - val_acc: 0.9910\n",
      "Epoch 65/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9927\n",
      "Epoch 00065: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0221 - acc: 0.9927 - val_loss: 0.0439 - val_acc: 0.9907\n",
      "Epoch 66/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9921\n",
      "Epoch 00066: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0237 - acc: 0.9921 - val_loss: 0.0454 - val_acc: 0.9904\n",
      "Epoch 67/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9922\n",
      "Epoch 00067: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0231 - acc: 0.9922 - val_loss: 0.0413 - val_acc: 0.9908\n",
      "Epoch 68/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9921\n",
      "Epoch 00068: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0250 - acc: 0.9921 - val_loss: 0.0424 - val_acc: 0.9908\n",
      "Epoch 69/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9921\n",
      "Epoch 00069: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0241 - acc: 0.9922 - val_loss: 0.0406 - val_acc: 0.9913\n",
      "Epoch 70/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9927\n",
      "Epoch 00070: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0222 - acc: 0.9927 - val_loss: 0.0404 - val_acc: 0.9914\n",
      "Epoch 71/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9924\n",
      "Epoch 00071: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0224 - acc: 0.9924 - val_loss: 0.0402 - val_acc: 0.9907\n",
      "Epoch 72/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9935\n",
      "Epoch 00072: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 5s 130us/sample - loss: 0.0197 - acc: 0.9935 - val_loss: 0.0402 - val_acc: 0.9914\n",
      "Epoch 73/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9935\n",
      "Epoch 00073: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 5s 133us/sample - loss: 0.0203 - acc: 0.9935 - val_loss: 0.0398 - val_acc: 0.9909\n",
      "Epoch 74/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9931\n",
      "Epoch 00074: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0205 - acc: 0.9930 - val_loss: 0.0429 - val_acc: 0.9911\n",
      "Epoch 75/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9935\n",
      "Epoch 00075: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0215 - acc: 0.9935 - val_loss: 0.0404 - val_acc: 0.9911\n",
      "Epoch 76/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9939\n",
      "Epoch 00076: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0199 - acc: 0.9939 - val_loss: 0.0433 - val_acc: 0.9908\n",
      "Epoch 77/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9939\n",
      "Epoch 00077: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0194 - acc: 0.9939 - val_loss: 0.0414 - val_acc: 0.9918\n",
      "Epoch 78/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9925\n",
      "Epoch 00078: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0215 - acc: 0.9925 - val_loss: 0.0476 - val_acc: 0.9901\n",
      "Epoch 79/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9943\n",
      "Epoch 00079: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0175 - acc: 0.9943 - val_loss: 0.0418 - val_acc: 0.9912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9936\n",
      "Epoch 00080: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0189 - acc: 0.9936 - val_loss: 0.0425 - val_acc: 0.9907\n",
      "Epoch 81/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9934\n",
      "Epoch 00081: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0195 - acc: 0.9934 - val_loss: 0.0410 - val_acc: 0.9914\n",
      "Epoch 82/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9938\n",
      "Epoch 00082: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 138us/sample - loss: 0.0190 - acc: 0.9938 - val_loss: 0.0422 - val_acc: 0.9907\n",
      "Epoch 83/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9935\n",
      "Epoch 00083: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 5s 128us/sample - loss: 0.0185 - acc: 0.9935 - val_loss: 0.0422 - val_acc: 0.9917\n",
      "Epoch 84/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9944\n",
      "Epoch 00084: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 5s 135us/sample - loss: 0.0180 - acc: 0.9944 - val_loss: 0.0393 - val_acc: 0.9911\n",
      "Epoch 85/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9939\n",
      "Epoch 00085: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0187 - acc: 0.9938 - val_loss: 0.0446 - val_acc: 0.9912\n",
      "Epoch 86/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9942\n",
      "Epoch 00086: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0168 - acc: 0.9942 - val_loss: 0.0459 - val_acc: 0.9907\n",
      "Epoch 87/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9945\n",
      "Epoch 00087: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0158 - acc: 0.9946 - val_loss: 0.0422 - val_acc: 0.9915\n",
      "Epoch 88/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9949\n",
      "Epoch 00088: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 139us/sample - loss: 0.0168 - acc: 0.9949 - val_loss: 0.0426 - val_acc: 0.9902\n",
      "Epoch 89/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9947\n",
      "Epoch 00089: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0154 - acc: 0.9947 - val_loss: 0.0453 - val_acc: 0.9910\n",
      "Epoch 90/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9943\n",
      "Epoch 00090: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0169 - acc: 0.9944 - val_loss: 0.0433 - val_acc: 0.9914\n",
      "Epoch 91/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9942\n",
      "Epoch 00091: val_loss did not improve from 0.03379\n",
      "40200/40200 [==============================] - 6s 140us/sample - loss: 0.0169 - acc: 0.9942 - val_loss: 0.0448 - val_acc: 0.9915\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 109us/sample - loss: 0.0319 - acc: 0.9900\n",
      "Loss: 0.031944163388975814 Accuracy: 0.99\n",
      "\n",
      "Train on 40200 samples, validate on 19800 samples\n",
      "Epoch 1/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 1.9506 - acc: 0.5231\n",
      "Epoch 00001: val_loss improved from inf to 0.28376, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/001-0.2838.hdf5\n",
      "40200/40200 [==============================] - 8s 200us/sample - loss: 1.9416 - acc: 0.5249 - val_loss: 0.2838 - val_acc: 0.9233\n",
      "Epoch 2/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.4473 - acc: 0.8612\n",
      "Epoch 00002: val_loss improved from 0.28376 to 0.14248, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/002-0.1425.hdf5\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.4473 - acc: 0.8612 - val_loss: 0.1425 - val_acc: 0.9576\n",
      "Epoch 3/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9159\n",
      "Epoch 00003: val_loss improved from 0.14248 to 0.10299, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/003-0.1030.hdf5\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.2816 - acc: 0.9161 - val_loss: 0.1030 - val_acc: 0.9680\n",
      "Epoch 4/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9370\n",
      "Epoch 00004: val_loss improved from 0.10299 to 0.08784, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/004-0.0878.hdf5\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.2160 - acc: 0.9369 - val_loss: 0.0878 - val_acc: 0.9728\n",
      "Epoch 5/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9500\n",
      "Epoch 00005: val_loss improved from 0.08784 to 0.07552, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/005-0.0755.hdf5\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.1750 - acc: 0.9501 - val_loss: 0.0755 - val_acc: 0.9780\n",
      "Epoch 6/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9590\n",
      "Epoch 00006: val_loss improved from 0.07552 to 0.07316, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/006-0.0732.hdf5\n",
      "40200/40200 [==============================] - 6s 158us/sample - loss: 0.1453 - acc: 0.9591 - val_loss: 0.0732 - val_acc: 0.9791\n",
      "Epoch 7/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9629\n",
      "Epoch 00007: val_loss improved from 0.07316 to 0.06604, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/007-0.0660.hdf5\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.1300 - acc: 0.9629 - val_loss: 0.0660 - val_acc: 0.9808\n",
      "Epoch 8/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9691\n",
      "Epoch 00008: val_loss improved from 0.06604 to 0.05915, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/008-0.0591.hdf5\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.1084 - acc: 0.9691 - val_loss: 0.0591 - val_acc: 0.9832\n",
      "Epoch 9/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9722\n",
      "Epoch 00009: val_loss improved from 0.05915 to 0.05783, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/009-0.0578.hdf5\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.0968 - acc: 0.9724 - val_loss: 0.0578 - val_acc: 0.9839\n",
      "Epoch 10/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9754\n",
      "Epoch 00010: val_loss improved from 0.05783 to 0.05688, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/010-0.0569.hdf5\n",
      "40200/40200 [==============================] - 6s 151us/sample - loss: 0.0850 - acc: 0.9754 - val_loss: 0.0569 - val_acc: 0.9850\n",
      "Epoch 11/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9781\n",
      "Epoch 00011: val_loss improved from 0.05688 to 0.04978, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/011-0.0498.hdf5\n",
      "40200/40200 [==============================] - 6s 145us/sample - loss: 0.0774 - acc: 0.9781 - val_loss: 0.0498 - val_acc: 0.9856\n",
      "Epoch 12/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9796\n",
      "Epoch 00012: val_loss did not improve from 0.04978\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0701 - acc: 0.9796 - val_loss: 0.0534 - val_acc: 0.9860\n",
      "Epoch 13/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9820\n",
      "Epoch 00013: val_loss did not improve from 0.04978\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0635 - acc: 0.9820 - val_loss: 0.0530 - val_acc: 0.9871\n",
      "Epoch 14/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9841\n",
      "Epoch 00014: val_loss improved from 0.04978 to 0.04820, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/014-0.0482.hdf5\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0593 - acc: 0.9841 - val_loss: 0.0482 - val_acc: 0.9882\n",
      "Epoch 15/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9860\n",
      "Epoch 00015: val_loss improved from 0.04820 to 0.04688, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/015-0.0469.hdf5\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.0497 - acc: 0.9860 - val_loss: 0.0469 - val_acc: 0.9875\n",
      "Epoch 16/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9863\n",
      "Epoch 00016: val_loss did not improve from 0.04688\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.0455 - acc: 0.9863 - val_loss: 0.0477 - val_acc: 0.9885\n",
      "Epoch 17/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9879\n",
      "Epoch 00017: val_loss did not improve from 0.04688\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0431 - acc: 0.9879 - val_loss: 0.0490 - val_acc: 0.9879\n",
      "Epoch 18/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9889\n",
      "Epoch 00018: val_loss did not improve from 0.04688\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0402 - acc: 0.9889 - val_loss: 0.0527 - val_acc: 0.9883\n",
      "Epoch 19/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9897\n",
      "Epoch 00019: val_loss did not improve from 0.04688\n",
      "40200/40200 [==============================] - 6s 153us/sample - loss: 0.0359 - acc: 0.9897 - val_loss: 0.0502 - val_acc: 0.9892\n",
      "Epoch 20/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9908\n",
      "Epoch 00020: val_loss did not improve from 0.04688\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0325 - acc: 0.9908 - val_loss: 0.0504 - val_acc: 0.9891\n",
      "Epoch 21/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9908\n",
      "Epoch 00021: val_loss improved from 0.04688 to 0.04469, saving model to model/checkpoint/vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/021-0.0447.hdf5\n",
      "40200/40200 [==============================] - 6s 151us/sample - loss: 0.0313 - acc: 0.9908 - val_loss: 0.0447 - val_acc: 0.9898\n",
      "Epoch 22/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9919\n",
      "Epoch 00022: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0276 - acc: 0.9920 - val_loss: 0.0467 - val_acc: 0.9899\n",
      "Epoch 23/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9915\n",
      "Epoch 00023: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 157us/sample - loss: 0.0289 - acc: 0.9915 - val_loss: 0.0502 - val_acc: 0.9890\n",
      "Epoch 24/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9926\n",
      "Epoch 00024: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.0272 - acc: 0.9926 - val_loss: 0.0505 - val_acc: 0.9891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9923\n",
      "Epoch 00025: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0246 - acc: 0.9923 - val_loss: 0.0529 - val_acc: 0.9890\n",
      "Epoch 26/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9941\n",
      "Epoch 00026: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0198 - acc: 0.9941 - val_loss: 0.0506 - val_acc: 0.9897\n",
      "Epoch 27/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9948\n",
      "Epoch 00027: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 153us/sample - loss: 0.0176 - acc: 0.9948 - val_loss: 0.0534 - val_acc: 0.9890\n",
      "Epoch 28/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9945\n",
      "Epoch 00028: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0197 - acc: 0.9945 - val_loss: 0.0548 - val_acc: 0.9899\n",
      "Epoch 29/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9944\n",
      "Epoch 00029: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0198 - acc: 0.9944 - val_loss: 0.0538 - val_acc: 0.9900\n",
      "Epoch 30/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9947\n",
      "Epoch 00030: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0157 - acc: 0.9948 - val_loss: 0.0607 - val_acc: 0.9887\n",
      "Epoch 31/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9956\n",
      "Epoch 00031: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 153us/sample - loss: 0.0158 - acc: 0.9956 - val_loss: 0.0516 - val_acc: 0.9900\n",
      "Epoch 32/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9952\n",
      "Epoch 00032: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0149 - acc: 0.9952 - val_loss: 0.0633 - val_acc: 0.9882\n",
      "Epoch 33/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9952\n",
      "Epoch 00033: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.0145 - acc: 0.9952 - val_loss: 0.0620 - val_acc: 0.9887\n",
      "Epoch 34/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9956\n",
      "Epoch 00034: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 153us/sample - loss: 0.0141 - acc: 0.9956 - val_loss: 0.0578 - val_acc: 0.9898\n",
      "Epoch 35/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9960\n",
      "Epoch 00035: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0129 - acc: 0.9960 - val_loss: 0.0654 - val_acc: 0.9891\n",
      "Epoch 36/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9959\n",
      "Epoch 00036: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 153us/sample - loss: 0.0140 - acc: 0.9959 - val_loss: 0.0559 - val_acc: 0.9902\n",
      "Epoch 37/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9962\n",
      "Epoch 00037: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0115 - acc: 0.9962 - val_loss: 0.0642 - val_acc: 0.9897\n",
      "Epoch 38/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9966\n",
      "Epoch 00038: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 148us/sample - loss: 0.0118 - acc: 0.9967 - val_loss: 0.0616 - val_acc: 0.9892\n",
      "Epoch 39/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9971\n",
      "Epoch 00039: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 147us/sample - loss: 0.0098 - acc: 0.9971 - val_loss: 0.0559 - val_acc: 0.9907\n",
      "Epoch 40/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9963\n",
      "Epoch 00040: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 143us/sample - loss: 0.0136 - acc: 0.9963 - val_loss: 0.0591 - val_acc: 0.9900\n",
      "Epoch 41/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9970\n",
      "Epoch 00041: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 150us/sample - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0651 - val_acc: 0.9901\n",
      "Epoch 42/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9969\n",
      "Epoch 00042: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 149us/sample - loss: 0.0108 - acc: 0.9969 - val_loss: 0.0582 - val_acc: 0.9906\n",
      "Epoch 43/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9975\n",
      "Epoch 00043: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 152us/sample - loss: 0.0083 - acc: 0.9975 - val_loss: 0.0621 - val_acc: 0.9904\n",
      "Epoch 44/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9973\n",
      "Epoch 00044: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 148us/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 0.0635 - val_acc: 0.9899\n",
      "Epoch 45/500\n",
      "39744/40200 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9977\n",
      "Epoch 00045: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 145us/sample - loss: 0.0078 - acc: 0.9977 - val_loss: 0.0808 - val_acc: 0.9887\n",
      "Epoch 46/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9972\n",
      "Epoch 00046: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 147us/sample - loss: 0.0099 - acc: 0.9972 - val_loss: 0.0626 - val_acc: 0.9905\n",
      "Epoch 47/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9979\n",
      "Epoch 00047: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 146us/sample - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0714 - val_acc: 0.9901\n",
      "Epoch 48/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9972\n",
      "Epoch 00048: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 146us/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.0658 - val_acc: 0.9902\n",
      "Epoch 49/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9977\n",
      "Epoch 00049: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0081 - acc: 0.9977 - val_loss: 0.0687 - val_acc: 0.9900\n",
      "Epoch 50/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9972\n",
      "Epoch 00050: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 141us/sample - loss: 0.0088 - acc: 0.9972 - val_loss: 0.0665 - val_acc: 0.9898\n",
      "Epoch 51/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9985\n",
      "Epoch 00051: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 142us/sample - loss: 0.0055 - acc: 0.9985 - val_loss: 0.0710 - val_acc: 0.9899\n",
      "Epoch 52/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9972\n",
      "Epoch 00052: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 146us/sample - loss: 0.0091 - acc: 0.9972 - val_loss: 0.0682 - val_acc: 0.9889\n",
      "Epoch 53/500\n",
      "39872/40200 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9973\n",
      "Epoch 00053: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 145us/sample - loss: 0.0080 - acc: 0.9973 - val_loss: 0.0698 - val_acc: 0.9901\n",
      "Epoch 54/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9982\n",
      "Epoch 00054: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 147us/sample - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0694 - val_acc: 0.9909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9979\n",
      "Epoch 00055: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 145us/sample - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0703 - val_acc: 0.9904\n",
      "Epoch 56/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9970\n",
      "Epoch 00056: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 150us/sample - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0588 - val_acc: 0.9907\n",
      "Epoch 57/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9983\n",
      "Epoch 00057: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 157us/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0612 - val_acc: 0.9909\n",
      "Epoch 58/500\n",
      "39936/40200 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9986\n",
      "Epoch 00058: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0725 - val_acc: 0.9902\n",
      "Epoch 59/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9978\n",
      "Epoch 00059: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 150us/sample - loss: 0.0075 - acc: 0.9978 - val_loss: 0.0731 - val_acc: 0.9898\n",
      "Epoch 60/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9988\n",
      "Epoch 00060: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 144us/sample - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0738 - val_acc: 0.9904\n",
      "Epoch 61/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9975\n",
      "Epoch 00061: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0083 - acc: 0.9975 - val_loss: 0.0705 - val_acc: 0.9901\n",
      "Epoch 62/500\n",
      "40000/40200 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9984\n",
      "Epoch 00062: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 154us/sample - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0687 - val_acc: 0.9901\n",
      "Epoch 63/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9979\n",
      "Epoch 00063: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 153us/sample - loss: 0.0060 - acc: 0.9979 - val_loss: 0.0660 - val_acc: 0.9911\n",
      "Epoch 64/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9983\n",
      "Epoch 00064: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0061 - acc: 0.9983 - val_loss: 0.0730 - val_acc: 0.9902\n",
      "Epoch 65/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9987\n",
      "Epoch 00065: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 155us/sample - loss: 0.0047 - acc: 0.9987 - val_loss: 0.0737 - val_acc: 0.9905\n",
      "Epoch 66/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9988\n",
      "Epoch 00066: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 160us/sample - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0683 - val_acc: 0.9909\n",
      "Epoch 67/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9984\n",
      "Epoch 00067: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0753 - val_acc: 0.9905\n",
      "Epoch 68/500\n",
      "40192/40200 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9985\n",
      "Epoch 00068: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 156us/sample - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0700 - val_acc: 0.9910\n",
      "Epoch 69/500\n",
      "39808/40200 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9980\n",
      "Epoch 00069: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 147us/sample - loss: 0.0072 - acc: 0.9980 - val_loss: 0.0680 - val_acc: 0.9909\n",
      "Epoch 70/500\n",
      "40128/40200 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9979\n",
      "Epoch 00070: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 151us/sample - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0671 - val_acc: 0.9903\n",
      "Epoch 71/500\n",
      "40064/40200 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9987\n",
      "Epoch 00071: val_loss did not improve from 0.04469\n",
      "40200/40200 [==============================] - 6s 147us/sample - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0653 - val_acc: 0.9904\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 119us/sample - loss: 0.0372 - acc: 0.9904\n",
      "Loss: 0.03720244868916198 Accuracy: 0.9904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    base = 'vis_2D_CNN_custom_ch_32_DO_075_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_2d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train, y_train, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val, y_val], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 63,050\n",
      "Trainable params: 63,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 95us/sample - loss: 0.0591 - acc: 0.9839\n",
      "Loss: 0.05906217606815044 Accuracy: 0.9839\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                15690     \n",
      "=================================================================\n",
      "Total params: 25,258\n",
      "Trainable params: 25,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 115us/sample - loss: 0.0298 - acc: 0.9901\n",
      "Loss: 0.029803342605679062 Accuracy: 0.9901\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 38,314\n",
      "Trainable params: 38,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 114us/sample - loss: 0.0235 - acc: 0.9934\n",
      "Loss: 0.023460563211250975 Accuracy: 0.9934\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 67,562\n",
      "Trainable params: 67,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 129us/sample - loss: 0.0319 - acc: 0.9900\n",
      "Loss: 0.031944163388975814 Accuracy: 0.99\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 140,138\n",
      "Trainable params: 140,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 120us/sample - loss: 0.0372 - acc: 0.9904\n",
      "Loss: 0.03720244868916198 Accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = 'vis_2D_CNN_custom_ch_32_DO_075_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 6):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 63,050\n",
      "Trainable params: 63,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 117us/sample - loss: 0.0624 - acc: 0.9845\n",
      "Loss: 0.06241586427586153 Accuracy: 0.9845\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                15690     \n",
      "=================================================================\n",
      "Total params: 25,258\n",
      "Trainable params: 25,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 123us/sample - loss: 0.0302 - acc: 0.9902\n",
      "Loss: 0.03020477082282887 Accuracy: 0.9902\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 38,314\n",
      "Trainable params: 38,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 133us/sample - loss: 0.0263 - acc: 0.9925\n",
      "Loss: 0.026336425130608358 Accuracy: 0.9925\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 67,562\n",
      "Trainable params: 67,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.0342 - acc: 0.9928\n",
      "Loss: 0.03417731573839503 Accuracy: 0.9928\n",
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 140,138\n",
      "Trainable params: 140,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 146us/sample - loss: 0.0602 - acc: 0.9910\n",
      "Loss: 0.060186876521909495 Accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vis_2D_CNN_custom_ch_32_DO_075_DO_3_conv Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 38,314\n",
      "Trainable params: 38,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 2s 171us/sample - loss: 0.0235 - acc: 0.9934\n",
      "Loss: 0.023460563211250975 Accuracy: 0.9934\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = 'vis_2D_CNN_custom_ch_32_DO_075_DO'\n",
    "\n",
    "i = 3\n",
    "model_name = base+'_{}_conv'.format(i)\n",
    "print()\n",
    "print(model_name, 'Model')\n",
    "model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "model = load_model(model_filename)\n",
    "model.summary()\n",
    "\n",
    "[loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "#         del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 975    0    1    0    0    0    2    1    1    0]\n",
      " [   0 1133    1    1    0    0    0    0    0    0]\n",
      " [   1    0 1028    0    1    0    0    1    0    1]\n",
      " [   0    0    0 1008    0    1    0    0    1    0]\n",
      " [   0    0    0    0  975    0    0    0    1    6]\n",
      " [   1    0    0    3    0  887    1    0    0    0]\n",
      " [   4    1    0    0    2    1  946    0    4    0]\n",
      " [   0    3    4    0    0    0    0 1019    1    1]\n",
      " [   0    0    0    1    1    0    0    0  969    3]\n",
      " [   0    0    1    0    5    4    0    4    1  994]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       980\n",
      "           1       1.00      1.00      1.00      1135\n",
      "           2       0.99      1.00      0.99      1032\n",
      "           3       1.00      1.00      1.00      1010\n",
      "           4       0.99      0.99      0.99       982\n",
      "           5       0.99      0.99      0.99       892\n",
      "           6       1.00      0.99      0.99       958\n",
      "           7       0.99      0.99      0.99      1028\n",
      "           8       0.99      0.99      0.99       974\n",
      "           9       0.99      0.99      0.99      1009\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8a307e0978>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFdCAYAAABGoXXzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADwhJREFUeJzt3X+o3fV9x/Hn68Y2UJsbCjVajZN2VffDrvUnduu0WyhrQVpkUK0OXDdsdbVD3A8bnUgVZkbBWms2RqFWqKxbGXRUKS3KSkGLmhqrxgoiMdWaGmtFQ6dxJJ/9cc9ld3dG8z057/P1e+7zAYdwvtxvPu+D+MznfO+535vWGpKkyZvrewBJmlUGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpyyLQXTBLgSGD3tNeWpIOwBni6dbiBy9QDy0Jcn+phXUk6WOuBnx3oF/cR2N0ARx75bubmVk114W3b7p3qepJmw4svvsjRRx8NHd959xFYAObmVk09sPPz81NdT9LK5je5JKmIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSoyVmCTfCbJE0leTnJPktMmPZgkDV3nwCY5B7ge+DxwEvBj4LtJ1k14NkkatHF2sJcBX2mt3dxaewS4CPgv4M8mOpkkDVynwCZ5M3AycMfisdbavtHz9+/nnNVJ5hcfLNzyS5JmXtcd7NuBVcAzy44/Axyxn3M2Ai8seXirQkkrwjQ+RXAdsHbJY/0U1pSk3nW9XeEvgL3A4cuOHw78/NVOaK3tAfYsPl/4hQaSNPs67WBba68APwI2LB5LMjd6/sPJjiZJwzbODbevB25JsgW4F7gUOBS4eZKDSdLQdQ5sa+1fkxwGXMPCN7YeAD7cWlv+jS9JWtHG+pUxrbWbgJsmPIskzRTvRSBJRQysJBUxsJJUxMBKUhEDK0lFDKwkFTGwklTEwEpSkbF+0GAStm27l/n5+amu2deNZlprvawrqV/uYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpiYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpiYCWpiIGVpCIGVpKKGFhJKmJgJanIIX0PME2ttV7WPeywo3tZF+DZZ5/sbW1ppXMHK0lFDKwkFTGwklTEwEpSEQMrSUUMrCQVMbCSVMTASlIRAytJRQysJBXpFNgkG5Pcl2R3kl1JvpXk+KrhJGnIuu5gzwQ2A6cDHwLeBHwvyaGTHkyShq7TzV5aax9e+jzJnwK7gJOBH0xuLEkavoO9Brt29OcvD3YQSZo1Y9+uMMkccANwV2vt4df4utXA6iWH1oy7piQNycHsYDcDJwDnvs7XbQReWPJ46iDWlKTBGCuwSW4CzgL+oLX2esG8joVLCYuP9eOsKUlD0+kSQZIAXwbOBj7YWtv+eue01vYAe5b8HV1nlKRB6noNdjNwHvAxYHeSI0bHX2itvTTRySRp4LpeIriYhbf53wd2LnmcM9mxJGn4un4O1vf3knSAvBeBJBUxsJJUxMBKUhEDK0lFDKwkFTGwklTEwEpSEQMrSUUMrCQVGft+sDpwzz77ZG9rv+Mdv97Lujt3Pt7LutIbiTtYSSpiYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpiYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpiYCWpiIGVpCIGVpKKGFhJKnJI3wOo1s6dj/ey7jHH/HYv6+7Ysa2XdTU9+1obzJruYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBxXYJJ9L0pLcMKmBJGlWjB3YJKcCnwYenNw4kjQ7xgpskrcCtwIXAs9PdCJJmhHj7mA3A7e31u54vS9MsjrJ/OIDWDPmmpI0KJ1vV5jkXOAk4NQDPGUjcHXXdSRp6DrtYJMcDXwJOL+19vIBnnYdsHbJY32nCSVpoLruYE8G1gH3J1k8tgo4I8klwOrW2t6lJ7TW9gB7Fp8vOU+SZlrXwN4JvGfZsZuBR4F/WB5XSVrJOgW2tbYbeHjpsSS/Ap5rrT386mdJ0srkT3JJUpGD/qWHrbUPTmAOSZo57mAlqYiBlaQiBlaSihhYSSpiYCWpiIGVpCIGVpKKGFhJKnLQP2ggvZodO7b1su6JJ27oZd2tW+/sZd2VaK6HG0aNu6Y7WEkqYmAlqYiBlaQiBlaSihhYSSpiYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpiYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpySN8DSJO0deudvax77LGn9LLuY49t6WVdHRh3sJJUxMBKUhEDK0lFDKwkFTGwklTEwEpSEQMrSUUMrCQVMbCSVMTASlKRzoFNclSSryd5LslLSR5K0s/PCUrSG1inexEkeRtwF/CfwEeAZ4FjgecnP5okDVvXm71cDjzZWvvkkmPbJziPJM2MrpcIPgpsSfLNJLuSbE1yYcVgkjR0XQP7LuBi4DHgj4B/Am5McsH+TkiyOsn84gNYM/a0kjQgXS8RzAFbWmtXjJ5vTXICcBFwy37O2QhcPeZ8kjRYXXewO4FHlh37CfBrr3HOdcDaJY/1HdeUpEHquoO9Czh+2bHjgB37O6G1tgfYs/g8ScclJWmYuu5gvwicnuSKJO9Och7wKWDz5EeTpGHrFNjW2n3A2cAngIeBq4BLW2u3FswmSYPW+ZcettZuA24rmEWSZor3IpCkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpiYCWpSOcfNJD0/z322JZe1l237phe1gXYtWu/tyDRiDtYSSpiYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpiYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpiYCWpiIGVpCIGVpKKGFhJKmJgJanIIX0PIGl8u3bt6G3td77zd3pZd/v2B3tZdxzuYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQinQKbZFWSa5NsT/JSkseTXJUkVQNK0lB1vRfB5cDFwAXANuAU4GbgBeDGyY4mScPWNbC/C/xHa+320fMnknwCOG2yY0nS8HW9Bns3sCHJcQBJ3gt8APjO/k5IsjrJ/OIDWDP2tJI0IF13sJuAeeDRJHuBVcCVrbVbX+OcjcDVY84nSYPVdQf7ceB84DzgJBauxf51kgte45zrgLVLHuvHmFOSBqfrDvYLwKbW2jdGzx9KcgwLu9RbXu2E1toeYM/icz9wIGml6LqDfQuwb9mxvWP8PZI087ruYL8NXJnkpyx8TOtE4DLgq5MeTJKGrmtgPwtcC/wjsA54Gvhn4JoJzyVJg9cpsK213cClo4ck6TV47VSSihhYSSpiYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIl1/kks6IPta62XdOW8mNDXbtz/Yy7rr1x839TX37ds71nnuYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpiYCWpiIGVpCIGVpKKGFhJKmJgJamIgZWkIgZWkooYWEkqYmAlqYiBlaQiBlaSihhYSSpiYCWpiIGVpCIGVpKKGFhJKnJIXwu/+OKLfS2tKdjXWi/rziW9rKvp2bdvbw9r7hvrvLQp/4+Q5CjgqakuKkmTsb619rMD/eI+AhvgSGD3GKevYSHO68c8f2h8vbNvpb3mIb/eNcDTrUM0p36JYDTcAf8LsFT+9+3f7tbazF9j8PXOvpX2mgf+ejvP6ze5JKmIgZWkIkML7B7g86M/VwJf7+xbaa95Rb3eqX+TS5JWiqHtYCVpMAysJBUxsJJUxMBKUpHBBDbJZ5I8keTlJPckOa3vmaok2ZjkviS7k+xK8q0kx/c917Qk+VySluSGvmepkuSoJF9P8lySl5I8lOSUvueqkmRVkmuTbB+93seTXJXM9s0jBhHYJOcA17Pw8Y6TgB8D302yrtfB6pwJbAZOBz4EvAn4XpJDe51qCpKcCnwaeLDvWaokeRtwF/DfwEeA3wL+Cni+z7mKXQ5cDFwC/Obo+d8Cn+1zqGqD+JhWknuA+1prl4yezwFPAl9urW3qdbgpSHIYsAs4s7X2g77nqZLkrcD9wF8Afwc80Fq7tN+pJi/JJuD3Wmu/3/cs05LkNuCZ1tqfLzn278BLrbU/6W+yWm/4HWySNwMnA3csHmut7Rs9f39fc03Z2tGfv+x1inqbgdtba3e87lcO20eBLUm+OboEtDXJhX0PVexuYEOS4wCSvBf4APCdXqcq1tv9YDt4O7AKeGbZ8WeA35j+ONM12q3fANzVWnu473mqJDmXhcs/p/Y9yxS8i4W3y9cDf8/Ca74xySuttVt6nazOJmAeeDTJXhb+n76ytXZrv2PVGkJgV7rNwAks/Gs/k5IcDXwJ+FBr7eW+55mCOWBLa+2K0fOtSU4ALgJmNbAfB84HzgO2Ae8Dbkjy9Az/ozKIwP4C2Ascvuz44cDPpz/O9CS5CTgLOKO1Nss3KT8ZWAfcv+SbyquAM5JcAqxurU3/NvZ1dgKPLDv2E+CPe5hlWr4AbGqtfWP0/KEkxwAbmd1/VN7412Bba68APwI2LB4bvW3eAPywr7kqZcFNwNnAH7bWtvc9U7E7gfewsKtZfGwBbgXeN2NxhYVPECz/2N1xwI4eZpmWtwDLf+/KXgbQoIMxhB0sLFyruiXJFuBe4FLgUODmXqeqs5mFt1IfA3YnOWJ0/IXW2kv9jVWjtbYb+D/Xl5P8CnhuRq87fxG4O8kVwL8BpwGfGj1m1beBK5P8lIVLBCcClwFf7XWqYoP4mBbA6K3i3wBHAA8Af9lau6ffqWok2d9/lE+21r42zVn6kuT7zOjHtACSnAVcBxwLbAeub619pd+p6iRZA1zLwruydcDTwL8A14zepc6kwQRWkoZmpq9/SFKfDKwkFTGwklTEwEpSEQMrSUUMrCQVMbCSVMTASlIRAytJRQysJBUxsJJUxMBKUpH/AUJxpanEM5miAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_real = np.argmax(y_test, axis=1)\n",
    "confusion_mat = confusion_matrix(y_real, y_pred)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_mat)\n",
    "print()\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_real, y_pred))\n",
    "print()\n",
    "\n",
    "# labels = y_table.T[0]\n",
    "plt.figure(figsize=(4,4), dpi=100)\n",
    "# plt.xticks(y_list, labels)\n",
    "# plt.yticks(y_list, labels)\n",
    "plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.bone_r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
