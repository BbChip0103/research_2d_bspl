{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical, multi_gpu_model, Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.load(path.join(data_dir, 'imagenet_6_class_172_train_data.npz'))\n",
    "# val_data = np.load(path.join(data_dir, 'imagenet_6_class_172_val_data.npz'))\n",
    "\n",
    "x_train = np.load(path.join(data_dir, 'imagenet_6_class_172_x_train.npy'))\n",
    "y_train = np.load(path.join(data_dir, 'imagenet_6_class_172_y_train.npy'))\n",
    "x_val = np.load(path.join(data_dir, 'imagenet_6_class_172_x_val.npy'))\n",
    "y_val = np.load(path.join(data_dir, 'imagenet_6_class_172_y_val.npy'))\n",
    "y_list = np.load(path.join(data_dir, 'imagenet_6_class_172_y_list.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((235111, 172, 172, 3),\n",
       " (235111,),\n",
       " (3183, 172, 172, 3),\n",
       " (3183,),\n",
       " (3183, 172, 172, 3),\n",
       " (3183,),\n",
       " (6,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train = train_data['x_data']\n",
    "# y_train = train_data['y_data']\n",
    "# x_val = val_data['x_data']\n",
    "# y_val = val_data['y_data']\n",
    "x_test = x_val\n",
    "y_test = y_val\n",
    "# y_list = val_data['y_list']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((235111, 172, 172, 3),\n",
       " (235111, 6),\n",
       " (3183, 172, 172, 3),\n",
       " (3183, 6),\n",
       " (3183, 172, 172, 3),\n",
       " (3183, 6))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = y_val\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train[0].shape\n",
    "output_size = len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2d_cnn_custom_ch_64_DO(conv_num=1):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    for i in range(conv_num):\n",
    "        x = Conv2D(kernel_size=5, filters=64*(2**(i//2)), strides=(1,1), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling2D(pool_size=3, strides=(2,2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "  \n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 473344)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 473344)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 2840070   \n",
      "=================================================================\n",
      "Total params: 2,844,934\n",
      "Trainable params: 2,844,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 118336)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 118336)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 710022    \n",
      "=================================================================\n",
      "Total params: 817,350\n",
      "Trainable params: 817,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 61952)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 61952)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 371718    \n",
      "=================================================================\n",
      "Total params: 683,974\n",
      "Trainable params: 683,974\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 15488)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15488)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 92934     \n",
      "=================================================================\n",
      "Total params: 814,918\n",
      "Trainable params: 814,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 55302     \n",
      "=================================================================\n",
      "Total params: 1,596,742\n",
      "Trainable params: 1,596,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 13830     \n",
      "=================================================================\n",
      "Total params: 3,193,926\n",
      "Trainable params: 3,193,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 172, 172, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 172, 172, 64)      4864      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 172, 172, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 86, 86, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 86, 86, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 43, 43, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 22, 22, 128)       409728    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 11, 11, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 6,469,702\n",
      "Trainable params: 6,469,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 8):\n",
    "    model = build_2d_cnn_custom_ch_64_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalanceDataGenerator(Sequence):\n",
    "    def __init__(self, x_data, y_data, batch_size, shuffle=True):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_size = int(np.sum(y_data, axis=0).min())\n",
    "        self.data_shape = x_data.shape[1:]\n",
    "        self.y_label = self.y_data.argmax(axis=1)\n",
    "        self.labels = np.unique(self.y_label)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.labels) * self.sample_size / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.zeros((len(self.labels), self.sample_size))\n",
    "        for i, label in enumerate(self.labels):\n",
    "            y_index = np.argwhere(self.y_label==label).squeeze()\n",
    "            if self.shuffle == True:\n",
    "                self.indexes[i] = np.random.choice(y_index, \n",
    "                                   self.sample_size, \n",
    "                                   replace=False)\n",
    "            else:\n",
    "                self.indexes[i] = y_index[:self.sample_size]\n",
    "                \n",
    "        self.indexes = self.indexes.flatten().astype(np.int32)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "                \n",
    "    def __getitem__(self, batch_idx):\n",
    "        indices = self.indexes[batch_idx*self.batch_size: (batch_idx+1)*self.batch_size]\n",
    "        return self.x_data[indices], self.y_data[indices]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "data_generator = BalanceDataGenerator(x_train, y_train,\n",
    "                                      batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.3874 - acc: 0.4302WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 17ms/sample - loss: 1.1170 - acc: 0.6275\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.11702, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/001-1.1170.hdf5\n",
      "242/242 [==============================] - 88s 362ms/step - loss: 1.3866 - acc: 0.4306 - val_loss: 1.1170 - val_acc: 0.6275\n",
      "Epoch 2/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 1.0855 - acc: 0.5893WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 57s 18ms/sample - loss: 1.3218 - acc: 0.4375\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.11702\n",
      "242/242 [==============================] - 89s 368ms/step - loss: 1.0847 - acc: 0.5899 - val_loss: 1.3218 - val_acc: 0.4375\n",
      "Epoch 3/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.9997 - acc: 0.6166WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 53s 17ms/sample - loss: 1.4405 - acc: 0.3975\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.11702\n",
      "242/242 [==============================] - 88s 365ms/step - loss: 0.9995 - acc: 0.6167 - val_loss: 1.4405 - val_acc: 0.3975\n",
      "Epoch 4/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8848 - acc: 0.6667WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3198/3183 [==============================] - 51s 16ms/sample - loss: 0.9269 - acc: 0.6457\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.11702 to 0.92688, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/004-0.9269.hdf5\n",
      "242/242 [==============================] - 103s 425ms/step - loss: 0.8844 - acc: 0.6667 - val_loss: 0.9269 - val_acc: 0.6457\n",
      "Epoch 5/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.8300 - acc: 0.6898WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 40s 12ms/sample - loss: 0.7542 - acc: 0.7541\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.92688 to 0.75417, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/005-0.7542.hdf5\n",
      "242/242 [==============================] - 92s 382ms/step - loss: 0.8297 - acc: 0.6899 - val_loss: 0.7542 - val_acc: 0.7541\n",
      "Epoch 6/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7616 - acc: 0.7091WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 42s 13ms/sample - loss: 0.6660 - acc: 0.7594\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.75417 to 0.66599, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/006-0.6660.hdf5\n",
      "242/242 [==============================] - 161s 665ms/step - loss: 0.7612 - acc: 0.7092 - val_loss: 0.6660 - val_acc: 0.7594\n",
      "Epoch 7/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.7287 - acc: 0.7256WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 207s 65ms/sample - loss: 0.9228 - acc: 0.6428\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.66599\n",
      "242/242 [==============================] - 280s 1s/step - loss: 0.7296 - acc: 0.7256 - val_loss: 0.9228 - val_acc: 0.6428\n",
      "Epoch 8/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6888 - acc: 0.7414WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 43s 13ms/sample - loss: 0.7116 - acc: 0.7500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.66599\n",
      "242/242 [==============================] - 60s 247ms/step - loss: 0.6889 - acc: 0.7417 - val_loss: 0.7116 - val_acc: 0.7500\n",
      "Epoch 9/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6845 - acc: 0.7418WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.8970 - acc: 0.6675\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.66599\n",
      "242/242 [==============================] - 65s 267ms/step - loss: 0.6837 - acc: 0.7420 - val_loss: 0.8970 - val_acc: 0.6675\n",
      "Epoch 10/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6550 - acc: 0.7571WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 70s 22ms/sample - loss: 0.9384 - acc: 0.6388\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.66599\n",
      "242/242 [==============================] - 99s 410ms/step - loss: 0.6554 - acc: 0.7571 - val_loss: 0.9384 - val_acc: 0.6388\n",
      "Epoch 11/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6201 - acc: 0.7705WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 61s 19ms/sample - loss: 0.7614 - acc: 0.7106\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.66599\n",
      "242/242 [==============================] - 100s 413ms/step - loss: 0.6199 - acc: 0.7706 - val_loss: 0.7614 - val_acc: 0.7106\n",
      "Epoch 12/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.6128 - acc: 0.7719WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 55s 17ms/sample - loss: 0.5783 - acc: 0.8034\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.66599 to 0.57834, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/012-0.5783.hdf5\n",
      "242/242 [==============================] - 101s 417ms/step - loss: 0.6126 - acc: 0.7718 - val_loss: 0.5783 - val_acc: 0.8034\n",
      "Epoch 13/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5754 - acc: 0.7852WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 16ms/sample - loss: 0.5294 - acc: 0.8175\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.57834 to 0.52942, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/013-0.5294.hdf5\n",
      "242/242 [==============================] - 197s 814ms/step - loss: 0.5749 - acc: 0.7854 - val_loss: 0.5294 - val_acc: 0.8175\n",
      "Epoch 14/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5699 - acc: 0.7909WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.7012 - acc: 0.7350\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.52942\n",
      "242/242 [==============================] - 78s 321ms/step - loss: 0.5707 - acc: 0.7903 - val_loss: 0.7012 - val_acc: 0.7350\n",
      "Epoch 15/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.7909WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.4684 - acc: 0.8478\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.52942 to 0.46844, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/015-0.4684.hdf5\n",
      "242/242 [==============================] - 88s 362ms/step - loss: 0.5560 - acc: 0.7911 - val_loss: 0.4684 - val_acc: 0.8478\n",
      "Epoch 16/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7953WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.7858 - acc: 0.7072\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 270s 1s/step - loss: 0.5488 - acc: 0.7957 - val_loss: 0.7858 - val_acc: 0.7072\n",
      "Epoch 17/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.8024WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.6914 - acc: 0.7500\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 80s 330ms/step - loss: 0.5344 - acc: 0.8024 - val_loss: 0.6914 - val_acc: 0.7500\n",
      "Epoch 18/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5164 - acc: 0.8097WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 35s 11ms/sample - loss: 0.6563 - acc: 0.7719\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 64s 266ms/step - loss: 0.5161 - acc: 0.8098 - val_loss: 0.6563 - val_acc: 0.7719\n",
      "Epoch 19/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8116WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 48s 15ms/sample - loss: 0.4794 - acc: 0.8606\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 78s 323ms/step - loss: 0.5037 - acc: 0.8115 - val_loss: 0.4794 - val_acc: 0.8606\n",
      "Epoch 20/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.8163WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 41s 13ms/sample - loss: 0.5162 - acc: 0.8125\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 72s 299ms/step - loss: 0.4952 - acc: 0.8162 - val_loss: 0.5162 - val_acc: 0.8125\n",
      "Epoch 21/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8175WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 42s 13ms/sample - loss: 0.5596 - acc: 0.7972\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 70s 289ms/step - loss: 0.4870 - acc: 0.8174 - val_loss: 0.5596 - val_acc: 0.7972\n",
      "Epoch 22/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4748 - acc: 0.8259WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 52s 16ms/sample - loss: 0.4704 - acc: 0.85281s - loss: 0.4538 \n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 83s 344ms/step - loss: 0.4755 - acc: 0.8256 - val_loss: 0.4704 - val_acc: 0.8528\n",
      "Epoch 23/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.8253WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.5432 - acc: 0.8206\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 85s 350ms/step - loss: 0.4642 - acc: 0.8252 - val_loss: 0.5432 - val_acc: 0.8206\n",
      "Epoch 24/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8339WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 44s 14ms/sample - loss: 0.7986 - acc: 0.7150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 81s 335ms/step - loss: 0.4559 - acc: 0.8336 - val_loss: 0.7986 - val_acc: 0.7150\n",
      "Epoch 25/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8303WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.5063 - acc: 0.8206\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 79s 328ms/step - loss: 0.4558 - acc: 0.8298 - val_loss: 0.5063 - val_acc: 0.8206\n",
      "Epoch 26/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8352WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 56s 18ms/sample - loss: 0.8229 - acc: 0.6825\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 87s 361ms/step - loss: 0.4428 - acc: 0.8349 - val_loss: 0.8229 - val_acc: 0.6825\n",
      "Epoch 27/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4455 - acc: 0.8375WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.6367 - acc: 0.7625\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.46844\n",
      "242/242 [==============================] - 86s 357ms/step - loss: 0.4459 - acc: 0.8372 - val_loss: 0.6367 - val_acc: 0.7625\n",
      "Epoch 28/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.8391WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 40s 12ms/sample - loss: 0.4444 - acc: 0.8675\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.46844 to 0.44440, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/028-0.4444.hdf5\n",
      "242/242 [==============================] - 80s 329ms/step - loss: 0.4326 - acc: 0.8385 - val_loss: 0.4444 - val_acc: 0.8675\n",
      "Epoch 29/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.8445WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.5319 - acc: 0.8253\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.44440\n",
      "242/242 [==============================] - 81s 337ms/step - loss: 0.4166 - acc: 0.8445 - val_loss: 0.5319 - val_acc: 0.8253\n",
      "Epoch 30/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8486WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 40s 13ms/sample - loss: 0.6393 - acc: 0.7806\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.44440\n",
      "242/242 [==============================] - 80s 331ms/step - loss: 0.4085 - acc: 0.8484 - val_loss: 0.6393 - val_acc: 0.7806\n",
      "Epoch 31/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.8514WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 45s 14ms/sample - loss: 0.4389 - acc: 0.8556\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.44440 to 0.43894, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/031-0.4389.hdf5\n",
      "242/242 [==============================] - 78s 323ms/step - loss: 0.4039 - acc: 0.8516 - val_loss: 0.4389 - val_acc: 0.8556\n",
      "Epoch 32/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.4114 - acc: 0.8465WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 41s 13ms/sample - loss: 0.5224 - acc: 0.8150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.43894\n",
      "242/242 [==============================] - 91s 378ms/step - loss: 0.4119 - acc: 0.8465 - val_loss: 0.5224 - val_acc: 0.8150\n",
      "Epoch 33/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8571WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 37s 12ms/sample - loss: 0.3818 - acc: 0.8725\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.43894 to 0.38182, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/033-0.3818.hdf5\n",
      "242/242 [==============================] - 78s 324ms/step - loss: 0.3900 - acc: 0.8567 - val_loss: 0.3818 - val_acc: 0.8725\n",
      "Epoch 34/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3892 - acc: 0.8582WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 43s 13ms/sample - loss: 0.5123 - acc: 0.8125\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.38182\n",
      "242/242 [==============================] - 84s 348ms/step - loss: 0.3888 - acc: 0.8584 - val_loss: 0.5123 - val_acc: 0.8125\n",
      "Epoch 35/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8602WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 50s 16ms/sample - loss: 0.5136 - acc: 0.8112\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.38182\n",
      "242/242 [==============================] - 88s 363ms/step - loss: 0.3800 - acc: 0.8597 - val_loss: 0.5136 - val_acc: 0.8112\n",
      "Epoch 36/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8685WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.3511 - acc: 0.9003\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.38182 to 0.35115, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/036-0.3511.hdf5\n",
      "242/242 [==============================] - 91s 376ms/step - loss: 0.3636 - acc: 0.8685 - val_loss: 0.3511 - val_acc: 0.9003\n",
      "Epoch 37/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3752 - acc: 0.8628WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 47s 15ms/sample - loss: 0.3595 - acc: 0.8734\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.35115\n",
      "242/242 [==============================] - 80s 329ms/step - loss: 0.3750 - acc: 0.8628 - val_loss: 0.3595 - val_acc: 0.8734\n",
      "Epoch 38/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3744 - acc: 0.8660WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 41s 13ms/sample - loss: 0.4541 - acc: 0.8591\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.35115\n",
      "242/242 [==============================] - 81s 336ms/step - loss: 0.3744 - acc: 0.8659 - val_loss: 0.4541 - val_acc: 0.8591\n",
      "Epoch 39/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8652WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 31s 10ms/sample - loss: 0.4564 - acc: 0.8450\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.35115\n",
      "242/242 [==============================] - 75s 311ms/step - loss: 0.3628 - acc: 0.8647 - val_loss: 0.4564 - val_acc: 0.8450\n",
      "Epoch 40/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8686WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 39s 12ms/sample - loss: 0.4050 - acc: 0.8600\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.35115\n",
      "242/242 [==============================] - 73s 302ms/step - loss: 0.3573 - acc: 0.8685 - val_loss: 0.4050 - val_acc: 0.8600\n",
      "Epoch 41/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8687WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.3559 - acc: 0.8850\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.35115\n",
      "242/242 [==============================] - 88s 364ms/step - loss: 0.3653 - acc: 0.8688 - val_loss: 0.3559 - val_acc: 0.8850\n",
      "Epoch 42/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8686WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 77s 24ms/sample - loss: 0.3649 - acc: 0.8831\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.35115\n",
      "242/242 [==============================] - 124s 511ms/step - loss: 0.3558 - acc: 0.8685 - val_loss: 0.3649 - val_acc: 0.8831\n",
      "Epoch 43/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3550 - acc: 0.8663WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 90s 28ms/sample - loss: 0.3726 - acc: 0.8700\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.35115\n",
      "242/242 [==============================] - 123s 507ms/step - loss: 0.3548 - acc: 0.8664 - val_loss: 0.3726 - val_acc: 0.8700\n",
      "Epoch 44/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/242 [============================>.] - ETA: 0s - loss: 0.3418 - acc: 0.8774WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 81s 25ms/sample - loss: 0.4933 - acc: 0.8234\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.35115\n",
      "242/242 [==============================] - 112s 463ms/step - loss: 0.3419 - acc: 0.8776 - val_loss: 0.4933 - val_acc: 0.8234\n",
      "Epoch 45/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8774WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 55s 17ms/sample - loss: 0.3618 - acc: 0.8675\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.35115\n",
      "242/242 [==============================] - 85s 351ms/step - loss: 0.3313 - acc: 0.8775 - val_loss: 0.3618 - val_acc: 0.8675\n",
      "Epoch 46/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8807WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 95s 30ms/sample - loss: 0.6503 - acc: 0.7672\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.35115\n",
      "242/242 [==============================] - 133s 548ms/step - loss: 0.3227 - acc: 0.8806 - val_loss: 0.6503 - val_acc: 0.7672\n",
      "Epoch 47/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8850WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 81s 25ms/sample - loss: 0.5607 - acc: 0.8078\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.35115\n",
      "242/242 [==============================] - 117s 483ms/step - loss: 0.3206 - acc: 0.8853 - val_loss: 0.5607 - val_acc: 0.8078\n",
      "Epoch 48/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.8851WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 80s 25ms/sample - loss: 0.2799 - acc: 0.9084\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.35115 to 0.27991, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/048-0.2799.hdf5\n",
      "242/242 [==============================] - 118s 488ms/step - loss: 0.3169 - acc: 0.8852 - val_loss: 0.2799 - val_acc: 0.9084\n",
      "Epoch 49/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3333 - acc: 0.8760WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 94s 29ms/sample - loss: 0.4347 - acc: 0.8525\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.27991\n",
      "242/242 [==============================] - 131s 543ms/step - loss: 0.3329 - acc: 0.8761 - val_loss: 0.4347 - val_acc: 0.8525\n",
      "Epoch 50/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8849WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 127s 40ms/sample - loss: 0.4744 - acc: 0.8275\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.27991\n",
      "242/242 [==============================] - 153s 634ms/step - loss: 0.3162 - acc: 0.8848 - val_loss: 0.4744 - val_acc: 0.8275\n",
      "Epoch 51/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.8817WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 82s 26ms/sample - loss: 0.3313 - acc: 0.8906\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.27991\n",
      "242/242 [==============================] - 112s 463ms/step - loss: 0.3181 - acc: 0.8816 - val_loss: 0.3313 - val_acc: 0.8906\n",
      "Epoch 52/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.8821WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 123s 39ms/sample - loss: 0.4415 - acc: 0.8550\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.27991\n",
      "242/242 [==============================] - 164s 679ms/step - loss: 0.3250 - acc: 0.8821 - val_loss: 0.4415 - val_acc: 0.8550\n",
      "Epoch 53/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.8828WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 128s 40ms/sample - loss: 0.2492 - acc: 0.9262\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.27991 to 0.24916, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/053-0.2492.hdf5\n",
      "242/242 [==============================] - 159s 658ms/step - loss: 0.3136 - acc: 0.8828 - val_loss: 0.2492 - val_acc: 0.9262\n",
      "Epoch 54/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.8874WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 71s 22ms/sample - loss: 0.3567 - acc: 0.8675\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 111s 460ms/step - loss: 0.3032 - acc: 0.8874 - val_loss: 0.3567 - val_acc: 0.8675\n",
      "Epoch 55/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.8918WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 111s 35ms/sample - loss: 0.4016 - acc: 0.8572\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 149s 617ms/step - loss: 0.2948 - acc: 0.8917 - val_loss: 0.4016 - val_acc: 0.8572\n",
      "Epoch 56/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.8899WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 82s 26ms/sample - loss: 0.4125 - acc: 0.8500\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 131s 540ms/step - loss: 0.3071 - acc: 0.8899 - val_loss: 0.4125 - val_acc: 0.8500\n",
      "Epoch 57/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.8887WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 61s 19ms/sample - loss: 0.3581 - acc: 0.8800\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 112s 464ms/step - loss: 0.3041 - acc: 0.8887 - val_loss: 0.3581 - val_acc: 0.8800\n",
      "Epoch 58/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.8869WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 150s 47ms/sample - loss: 0.3842 - acc: 0.8681\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 193s 797ms/step - loss: 0.3038 - acc: 0.8870 - val_loss: 0.3842 - val_acc: 0.8681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.8895WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 129s 40ms/sample - loss: 0.3717 - acc: 0.8725\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 162s 669ms/step - loss: 0.2988 - acc: 0.8894 - val_loss: 0.3717 - val_acc: 0.8725\n",
      "Epoch 60/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2955 - acc: 0.8950WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 66s 21ms/sample - loss: 0.4062 - acc: 0.8700\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 96s 397ms/step - loss: 0.2959 - acc: 0.8949 - val_loss: 0.4062 - val_acc: 0.8700\n",
      "Epoch 61/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.8935WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 149s 47ms/sample - loss: 0.2887 - acc: 0.8959\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 197s 815ms/step - loss: 0.2885 - acc: 0.8932 - val_loss: 0.2887 - val_acc: 0.8959\n",
      "Epoch 62/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.8990WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 136s 42ms/sample - loss: 0.2685 - acc: 0.9159\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 166s 686ms/step - loss: 0.2782 - acc: 0.8991 - val_loss: 0.2685 - val_acc: 0.9159\n",
      "Epoch 63/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.8990WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 128s 40ms/sample - loss: 0.5196 - acc: 0.8356\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 172s 709ms/step - loss: 0.2712 - acc: 0.8989 - val_loss: 0.5196 - val_acc: 0.8356\n",
      "Epoch 64/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.8995WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 141s 44ms/sample - loss: 0.2965 - acc: 0.9006\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 189s 782ms/step - loss: 0.2738 - acc: 0.8998 - val_loss: 0.2965 - val_acc: 0.9006\n",
      "Epoch 65/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2637 - acc: 0.9026WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 63s 20ms/sample - loss: 0.3450 - acc: 0.8931\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 93s 386ms/step - loss: 0.2638 - acc: 0.9022 - val_loss: 0.3450 - val_acc: 0.8931\n",
      "Epoch 66/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2637 - acc: 0.9021WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 130s 41ms/sample - loss: 0.3329 - acc: 0.8888\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 162s 669ms/step - loss: 0.2642 - acc: 0.9017 - val_loss: 0.3329 - val_acc: 0.8888\n",
      "Epoch 67/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9001WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 97s 30ms/sample - loss: 0.3838 - acc: 0.8788\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 145s 597ms/step - loss: 0.2688 - acc: 0.9003 - val_loss: 0.3838 - val_acc: 0.8788\n",
      "Epoch 68/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.8976WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 78s 24ms/sample - loss: 0.3548 - acc: 0.8875\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 108s 447ms/step - loss: 0.2742 - acc: 0.8975 - val_loss: 0.3548 - val_acc: 0.8875\n",
      "Epoch 69/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9058WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 164s 51ms/sample - loss: 0.3361 - acc: 0.8850\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 197s 812ms/step - loss: 0.2574 - acc: 0.9058 - val_loss: 0.3361 - val_acc: 0.8850\n",
      "Epoch 70/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9046WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 162s 50ms/sample - loss: 0.3789 - acc: 0.8866\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 192s 794ms/step - loss: 0.2638 - acc: 0.9047 - val_loss: 0.3789 - val_acc: 0.8866\n",
      "Epoch 71/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.9044WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 56s 18ms/sample - loss: 0.3724 - acc: 0.8819\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 92s 381ms/step - loss: 0.2640 - acc: 0.9045 - val_loss: 0.3724 - val_acc: 0.8819\n",
      "Epoch 72/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9075WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 153s 48ms/sample - loss: 0.3391 - acc: 0.8950\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 181s 747ms/step - loss: 0.2540 - acc: 0.9071 - val_loss: 0.3391 - val_acc: 0.8950\n",
      "Epoch 73/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9087WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 102s 32ms/sample - loss: 0.4272 - acc: 0.8750\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 135s 556ms/step - loss: 0.2550 - acc: 0.9089 - val_loss: 0.4272 - val_acc: 0.8750\n",
      "Epoch 74/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9052WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 63s 20ms/sample - loss: 0.3392 - acc: 0.8963\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 102s 422ms/step - loss: 0.2510 - acc: 0.9050 - val_loss: 0.3392 - val_acc: 0.8963\n",
      "Epoch 75/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9047WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 81s 25ms/sample - loss: 0.4440 - acc: 0.8675\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 163s 672ms/step - loss: 0.2551 - acc: 0.9046 - val_loss: 0.4440 - val_acc: 0.8675\n",
      "Epoch 76/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.9052WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 107s 33ms/sample - loss: 0.3315 - acc: 0.8925\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 153s 632ms/step - loss: 0.2584 - acc: 0.9050 - val_loss: 0.3315 - val_acc: 0.8925\n",
      "Epoch 77/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9098WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 87s 27ms/sample - loss: 0.3514 - acc: 0.8925\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 117s 482ms/step - loss: 0.2493 - acc: 0.9095 - val_loss: 0.3514 - val_acc: 0.8925\n",
      "Epoch 78/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9072WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 93s 29ms/sample - loss: 0.3287 - acc: 0.8975\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 157s 647ms/step - loss: 0.2477 - acc: 0.9073 - val_loss: 0.3287 - val_acc: 0.8975\n",
      "Epoch 79/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9128WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 112s 35ms/sample - loss: 0.4102 - acc: 0.8531\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 145s 600ms/step - loss: 0.2468 - acc: 0.9131 - val_loss: 0.4102 - val_acc: 0.8531\n",
      "Epoch 80/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9151WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 76s 24ms/sample - loss: 0.3048 - acc: 0.8888\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.2373 - acc: 0.9151 - val_loss: 0.3048 - val_acc: 0.8888\n",
      "Epoch 81/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9103WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 110s 34ms/sample - loss: 0.2736 - acc: 0.9028\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 159s 655ms/step - loss: 0.2433 - acc: 0.9104 - val_loss: 0.2736 - val_acc: 0.9028\n",
      "Epoch 82/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9090WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 155s 49ms/sample - loss: 0.3997 - acc: 0.8650\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.24916\n",
      "242/242 [==============================] - 187s 771ms/step - loss: 0.2498 - acc: 0.9091 - val_loss: 0.3997 - val_acc: 0.8650\n",
      "Epoch 83/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9151WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 60s 19ms/sample - loss: 0.2418 - acc: 0.9275\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.24916 to 0.24179, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/083-0.2418.hdf5\n",
      "242/242 [==============================] - 109s 449ms/step - loss: 0.2281 - acc: 0.9154 - val_loss: 0.2418 - val_acc: 0.9275\n",
      "Epoch 84/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9159WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 140s 44ms/sample - loss: 0.4768 - acc: 0.8550\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 174s 719ms/step - loss: 0.2333 - acc: 0.9161 - val_loss: 0.4768 - val_acc: 0.8550\n",
      "Epoch 85/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9188WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 107s 34ms/sample - loss: 0.3124 - acc: 0.9025\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 140s 578ms/step - loss: 0.2292 - acc: 0.9186 - val_loss: 0.3124 - val_acc: 0.9025\n",
      "Epoch 86/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9153WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 69s 22ms/sample - loss: 0.3333 - acc: 0.8950\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 115s 475ms/step - loss: 0.2391 - acc: 0.9153 - val_loss: 0.3333 - val_acc: 0.8950\n",
      "Epoch 87/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9141WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 120s 38ms/sample - loss: 0.3375 - acc: 0.8934\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 150s 620ms/step - loss: 0.2390 - acc: 0.9144 - val_loss: 0.3375 - val_acc: 0.8934\n",
      "Epoch 88/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9151WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 113s 35ms/sample - loss: 0.2508 - acc: 0.9228\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 142s 585ms/step - loss: 0.2341 - acc: 0.9153 - val_loss: 0.2508 - val_acc: 0.9228\n",
      "Epoch 89/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9177WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3183 [==============================] - 94s 29ms/sample - loss: 0.3543 - acc: 0.8775\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 124s 514ms/step - loss: 0.2281 - acc: 0.9178 - val_loss: 0.3543 - val_acc: 0.8775\n",
      "Epoch 90/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9121WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.4174 - acc: 0.8653\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 102s 422ms/step - loss: 0.2364 - acc: 0.9120 - val_loss: 0.4174 - val_acc: 0.8653\n",
      "Epoch 91/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9203WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 136s 42ms/sample - loss: 0.3039 - acc: 0.9150\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 174s 719ms/step - loss: 0.2219 - acc: 0.9205 - val_loss: 0.3039 - val_acc: 0.9150\n",
      "Epoch 92/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9182WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 96s 30ms/sample - loss: 0.3744 - acc: 0.8725\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 140s 577ms/step - loss: 0.2310 - acc: 0.9182 - val_loss: 0.3744 - val_acc: 0.8725\n",
      "Epoch 93/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9200WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 56s 18ms/sample - loss: 0.2743 - acc: 0.9159\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 96s 398ms/step - loss: 0.2186 - acc: 0.9199 - val_loss: 0.2743 - val_acc: 0.9159\n",
      "Epoch 94/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9157WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 133s 42ms/sample - loss: 0.3503 - acc: 0.8913\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 176s 726ms/step - loss: 0.2307 - acc: 0.9157 - val_loss: 0.3503 - val_acc: 0.8913\n",
      "Epoch 95/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9221WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 137s 43ms/sample - loss: 0.3058 - acc: 0.9050\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 204s 844ms/step - loss: 0.2171 - acc: 0.9223 - val_loss: 0.3058 - val_acc: 0.9050\n",
      "Epoch 96/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9233WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 46s 14ms/sample - loss: 0.3212 - acc: 0.9000\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 93s 386ms/step - loss: 0.2234 - acc: 0.9234 - val_loss: 0.3212 - val_acc: 0.9000\n",
      "Epoch 97/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9208WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 132s 41ms/sample - loss: 0.3543 - acc: 0.8963\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 187s 774ms/step - loss: 0.2197 - acc: 0.9209 - val_loss: 0.3543 - val_acc: 0.8963\n",
      "Epoch 98/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9207WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 101s 32ms/sample - loss: 0.3928 - acc: 0.8759\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 139s 573ms/step - loss: 0.2157 - acc: 0.9208 - val_loss: 0.3928 - val_acc: 0.8759\n",
      "Epoch 99/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9219WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 68s 21ms/sample - loss: 0.3146 - acc: 0.9125\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 100s 414ms/step - loss: 0.2198 - acc: 0.9220 - val_loss: 0.3146 - val_acc: 0.9125\n",
      "Epoch 100/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9224WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 163s 51ms/sample - loss: 0.2757 - acc: 0.9100\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 200s 827ms/step - loss: 0.2156 - acc: 0.9225 - val_loss: 0.2757 - val_acc: 0.9100\n",
      "Epoch 101/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9194WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 89s 28ms/sample - loss: 0.3043 - acc: 0.9059\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 121s 502ms/step - loss: 0.2204 - acc: 0.9194 - val_loss: 0.3043 - val_acc: 0.9059\n",
      "Epoch 102/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9212WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 76s 24ms/sample - loss: 0.3173 - acc: 0.9100\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 119s 493ms/step - loss: 0.2171 - acc: 0.9213 - val_loss: 0.3173 - val_acc: 0.9100\n",
      "Epoch 103/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9221WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 130s 41ms/sample - loss: 0.4942 - acc: 0.8234\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 176s 729ms/step - loss: 0.2133 - acc: 0.9224 - val_loss: 0.4942 - val_acc: 0.8234\n",
      "Epoch 104/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9242WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 83s 26ms/sample - loss: 0.3269 - acc: 0.8925\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 143s 590ms/step - loss: 0.2081 - acc: 0.9242 - val_loss: 0.3269 - val_acc: 0.8925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9254WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 95s 30ms/sample - loss: 0.4216 - acc: 0.8666\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 135s 556ms/step - loss: 0.2108 - acc: 0.9256 - val_loss: 0.4216 - val_acc: 0.8666\n",
      "Epoch 106/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9250WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 123s 38ms/sample - loss: 0.3170 - acc: 0.8825\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 157s 647ms/step - loss: 0.2151 - acc: 0.9251 - val_loss: 0.3170 - val_acc: 0.8825\n",
      "Epoch 107/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9231WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 117s 37ms/sample - loss: 0.2965 - acc: 0.9078\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 149s 615ms/step - loss: 0.2134 - acc: 0.9231 - val_loss: 0.2965 - val_acc: 0.9078\n",
      "Epoch 108/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9273WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 88s 27ms/sample - loss: 0.3215 - acc: 0.9000\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.2068 - acc: 0.9272 - val_loss: 0.3215 - val_acc: 0.9000\n",
      "Epoch 109/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9247WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 131s 41ms/sample - loss: 0.2451 - acc: 0.9225\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 179s 741ms/step - loss: 0.2033 - acc: 0.9246 - val_loss: 0.2451 - val_acc: 0.9225\n",
      "Epoch 110/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9226WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 111s 35ms/sample - loss: 0.4322 - acc: 0.8525s - l\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 155s 639ms/step - loss: 0.2081 - acc: 0.9226 - val_loss: 0.4322 - val_acc: 0.8525\n",
      "Epoch 111/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9294WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 66s 21ms/sample - loss: 0.2915 - acc: 0.9075\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 122s 502ms/step - loss: 0.2011 - acc: 0.9294 - val_loss: 0.2915 - val_acc: 0.9075\n",
      "Epoch 112/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9219WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 159s 50ms/sample - loss: 0.2794 - acc: 0.9200\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 201s 829ms/step - loss: 0.2091 - acc: 0.9220 - val_loss: 0.2794 - val_acc: 0.9200\n",
      "Epoch 113/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9281- ETA: 4sWARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 134s 42ms/sample - loss: 0.3281 - acc: 0.8869\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.24179\n",
      "242/242 [==============================] - 173s 715ms/step - loss: 0.2034 - acc: 0.9281 - val_loss: 0.3281 - val_acc: 0.8869\n",
      "Epoch 114/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9320WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 74s 23ms/sample - loss: 0.1905 - acc: 0.9300\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.24179 to 0.19049, saving model to model/checkpoint/vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO_3_conv_checkpoint/114-0.1905.hdf5\n",
      "242/242 [==============================] - 122s 505ms/step - loss: 0.1900 - acc: 0.9319 - val_loss: 0.1905 - val_acc: 0.9300\n",
      "Epoch 115/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9290WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 170s 53ms/sample - loss: 0.2826 - acc: 0.9016\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.19049\n",
      "242/242 [==============================] - 204s 842ms/step - loss: 0.1998 - acc: 0.9291 - val_loss: 0.2826 - val_acc: 0.9016\n",
      "Epoch 116/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9296WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 76s 24ms/sample - loss: 0.3423 - acc: 0.8900\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.19049\n",
      "242/242 [==============================] - 131s 543ms/step - loss: 0.1920 - acc: 0.9294 - val_loss: 0.3423 - val_acc: 0.8900\n",
      "Epoch 117/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9309WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "3200/3183 [==============================] - 116s 36ms/sample - loss: 0.3687 - acc: 0.8675\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.19049\n",
      "242/242 [==============================] - 146s 603ms/step - loss: 0.1876 - acc: 0.9308 - val_loss: 0.3687 - val_acc: 0.8675\n",
      "Epoch 118/10000\n",
      "241/242 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9285WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "  40/3183 [..............................] - ETA: 2:31:30 - loss: 0.6279 - acc: 0.8250"
     ]
    }
   ],
   "source": [
    "for i in range(3, 8):\n",
    "    base = 'vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_2d_cnn_custom_ch_64_DO(conv_num=i)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit_generator(data_generator,\n",
    "            steps_per_epoch=len(x_train)//batch_size,\n",
    "            epochs=10000,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks = [checkpointer, early_stopping],\n",
    "            workers=8, \n",
    "            use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = 'vis_imagenet_6_class_augmentation_2D_CNN_custom_ch_64_DO_050_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 8):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 8):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
